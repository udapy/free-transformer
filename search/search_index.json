{"config":{"lang":["en"],"separator":"[\\s\\-]+","pipeline":["stopWordFilter"]},"docs":[{"location":"","title":"Free Transformer","text":"<p>Free Transformer: A Llama-style decoder architecture with explicit latent plans, conditional VAE training, and benchmark comparisons against standard Transformers.</p> <p>Designed for efficient PyTorch training on modern GPUs with full FSDP support and modern optimizations.</p>"},{"location":"#what-is-the-free-transformer","title":"What Is the Free Transformer?","text":"<p>Traditional autoregressive Transformers generate each token by conditioning only on the sequence so far (\"reactive\" behavior). Free Transformer introduces a latent planning mechanism\u2014first choosing a stochastic abstract plan (<code>Z</code>), then generating tokens to fit that plan. This scalable conditional VAE architecture maintains high-level coherence, improves controllable generation, and enables richer sequence modeling.</p>"},{"location":"#key-features","title":"Key Features","text":""},{"location":"#architecture","title":"\ud83c\udfd7\ufe0f Architecture","text":"<ul> <li>Llama-style backbone: RMSNorm, SwiGLU, RoPE, Grouped-Query Attention (GQA)</li> <li>Latent Planning: Explicit plan variable <code>Z</code> with differentiable binary coding</li> <li>Conditional VAE: Reconstruction + KL loss with free bits regularization</li> </ul>"},{"location":"#performance-scaling","title":"\u26a1 Performance &amp; Scaling","text":"<ul> <li>FSDP Support: Multi-GPU training with PyTorch Fully Sharded Data Parallel</li> <li>Mixed Precision: Automatic Mixed Precision (AMP) with gradient scaling</li> <li>Memory Efficient: Gradient checkpointing and optimized attention patterns</li> <li>Modern Optimizations: bfloat16, efficient parameter sharding</li> </ul>"},{"location":"#development-training","title":"\ud83d\udd27 Development &amp; Training","text":"<ul> <li>Flexible Training: Switchable inference/training flows with mode selection</li> <li>Synthetic + Real Data: Fast prototyping with built-in synthetic data generation</li> <li>Comprehensive Testing: Unit/integration tests, benchmark comparisons</li> <li>Quality Assurance: Type checking, linting, formatting, CI-ready</li> </ul>"},{"location":"#quick-start","title":"Quick Start","text":"<p>The fastest way to get started is with Docker:</p> <pre><code>git clone https://github.com/udapy/free-transformer.git\ncd free-transformer\ndocker-compose up free-transformer-demo\n</code></pre> <p>Or install locally:</p> <pre><code>uv pip install -e \".[dev]\"\nmake demo\n</code></pre>"},{"location":"#architecture-overview","title":"Architecture Overview","text":"<pre><code>flowchart TD\n    subgraph \"Training Mode\"\n        A[Input Tokens] --&gt; B[Embedding Layer]\n        B --&gt; C[\"Decoder Blocks 1..L/2\"]\n        C --&gt; D[\"Encoder Block&lt;br/&gt;(Non-causal, learned query \u03b6)\"]\n        D --&gt; E[Encoder Readout FC]\n        E --&gt; F[\"Binary Mapper&lt;br/&gt;Diff. discrete plan Z\"]\n        F --&gt; G[\"Inject Z into model&lt;br/&gt;via Post-sampler FC\"]\n        C --&gt; G\n        G --&gt; H[\"Decoder Blocks L/2+1..L\"]\n        H --&gt; I[Output Logits]\n    end\n\n    subgraph \"Inference Mode\"\n        AA[Prompt] --&gt; BB[Embedding Layer]\n        BB --&gt; CC[\"Decoder Blocks 1..L/2\"]\n        subgraph \"Plan Sampling\"\n            DD[\"Sample Random Z&lt;br/&gt;(Uniform prior)\"]\n        end\n        DD --&gt; GG[Inject Z via FC]\n        CC --&gt; GG\n        GG --&gt; HH[\"Decoder Blocks L/2+1..L\"]\n        HH --&gt; II[Generate Tokens]\n    end</code></pre>"},{"location":"#navigation","title":"Navigation","text":"<ul> <li>Getting Started: Installation and setup</li> <li>Architecture: Deep dive into the model design</li> <li>Training: Training guides and best practices</li> <li>API Reference: Complete API documentation</li> <li>Examples: Code examples and tutorials</li> </ul>"},{"location":"#license","title":"License","text":"<p>MIT License \u2014 see LICENSE</p>"},{"location":"faq/","title":"Frequently Asked Questions","text":""},{"location":"faq/#general-questions","title":"General Questions","text":""},{"location":"faq/#what-is-the-free-transformer","title":"What is the Free Transformer?","text":"<p>The Free Transformer is a novel neural architecture that extends traditional autoregressive Transformers with explicit latent planning. Instead of generating tokens purely reactively based on previous tokens, it first creates an abstract \"plan\" (latent variable Z) and then generates tokens conditioned on both the history and this plan.</p>"},{"location":"faq/#how-does-it-differ-from-standard-transformers","title":"How does it differ from standard Transformers?","text":"Aspect Standard Transformer Free Transformer Generation Reactive (token-by-token) Plan-then-generate Training Language modeling loss Conditional VAE loss Coherence Local Global + Local Controllability Limited High (via plan manipulation) Architecture Decoder-only Decoder + Encoder + Latent"},{"location":"faq/#what-are-the-main-benefits","title":"What are the main benefits?","text":"<ol> <li>Better long-range coherence: The latent plan helps maintain consistency across long sequences</li> <li>Controllable generation: You can potentially manipulate the latent plan for controlled text generation</li> <li>Richer representations: The model learns more structured internal representations</li> <li>Improved sample diversity: Different plans lead to different generation styles</li> </ol>"},{"location":"faq/#technical-questions","title":"Technical Questions","text":""},{"location":"faq/#what-is-the-latent-dimension-and-how-do-i-choose-it","title":"What is the latent dimension and how do I choose it?","text":"<p>The latent dimension (<code>latent_dim</code>) determines the size of the binary plan vector Z. Typical values:</p> <ul> <li>Small models (&lt; 100M params): 8-16 dimensions</li> <li>Medium models (100M-1B params): 16-32 dimensions  </li> <li>Large models (&gt; 1B params): 32-64 dimensions</li> </ul> <p>Start with 16-32 and adjust based on your model size and task complexity.</p>"},{"location":"faq/#what-is-free-bits-and-why-is-it-important","title":"What is \"free bits\" and why is it important?","text":"<p>Free bits is a regularization technique that prevents posterior collapse in VAE training. It sets a minimum threshold for the KL divergence loss:</p> <pre><code>kl_loss = torch.clamp(kl_loss, min=free_bits)\n</code></pre> <p>Typical values: 0.5-2.0. Higher values encourage more latent variable usage but may hurt reconstruction quality.</p>"},{"location":"faq/#how-do-i-know-if-my-model-is-working-correctly","title":"How do I know if my model is working correctly?","text":"<p>Monitor these metrics during training:</p> <ol> <li>KL loss should be positive: If it drops to zero, you have posterior collapse</li> <li>Reconstruction loss should decrease: Standard language modeling progress</li> <li>Total loss should be stable: No sudden spikes or instability</li> <li>Generation quality: Manually inspect generated text</li> </ol>"},{"location":"faq/#whats-the-difference-between-training-and-inference-modes","title":"What's the difference between training and inference modes?","text":"<p>Training mode: - Uses the encoder to compute latent plan from the full sequence - Optimizes both reconstruction and KL losses - Plan is derived from the actual data</p> <p>Inference mode: - Samples latent plan from uniform prior (no encoder needed) - Only uses reconstruction for generation - Plan is randomly sampled</p>"},{"location":"faq/#usage-questions","title":"Usage Questions","text":""},{"location":"faq/#can-i-use-this-for-real-world-datasets","title":"Can I use this for real-world datasets?","text":"<p>Yes! While the examples use synthetic data for quick prototyping, the model works with any text dataset. You can:</p> <ol> <li>Use HuggingFace datasets directly</li> <li>Provide your own text files</li> <li>Modify the data loading pipeline in <code>synthetic_data.py</code></li> </ol>"},{"location":"faq/#how-do-i-run-on-multiple-gpus","title":"How do I run on multiple GPUs?","text":"<p>Use FSDP (Fully Sharded Data Parallel):</p> <pre><code># Automatic GPU detection\ntorchrun --nproc_per_node=auto examples/train_free.py --config configs/free_transformer.yaml --use-fsdp\n\n# Or use the Makefile\nmake train-free-fsdp\n</code></pre>"},{"location":"faq/#can-i-run-this-without-a-gpu","title":"Can I run this without a GPU?","text":"<p>Yes, but it will be much slower. Use the CPU Docker image:</p> <pre><code>make docker-build-cpu\nmake docker-run-cpu\n</code></pre> <p>Or set device in your code: <pre><code>model = FreeTransformer(config)\nmodel = model.to('cpu')\n</code></pre></p>"},{"location":"faq/#how-do-i-change-the-model-size","title":"How do I change the model size?","text":"<p>Edit the configuration file or create a new one:</p> <pre><code>model:\n  hidden_dim: 768      # Increase for larger model\n  num_layers: 24       # More layers = more capacity\n  num_heads: 12        # Usually hidden_dim // 64\n  latent_dim: 32       # Scale with model size\n</code></pre>"},{"location":"faq/#training-questions","title":"Training Questions","text":""},{"location":"faq/#my-kl-loss-is-zero-whats-wrong","title":"My KL loss is zero. What's wrong?","text":"<p>This is posterior collapse. The model is ignoring the latent variable. Solutions:</p> <ol> <li>Increase free bits: Try 1.0-2.0 instead of 0.5</li> <li>Reduce KL weight: Start with 0.01-0.05 instead of 0.1</li> <li>Use KL annealing: Gradually increase KL weight during training</li> <li>Check latent dimension: Might be too large for your model</li> </ol>"},{"location":"faq/#training-is-unstable-how-do-i-fix-it","title":"Training is unstable. How do I fix it?","text":"<p>Common solutions:</p> <ol> <li>Gradient clipping: <code>torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)</code></li> <li>Lower learning rate: Try 1e-5 instead of 1e-4</li> <li>Warmup: Use learning rate warmup for first 1000 steps</li> <li>Mixed precision: Can help with stability and speed</li> </ol>"},{"location":"faq/#how-long-should-i-train","title":"How long should I train?","text":"<p>Depends on your dataset and model size:</p> <ul> <li>Small synthetic data: 5-10 epochs</li> <li>Medium datasets (1M-10M tokens): 10-50 epochs</li> <li>Large datasets (100M+ tokens): 1-5 epochs</li> </ul> <p>Monitor validation loss and stop when it plateaus.</p>"},{"location":"faq/#should-i-use-curriculum-learning","title":"Should I use curriculum learning?","text":"<p>Yes, it often helps! Start with:</p> <ol> <li>Short sequences (128 tokens) \u2192 Long sequences (512+ tokens)</li> <li>High KL weight (1.0) \u2192 Low KL weight (0.1)</li> <li>Simple data \u2192 Complex data</li> </ol>"},{"location":"faq/#comparison-questions","title":"Comparison Questions","text":""},{"location":"faq/#how-does-it-compare-to-other-vae-based-language-models","title":"How does it compare to other VAE-based language models?","text":"<p>The Free Transformer is specifically designed for autoregressive generation with:</p> <ul> <li>Explicit binary plans: More interpretable than continuous latents</li> <li>Llama-style backbone: Modern, efficient architecture</li> <li>Flexible injection: Plan can influence multiple layers</li> <li>Training efficiency: Competitive with standard Transformers</li> </ul>"},{"location":"faq/#when-should-i-use-free-transformer-vs-standard-transformer","title":"When should I use Free Transformer vs standard Transformer?","text":"<p>Use Free Transformer when: - You need better long-range coherence - Controllable generation is important - You're working with structured text (stories, articles) - Sample diversity matters</p> <p>Use standard Transformer when: - You need maximum training efficiency - Working with very short sequences - Simplicity is preferred - You have limited computational resources</p>"},{"location":"faq/#deployment-questions","title":"Deployment Questions","text":""},{"location":"faq/#can-i-deploy-this-in-production","title":"Can I deploy this in production?","text":"<p>Yes, but consider:</p> <ol> <li>Inference mode is efficient: No encoder overhead</li> <li>Model size: Similar to equivalent standard Transformer</li> <li>Memory usage: Slightly higher due to latent computations</li> <li>Latency: Comparable to baseline models</li> </ol>"},{"location":"faq/#how-do-i-optimize-for-inference","title":"How do I optimize for inference?","text":"<ol> <li>Use inference mode: <code>model(tokens, mode='inference')</code></li> <li>Enable eval mode: <code>model.eval()</code></li> <li>Disable gradients: <code>torch.no_grad()</code></li> <li>Consider quantization: Standard PyTorch quantization works</li> <li>Batch inference: Process multiple sequences together</li> </ol>"},{"location":"faq/#can-i-convert-to-onnx-or-tensorrt","title":"Can I convert to ONNX or TensorRT?","text":"<p>The model uses standard PyTorch operations, so conversion should work, but:</p> <ol> <li>Test thoroughly: Some operations might not be supported</li> <li>Separate modes: Export training and inference modes separately</li> <li>Dynamic shapes: May need fixed input sizes</li> </ol>"},{"location":"faq/#development-questions","title":"Development Questions","text":""},{"location":"faq/#how-do-i-contribute","title":"How do I contribute?","text":"<ol> <li>Fork the repository</li> <li>Create a feature branch</li> <li>Make your changes</li> <li>Run tests: <code>make test</code></li> <li>Run quality checks: <code>make quality</code></li> <li>Submit a pull request</li> </ol>"},{"location":"faq/#how-do-i-add-custom-components","title":"How do I add custom components?","text":"<p>The architecture is modular:</p> <ol> <li>Custom encoder: Inherit from <code>nn.Module</code>, implement <code>forward()</code></li> <li>Custom injection: Modify <code>injection.py</code></li> <li>Custom losses: Add to <code>losses.py</code></li> <li>Custom data: Extend <code>synthetic_data.py</code></li> </ol>"},{"location":"faq/#where-can-i-get-help","title":"Where can I get help?","text":"<ol> <li>Documentation: This site covers most use cases</li> <li>GitHub Issues: Report bugs or ask questions</li> <li>Code examples: Check the <code>examples/</code> directory</li> <li>Tests: Look at <code>tests/</code> for usage patterns</li> </ol>"},{"location":"faq/#performance-questions","title":"Performance Questions","text":""},{"location":"faq/#how-much-slower-is-it-than-baseline-transformers","title":"How much slower is it than baseline Transformers?","text":"<p>Training: ~20-30% slower due to encoder and VAE loss Inference: ~5-10% slower due to latent computations</p> <p>The overhead is minimal and often worth it for the improved capabilities.</p>"},{"location":"faq/#how-much-memory-does-it-use","title":"How much memory does it use?","text":"<p>Training: ~30-40% more memory than baseline (due to encoder) Inference: ~10-15% more memory than baseline</p> <p>Use gradient checkpointing and mixed precision to reduce memory usage.</p>"},{"location":"faq/#can-i-make-it-faster","title":"Can I make it faster?","text":"<ol> <li>Use mixed precision: <code>torch.cuda.amp</code></li> <li>Gradient checkpointing: Trades compute for memory</li> <li>Efficient attention: Flash Attention (planned feature)</li> <li>Model parallelism: FSDP for large models</li> <li>Batch size tuning: Find optimal batch size for your hardware</li> </ol> <p>Still have questions? Open an issue on GitHub!</p>"},{"location":"api/config/","title":"Configuration API Reference","text":""},{"location":"api/config/#free_transformer.config.ModelConfig","title":"<code>free_transformer.config.ModelConfig(vocab_size=32000, hidden_dim=4096, num_layers=32, num_heads=32, num_kv_heads=None, ffn_hidden_dim=11008, max_seq_len=2048, latent_dim=16, split_layer=None, use_rmsnorm=True, use_rope=True, use_swiglu=True, dropout=0.0, attention_dropout=0.0)</code>  <code>dataclass</code>","text":"<p>Configuration for model architecture.</p>"},{"location":"api/config/#free_transformer.config.TrainingConfig","title":"<code>free_transformer.config.TrainingConfig(learning_rate=0.0003, weight_decay=0.1, beta1=0.9, beta2=0.95, grad_clip=1.0, warmup_steps=2000, max_steps=100000, beta_kl=1.0, kappa_free_bits=0.3466, batch_size=64, gradient_accumulation_steps=1, use_fsdp=False, use_deepspeed=False, fsdp_config=dict(), deepspeed_config=dict(), save_every=5000, eval_every=1000, checkpoint_dir='./checkpoints', log_every=100, wandb_project=None)</code>  <code>dataclass</code>","text":"<p>Configuration for training.</p>"},{"location":"api/config/#free_transformer.config.TrainingConfig.from_yaml","title":"<code>from_yaml(path)</code>  <code>classmethod</code>","text":"<p>Load configuration from YAML file.</p> Source code in <code>src/free_transformer/config.py</code> <pre><code>@classmethod\ndef from_yaml(cls, path: str):\n    \"\"\"Load configuration from YAML file.\"\"\"\n    with open(path, \"r\") as f:\n        config_dict = yaml.safe_load(f)\n    return cls(**config_dict)\n</code></pre>"},{"location":"api/config/#free_transformer.config.TrainingConfig.to_yaml","title":"<code>to_yaml(path)</code>","text":"<p>Save configuration to YAML file.</p> Source code in <code>src/free_transformer/config.py</code> <pre><code>def to_yaml(self, path: str):\n    \"\"\"Save configuration to YAML file.\"\"\"\n    with open(path, \"w\") as f:\n        yaml.dump(self.__dict__, f)\n</code></pre>"},{"location":"api/data/","title":"Data API Reference","text":""},{"location":"api/data/#free_transformer.synthetic_data","title":"<code>free_transformer.synthetic_data</code>","text":"<p>Synthetic data generation for training and testing.</p>"},{"location":"api/data/#free_transformer.synthetic_data.SyntheticDataGenerator","title":"<code>SyntheticDataGenerator(vocab_size=10000, seq_length=512, seed=42)</code>","text":"<p>Generate synthetic token sequences for training.</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def __init__(\n    self,\n    vocab_size: int = 10000,\n    seq_length: int = 512,\n    seed: int = 42,\n):\n    self.vocab_size = vocab_size\n    self.seq_length = seq_length\n    self.seed = seed\n    self.rng = np.random.RandomState(seed)\n</code></pre>"},{"location":"api/data/#free_transformer.synthetic_data.SyntheticDataGenerator.generate_batch","title":"<code>generate_batch(batch_size)</code>","text":"<p>Generate a batch of sequences.</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def generate_batch(self, batch_size: int) -&gt; torch.Tensor:\n    \"\"\"Generate a batch of sequences.\"\"\"\n    tokens = self.rng.randint(0, self.vocab_size, size=(batch_size, self.seq_length))\n    return torch.tensor(tokens, dtype=torch.long)\n</code></pre>"},{"location":"api/data/#free_transformer.synthetic_data.SyntheticDataGenerator.generate_dataset","title":"<code>generate_dataset(num_samples)</code>","text":"<p>Generate full dataset.</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def generate_dataset(self, num_samples: int) -&gt; torch.Tensor:\n    \"\"\"Generate full dataset.\"\"\"\n    tokens = self.rng.randint(0, self.vocab_size, size=(num_samples, self.seq_length))\n    return torch.tensor(tokens, dtype=torch.long)\n</code></pre>"},{"location":"api/data/#free_transformer.synthetic_data.SyntheticDataGenerator.generate_sample","title":"<code>generate_sample()</code>","text":"<p>Generate a single random sequence.</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def generate_sample(self) -&gt; torch.Tensor:\n    \"\"\"Generate a single random sequence.\"\"\"\n    tokens = self.rng.randint(0, self.vocab_size, size=self.seq_length)\n    return torch.tensor(tokens, dtype=torch.long)\n</code></pre>"},{"location":"api/data/#free_transformer.synthetic_data.SyntheticDataGenerator.save_dataset","title":"<code>save_dataset(num_samples, output_path)</code>","text":"<p>Generate and save dataset to disk.</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def save_dataset(self, num_samples: int, output_path: str):\n    \"\"\"Generate and save dataset to disk.\"\"\"\n    data = self.generate_dataset(num_samples)\n    torch.save(data, output_path)\n\n    # Save metadata\n    metadata = {\n        \"vocab_size\": self.vocab_size,\n        \"seq_length\": self.seq_length,\n        \"num_samples\": num_samples,\n        \"seed\": self.seed,\n    }\n    metadata_path = Path(output_path).parent / f\"{Path(output_path).stem}_metadata.json\"\n    with open(metadata_path, \"w\") as f:\n        json.dump(metadata, f, indent=2)\n</code></pre>"},{"location":"api/data/#free_transformer.synthetic_data.SyntheticDataset","title":"<code>SyntheticDataset(data_path)</code>","text":"<p>               Bases: <code>Dataset</code></p> <p>PyTorch Dataset for synthetic data.</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def __init__(self, data_path: str):\n    self.data = torch.load(data_path)\n    metadata_path = Path(data_path).parent / f\"{Path(data_path).stem}_metadata.json\"\n    if metadata_path.exists():\n        with open(metadata_path, \"r\") as f:\n            self.metadata = json.load(f)\n    else:\n        self.metadata = {}\n</code></pre>"},{"location":"api/data/#free_transformer.synthetic_data.SyntheticDataset.__getitem__","title":"<code>__getitem__(idx)</code>","text":"<p>Return input and target (shifted by 1).</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def __getitem__(self, idx: int) -&gt; Tuple[torch.Tensor, torch.Tensor]:\n    \"\"\"Return input and target (shifted by 1).\"\"\"\n    tokens = self.data[idx]\n    # Input: all tokens except last\n    # Target: all tokens except first (shifted by 1)\n    return tokens[:-1], tokens[1:]\n</code></pre>"},{"location":"api/data/#free_transformer.synthetic_data.create_dataloaders","title":"<code>create_dataloaders(train_path, val_path, batch_size=32, num_workers=4, device=None)</code>","text":"<p>Create train and validation dataloaders.</p> Source code in <code>src/free_transformer/synthetic_data.py</code> <pre><code>def create_dataloaders(\n    train_path: str,\n    val_path: str,\n    batch_size: int = 32,\n    num_workers: int = 4,\n    device: Optional[torch.device] = None,\n) -&gt; Tuple[DataLoader, DataLoader]:\n    \"\"\"Create train and validation dataloaders.\"\"\"\n    train_dataset = SyntheticDataset(train_path)\n    val_dataset = SyntheticDataset(val_path)\n\n    # Only use pin_memory for CUDA devices\n    pin_memory = device is not None and device.type == \"cuda\"\n\n    train_loader = DataLoader(\n        train_dataset,\n        batch_size=batch_size,\n        shuffle=True,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    val_loader = DataLoader(\n        val_dataset,\n        batch_size=batch_size,\n        shuffle=False,\n        num_workers=num_workers,\n        pin_memory=pin_memory,\n    )\n\n    return train_loader, val_loader\n</code></pre>"},{"location":"api/losses/","title":"Losses API Reference","text":""},{"location":"api/losses/#free_transformer.losses","title":"<code>free_transformer.losses</code>","text":"<p>Loss functions for Free Transformer training.</p>"},{"location":"api/losses/#free_transformer.losses.FreeTransformerLoss","title":"<code>FreeTransformerLoss(latent_dim=16, beta_kl=1.0, free_bits=0.3466, ignore_index=-100)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Wrapper class for Free Transformer loss.</p> Source code in <code>src/free_transformer/losses.py</code> <pre><code>def __init__(\n    self,\n    latent_dim: int = 16,\n    beta_kl: float = 1.0,\n    free_bits: float = 0.3466,\n    ignore_index: int = -100,\n):\n    super().__init__()\n    self.latent_dim = latent_dim\n    self.beta_kl = beta_kl\n    self.free_bits = free_bits\n    self.ignore_index = ignore_index\n</code></pre>"},{"location":"api/losses/#free_transformer.losses.compute_kl_divergence","title":"<code>compute_kl_divergence(z_logits, latent_dim=16, free_bits=0.3466)</code>","text":"<p>Compute KL divergence with free bits for latent plan Z.</p> <p>Implements Equation (5) from paper: (1/T) * sum_t max(0, D_KL(Q(Z_t|S) || P(Z_t)) - kappa)</p> <p>Parameters:</p> Name Type Description Default <code>z_logits</code> <code>Tensor</code> <p>Encoder logits [batch, seq_len, latent_dim]</p> required <code>latent_dim</code> <code>int</code> <p>Dimension of latent space (H)</p> <code>16</code> <code>free_bits</code> <code>float</code> <p>Free bits budget (kappa)</p> <code>0.3466</code> <p>Returns:</p> Name Type Description <code>kl_loss</code> <code>Tensor</code> <p>Scalar KL divergence loss</p> Source code in <code>src/free_transformer/losses.py</code> <pre><code>def compute_kl_divergence(\n    z_logits: torch.Tensor,\n    latent_dim: int = 16,\n    free_bits: float = 0.3466,  # log(2)/2\n) -&gt; torch.Tensor:\n    \"\"\"\n    Compute KL divergence with free bits for latent plan Z.\n\n    Implements Equation (5) from paper:\n    (1/T) * sum_t max(0, D_KL(Q(Z_t|S) || P(Z_t)) - kappa)\n\n    Args:\n        z_logits: Encoder logits [batch, seq_len, latent_dim]\n        latent_dim: Dimension of latent space (H)\n        free_bits: Free bits budget (kappa)\n\n    Returns:\n        kl_loss: Scalar KL divergence loss\n    \"\"\"\n    batch_size, seq_len, H = z_logits.shape\n    assert H == latent_dim, f\"Expected latent_dim={latent_dim}, got {H}\"\n\n    # Convert logits to bit probabilities\n    bit_probs = torch.sigmoid(z_logits)  # [batch, seq_len, H]\n\n    # For each bit, compute KL between Bernoulli(p) and Uniform(0.5)\n    # KL(Bernoulli(p) || Bernoulli(0.5)) = p*log(2p) + (1-p)*log(2(1-p))\n    eps = 1e-10\n    kl_per_bit = bit_probs * torch.log(2 * bit_probs + eps) + (1 - bit_probs) * torch.log(\n        2 * (1 - bit_probs) + eps\n    )\n\n    # Sum over H bits to get KL for each Z_t\n    kl_per_position = kl_per_bit.sum(dim=-1)  # [batch, seq_len]\n\n    # Apply free bits: max(0, KL - kappa)\n    kl_with_free_bits = torch.clamp(kl_per_position - free_bits, min=0.0)\n\n    # Average over batch and sequence\n    kl_loss = kl_with_free_bits.mean()\n\n    return kl_loss\n</code></pre>"},{"location":"api/losses/#free_transformer.losses.compute_reconstruction_loss","title":"<code>compute_reconstruction_loss(logits, targets, ignore_index=-100)</code>","text":"<p>Standard cross-entropy loss for token prediction.</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Model predictions [batch, seq_len, vocab_size]</p> required <code>targets</code> <code>Tensor</code> <p>Target tokens [batch, seq_len]</p> required <code>ignore_index</code> <code>int</code> <p>Index to ignore in loss computation</p> <code>-100</code> <p>Returns:</p> Name Type Description <code>loss</code> <code>Tensor</code> <p>Scalar loss value</p> Source code in <code>src/free_transformer/losses.py</code> <pre><code>def compute_reconstruction_loss(\n    logits: torch.Tensor,\n    targets: torch.Tensor,\n    ignore_index: int = -100,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Standard cross-entropy loss for token prediction.\n\n    Args:\n        logits: Model predictions [batch, seq_len, vocab_size]\n        targets: Target tokens [batch, seq_len]\n        ignore_index: Index to ignore in loss computation\n\n    Returns:\n        loss: Scalar loss value\n    \"\"\"\n    batch_size, seq_len, vocab_size = logits.shape\n\n    # Reshape for cross-entropy\n    logits_flat = logits.reshape(-1, vocab_size)\n    targets_flat = targets.reshape(-1)\n\n    loss = F.cross_entropy(logits_flat, targets_flat, ignore_index=ignore_index, reduction=\"mean\")\n\n    return loss\n</code></pre>"},{"location":"api/losses/#free_transformer.losses.compute_vae_loss","title":"<code>compute_vae_loss(logits, targets, z_logits, latent_dim=16, beta_kl=1.0, free_bits=0.3466, ignore_index=-100)</code>","text":"<p>Complete VAE loss for Free Transformer.</p> <p>Loss = Reconstruction + beta * KL_with_free_bits</p> <p>Parameters:</p> Name Type Description Default <code>logits</code> <code>Tensor</code> <p>Model predictions [batch, seq_len, vocab_size]</p> required <code>targets</code> <code>Tensor</code> <p>Target tokens [batch, seq_len]</p> required <code>z_logits</code> <code>Tensor</code> <p>Encoder logits [batch, seq_len, latent_dim]</p> required <code>latent_dim</code> <code>int</code> <p>Latent dimension</p> <code>16</code> <code>beta_kl</code> <code>float</code> <p>Weight for KL term</p> <code>1.0</code> <code>free_bits</code> <code>float</code> <p>Free bits budget</p> <code>0.3466</code> <code>ignore_index</code> <code>int</code> <p>Index to ignore in reconstruction loss</p> <code>-100</code> <p>Returns:</p> Name Type Description <code>total_loss</code> <code>Tensor</code> <p>Combined loss</p> <code>metrics</code> <code>dict</code> <p>Dictionary of individual loss components</p> Source code in <code>src/free_transformer/losses.py</code> <pre><code>def compute_vae_loss(\n    logits: torch.Tensor,\n    targets: torch.Tensor,\n    z_logits: torch.Tensor,\n    latent_dim: int = 16,\n    beta_kl: float = 1.0,\n    free_bits: float = 0.3466,\n    ignore_index: int = -100,\n) -&gt; Tuple[torch.Tensor, dict]:\n    \"\"\"\n    Complete VAE loss for Free Transformer.\n\n    Loss = Reconstruction + beta * KL_with_free_bits\n\n    Args:\n        logits: Model predictions [batch, seq_len, vocab_size]\n        targets: Target tokens [batch, seq_len]\n        z_logits: Encoder logits [batch, seq_len, latent_dim]\n        latent_dim: Latent dimension\n        beta_kl: Weight for KL term\n        free_bits: Free bits budget\n        ignore_index: Index to ignore in reconstruction loss\n\n    Returns:\n        total_loss: Combined loss\n        metrics: Dictionary of individual loss components\n    \"\"\"\n    # Reconstruction loss\n    recon_loss = compute_reconstruction_loss(logits, targets, ignore_index)\n\n    # KL divergence loss with free bits\n    kl_loss = compute_kl_divergence(z_logits, latent_dim, free_bits)\n\n    # Combined loss\n    total_loss = recon_loss + beta_kl * kl_loss\n\n    # Metrics for logging\n    metrics = {\n        \"loss/total\": total_loss.item(),\n        \"loss/reconstruction\": recon_loss.item(),\n        \"loss/kl\": kl_loss.item(),\n        \"loss/kl_weighted\": (beta_kl * kl_loss).item(),\n    }\n\n    # Compute perplexity\n    with torch.no_grad():\n        perplexity = torch.exp(recon_loss)\n        metrics[\"metrics/perplexity\"] = perplexity.item()\n\n    return total_loss, metrics\n</code></pre>"},{"location":"api/model/","title":"Model API Reference","text":""},{"location":"api/model/#free_transformer.model.FreeTransformer","title":"<code>free_transformer.model.FreeTransformer(config)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Free Transformer: Conditional VAE-based language model with latent planning.</p> <p>Implements the architecture from the Free Transformer paper with: - Split decoder stack (first half for context, second half for generation) - Non-causal encoder for latent plan inference - Binary mapper for differentiable discrete sampling - Injection mechanism for plan integration</p> Source code in <code>src/free_transformer/model.py</code> <pre><code>def __init__(self, config):\n    super().__init__()\n    self.config = config\n\n    # Token embeddings\n    self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n\n    # First half of decoder blocks (context processing)\n    self.first_half_blocks = nn.ModuleList(\n        [\n            TransformerBlock(\n                dim=config.hidden_dim,\n                num_heads=config.num_heads,\n                num_kv_heads=config.num_kv_heads,\n                ffn_dim=config.ffn_hidden_dim,\n                dropout=config.dropout,\n                causal=True,\n                use_rope=config.use_rope,\n                max_seq_len=config.max_seq_len,\n            )\n            for _ in range(config.split_layer)\n        ]\n    )\n\n    # Encoder module (non-causal, for plan inference)\n    self.encoder = EncoderBlock(\n        dim=config.hidden_dim,\n        num_heads=config.num_heads,\n        ffn_dim=config.ffn_hidden_dim,\n        latent_dim=config.latent_dim,\n        dropout=config.dropout,\n    )\n\n    # Binary mapper and latent plan handler\n    self.latent_plan = LatentPlan(\n        latent_dim=config.latent_dim,\n        hidden_dim=config.hidden_dim,\n    )\n\n    # Injection mechanism\n    self.injection = InjectionMechanism(config.hidden_dim)\n\n    # Second half of decoder blocks (generation with plan)\n    second_half_layers = config.num_layers - config.split_layer\n    if second_half_layers &lt;= 0:\n        raise ValueError(\n            f\"split_layer ({config.split_layer}) must be less than num_layers ({config.num_layers})\"\n        )\n\n    self.second_half_blocks = nn.ModuleList(\n        [\n            TransformerBlock(\n                dim=config.hidden_dim,\n                num_heads=config.num_heads,\n                num_kv_heads=config.num_kv_heads,\n                ffn_dim=config.ffn_hidden_dim,\n                dropout=config.dropout,\n                causal=True,\n                use_rope=config.use_rope,\n                max_seq_len=config.max_seq_len,\n            )\n            for _ in range(second_half_layers)\n        ]\n    )\n\n    # Final output\n    self.norm = RMSNorm(config.hidden_dim)\n    self.output = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n\n    # Tie weights\n    self.output.weight = self.token_embedding.weight\n\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"api/model/#free_transformer.model.FreeTransformer.forward","title":"<code>forward(tokens, mode='training')</code>","text":"<p>Forward pass with mode switching.</p> <p>Parameters:</p> Name Type Description Default <code>tokens</code> <code>Tensor</code> <p>Input token IDs [batch, seq_len]</p> required <code>mode</code> <code>Literal['training', 'inference']</code> <p>'training' uses encoder path, 'inference' samples random Z</p> <code>'training'</code> <p>Returns:</p> Name Type Description <code>logits</code> <code>Tensor</code> <p>Output logits [batch, seq_len, vocab_size]</p> <code>z_logits</code> <code>Optional[Tensor]</code> <p>Encoder logits for Z (only in training mode) [batch, seq_len, latent_dim]</p> Source code in <code>src/free_transformer/model.py</code> <pre><code>def forward(\n    self,\n    tokens: torch.Tensor,\n    mode: Literal[\"training\", \"inference\"] = \"training\",\n) -&gt; Tuple[torch.Tensor, Optional[torch.Tensor]]:\n    \"\"\"\n    Forward pass with mode switching.\n\n    Args:\n        tokens: Input token IDs [batch, seq_len]\n        mode: 'training' uses encoder path, 'inference' samples random Z\n\n    Returns:\n        logits: Output logits [batch, seq_len, vocab_size]\n        z_logits: Encoder logits for Z (only in training mode) [batch, seq_len, latent_dim]\n    \"\"\"\n    batch_size, seq_len = tokens.shape\n\n    # 1. Embed tokens\n    x = self.token_embedding(tokens)\n\n    # 2. Process through first half of decoder\n    for block in self.first_half_blocks:\n        x = block(x)\n\n    # 3. Generate or sample latent plan Z\n    if mode == \"training\":\n        # Encoder path: infer Z from context\n        z_logits = self.encoder(x)  # [batch, seq_len, latent_dim]\n        z_onehot = self.latent_plan.sample_from_logits(\n            z_logits\n        )  # [batch, seq_len, 2^latent_dim]\n    else:  # inference\n        # Sample Z from uniform prior\n        z_onehot = self.latent_plan.sample_from_prior(batch_size, seq_len, device=tokens.device)\n        z_logits = None\n\n    # 4. Project Z to hidden dimension and inject into decoder\n    z_projected = self.latent_plan.project_to_hidden(z_onehot)\n    x_with_z = self.injection(x, z_projected)\n\n    # 5. Process through second half of decoder\n    # First block uses injected kv, rest use standard self-attention\n    x = self.second_half_blocks[0](x, kv_input=x_with_z)\n    for block in self.second_half_blocks[1:]:\n        x = block(x)\n\n    # 6. Final output projection\n    x = self.norm(x)\n    logits = self.output(x)\n\n    assert isinstance(logits, torch.Tensor)\n    return logits, z_logits\n</code></pre>"},{"location":"api/model/#free_transformer.model.FreeTransformer.generate","title":"<code>generate(prompt_tokens, max_new_tokens=100, temperature=1.0, top_k=None)</code>","text":"<p>Autoregressive generation with random latent plans.</p> <p>Parameters:</p> Name Type Description Default <code>prompt_tokens</code> <code>Tensor</code> <p>Initial tokens [batch, prompt_len]</p> required <code>max_new_tokens</code> <code>int</code> <p>Number of tokens to generate</p> <code>100</code> <code>temperature</code> <code>float</code> <p>Sampling temperature</p> <code>1.0</code> <code>top_k</code> <code>Optional[int]</code> <p>Top-k filtering</p> <code>None</code> <p>Returns:</p> Type Description <code>Tensor</code> <p>Generated tokens [batch, prompt_len + max_new_tokens]</p> Source code in <code>src/free_transformer/model.py</code> <pre><code>@torch.no_grad()\ndef generate(\n    self,\n    prompt_tokens: torch.Tensor,\n    max_new_tokens: int = 100,\n    temperature: float = 1.0,\n    top_k: Optional[int] = None,\n) -&gt; torch.Tensor:\n    \"\"\"\n    Autoregressive generation with random latent plans.\n\n    Args:\n        prompt_tokens: Initial tokens [batch, prompt_len]\n        max_new_tokens: Number of tokens to generate\n        temperature: Sampling temperature\n        top_k: Top-k filtering\n\n    Returns:\n        Generated tokens [batch, prompt_len + max_new_tokens]\n    \"\"\"\n    tokens = prompt_tokens\n\n    for _ in range(max_new_tokens):\n        # Forward pass in inference mode\n        logits, _ = self.forward(tokens, mode=\"inference\")\n\n        # Get logits for last position\n        logits = logits[:, -1, :] / temperature\n\n        # Top-k filtering\n        if top_k is not None:\n            v, _ = torch.topk(logits, min(top_k, logits.size(-1)))\n            logits[logits &lt; v[:, [-1]]] = float(\"-inf\")\n\n        # Sample next token\n        probs = F.softmax(logits, dim=-1)\n        next_token = torch.multinomial(probs, num_samples=1)\n\n        # Append to sequence\n        tokens = torch.cat([tokens, next_token], dim=1)\n\n        # Truncate if exceeds max length\n        if tokens.shape[1] &gt; self.config.max_seq_len:\n            tokens = tokens[:, -self.config.max_seq_len :]\n\n    return tokens\n</code></pre>"},{"location":"api/model/#free_transformer.baseline.TransformerBaseline","title":"<code>free_transformer.baseline.TransformerBaseline(config)</code>","text":"<p>               Bases: <code>Module</code></p> <p>Standard autoregressive Transformer without latent planning.</p> Source code in <code>src/free_transformer/baseline.py</code> <pre><code>def __init__(self, config):\n    super().__init__()\n    self.config = config\n\n    self.token_embedding = nn.Embedding(config.vocab_size, config.hidden_dim)\n\n    self.blocks = nn.ModuleList(\n        [\n            TransformerBlock(\n                dim=config.hidden_dim,\n                num_heads=config.num_heads,\n                num_kv_heads=config.num_kv_heads,\n                ffn_dim=config.ffn_hidden_dim,\n                dropout=config.dropout,\n                causal=True,\n                use_rope=config.use_rope,\n                max_seq_len=config.max_seq_len,\n            )\n            for _ in range(config.num_layers)\n        ]\n    )\n\n    self.norm = RMSNorm(config.hidden_dim)\n    self.output = nn.Linear(config.hidden_dim, config.vocab_size, bias=False)\n    self.output.weight = self.token_embedding.weight\n\n    self.apply(self._init_weights)\n</code></pre>"},{"location":"api/training/","title":"Training Utilities API Reference","text":""},{"location":"api/training/#free_transformer.train_utils","title":"<code>free_transformer.train_utils</code>","text":"<p>Training utilities including FSDP and DeepSpeed support.</p>"},{"location":"api/training/#free_transformer.train_utils.LRScheduler","title":"<code>LRScheduler(optimizer, warmup_steps, max_lr, min_lr=0.0)</code>","text":"<p>Learning rate scheduler with warmup.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def __init__(\n    self,\n    optimizer: torch.optim.Optimizer,\n    warmup_steps: int,\n    max_lr: float,\n    min_lr: float = 0.0,\n):\n    self.optimizer = optimizer\n    self.warmup_steps = warmup_steps\n    self.max_lr = max_lr\n    self.min_lr = min_lr\n    self.step_count = 0\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.LRScheduler.step","title":"<code>step()</code>","text":"<p>Update learning rate.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def step(self):\n    \"\"\"Update learning rate.\"\"\"\n    self.step_count += 1\n\n    if self.step_count &lt; self.warmup_steps:\n        # Linear warmup\n        lr = self.max_lr * self.step_count / self.warmup_steps\n    else:\n        # Cosine decay\n        progress = (self.step_count - self.warmup_steps) / self.warmup_steps\n        lr = self.min_lr + (self.max_lr - self.min_lr) * 0.5 * (\n            1 + torch.cos(torch.tensor(progress * 3.14159))\n        )\n\n    for param_group in self.optimizer.param_groups:\n        param_group[\"lr\"] = lr\n\n    return lr\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.Trainer","title":"<code>Trainer(model, optimizer, device, use_amp=False)</code>","text":"<p>Base trainer class.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def __init__(\n    self,\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    device: torch.device,\n    use_amp: bool = False,\n):\n    self.model = model\n    self.optimizer = optimizer\n    self.device = device\n    self.use_amp = use_amp\n\n    if use_amp:\n        # Use the new torch.amp API with device detection\n        if device.type == \"cuda\":\n            from torch.cuda.amp import GradScaler\n\n            self.scaler: Optional[\"GradScaler\"] = GradScaler()\n        elif device.type == \"mps\":\n            # MPS doesn't support GradScaler, disable AMP\n            self.use_amp = False\n            self.scaler = None\n        else:\n            # CPU doesn't support GradScaler, disable AMP\n            self.use_amp = False\n            self.scaler = None\n    else:\n        self.scaler = None\n\n    self.step = 0\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.Trainer.eval_step","title":"<code>eval_step(batch)</code>","text":"<p>Single evaluation step.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>@torch.no_grad()\ndef eval_step(self, batch: tuple) -&gt; dict[str, float]:\n    \"\"\"Single evaluation step.\"\"\"\n    self.model.eval()\n    loss, metrics = self._compute_loss(batch)\n    return metrics\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.Trainer.train_step","title":"<code>train_step(batch, grad_clip=None)</code>","text":"<p>Single training step.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def train_step(\n    self,\n    batch: tuple,\n    grad_clip: Optional[float] = None,\n) -&gt; dict:\n    \"\"\"Single training step.\"\"\"\n    self.model.train()\n    self.optimizer.zero_grad()\n\n    # Forward pass with automatic mixed precision\n    if self.use_amp and self.scaler is not None:\n        with torch.amp.autocast(device_type=self.device.type):\n            loss, metrics = self._compute_loss(batch)\n\n        # Backward pass with gradient scaling\n        self.scaler.scale(loss).backward()\n\n        if grad_clip is not None:\n            self.scaler.unscale_(self.optimizer)\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n\n        self.scaler.step(self.optimizer)\n        self.scaler.update()\n    else:\n        loss, metrics = self._compute_loss(batch)\n        loss.backward()\n\n        if grad_clip is not None:\n            torch.nn.utils.clip_grad_norm_(self.model.parameters(), grad_clip)\n\n        self.optimizer.step()\n\n    self.step += 1\n    return metrics\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.cleanup_distributed","title":"<code>cleanup_distributed()</code>","text":"<p>Cleanup distributed training.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def cleanup_distributed():\n    \"\"\"Cleanup distributed training.\"\"\"\n    if torch.distributed.is_initialized():\n        torch.distributed.destroy_process_group()\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.count_parameters","title":"<code>count_parameters(model)</code>","text":"<p>Count trainable parameters.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def count_parameters(model: nn.Module) -&gt; int:\n    \"\"\"Count trainable parameters.\"\"\"\n    return sum(p.numel() for p in model.parameters() if p.requires_grad)\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.load_checkpoint","title":"<code>load_checkpoint(model, optimizer, path)</code>","text":"<p>Load training checkpoint with better error handling.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def load_checkpoint(\n    model: nn.Module,\n    optimizer: Optional[torch.optim.Optimizer],\n    path: str,\n) -&gt; int:\n    \"\"\"Load training checkpoint with better error handling.\"\"\"\n    try:\n        checkpoint = torch.load(path, map_location=\"cpu\")\n    except Exception as e:\n        raise RuntimeError(f\"Failed to load checkpoint from {path}: {e}\")\n\n    if not isinstance(checkpoint, dict) or \"model_state_dict\" not in checkpoint:\n        raise RuntimeError(f\"Checkpoint at {path} does not contain 'model_state_dict'\")\n\n    try:\n        model.load_state_dict(checkpoint[\"model_state_dict\"])\n    except RuntimeError as e:\n        # Provide more helpful error message\n        model_keys = set(model.state_dict().keys())\n        checkpoint_keys = set(checkpoint[\"model_state_dict\"].keys())\n\n        missing_in_checkpoint = model_keys - checkpoint_keys\n        missing_in_model = checkpoint_keys - model_keys\n\n        error_msg = f\"State dict mismatch when loading {path}:\\n\"\n        if missing_in_checkpoint:\n            error_msg += f\"  Missing in checkpoint: {sorted(list(missing_in_checkpoint))[:5]}...\\n\"\n        if missing_in_model:\n            error_msg += f\"  Missing in model: {sorted(list(missing_in_model))[:5]}...\\n\"\n        error_msg += f\"  Original error: {e}\"\n\n        raise RuntimeError(error_msg)\n\n    if optimizer is not None and \"optimizer_state_dict\" in checkpoint:\n        optimizer.load_state_dict(checkpoint[\"optimizer_state_dict\"])\n\n    return int(checkpoint.get(\"step\", 0))\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.save_checkpoint","title":"<code>save_checkpoint(model, optimizer, step, path, metadata=None)</code>","text":"<p>Save training checkpoint.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def save_checkpoint(\n    model: nn.Module,\n    optimizer: torch.optim.Optimizer,\n    step: int,\n    path: str,\n    metadata: Optional[dict] = None,\n):\n    \"\"\"Save training checkpoint.\"\"\"\n    checkpoint = {\n        \"model_state_dict\": model.state_dict(),\n        \"optimizer_state_dict\": optimizer.state_dict(),\n        \"step\": step,\n    }\n    if metadata is not None:\n        checkpoint.update(metadata)\n\n    torch.save(checkpoint, path)\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.setup_distributed","title":"<code>setup_distributed()</code>","text":"<p>Initialize distributed training.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def setup_distributed():\n    \"\"\"Initialize distributed training.\"\"\"\n    if \"RANK\" in os.environ and \"WORLD_SIZE\" in os.environ:\n        rank = int(os.environ[\"RANK\"])\n        world_size = int(os.environ[\"WORLD_SIZE\"])\n        local_rank = int(os.environ.get(\"LOCAL_RANK\", 0))\n\n        torch.cuda.set_device(local_rank)\n        torch.distributed.init_process_group(\n            backend=\"nccl\", init_method=\"env://\", world_size=world_size, rank=rank\n        )\n        return rank, world_size, local_rank\n    else:\n        return 0, 1, 0\n</code></pre>"},{"location":"api/training/#free_transformer.train_utils.wrap_model_fsdp","title":"<code>wrap_model_fsdp(model, mixed_precision=True, min_num_params=1000000)</code>","text":"<p>Wrap model with FSDP for distributed training.</p> Source code in <code>src/free_transformer/train_utils.py</code> <pre><code>def wrap_model_fsdp(\n    model: nn.Module,\n    mixed_precision: bool = True,\n    min_num_params: int = 1000000,\n) -&gt; FSDP:\n    \"\"\"Wrap model with FSDP for distributed training.\"\"\"\n\n    # Mixed precision policy\n    mp_policy = None\n    if mixed_precision:\n        mp_policy = MixedPrecision(\n            param_dtype=torch.bfloat16,\n            reduce_dtype=torch.bfloat16,\n            buffer_dtype=torch.bfloat16,\n        )\n\n    # Auto wrap policy\n    auto_wrap_policy = functools.partial(size_based_auto_wrap_policy, min_num_params=min_num_params)\n\n    # Wrap with FSDP\n    model = FSDP(\n        model,\n        auto_wrap_policy=auto_wrap_policy,\n        mixed_precision=mp_policy,\n        device_id=torch.cuda.current_device(),\n    )\n\n    return model\n</code></pre>"},{"location":"architecture/baseline/","title":"Baseline Transformer Architecture","text":"<p>This page describes the baseline Transformer model used for comparison with the Free Transformer.</p>"},{"location":"architecture/baseline/#overview","title":"Overview","text":"<p>The baseline model is a standard decoder-only Transformer architecture based on the Llama design. It serves as a control to evaluate the benefits of the Free Transformer's latent planning mechanism.</p>"},{"location":"architecture/baseline/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/baseline/#1-decoder-only-design","title":"1. Decoder-Only Design","text":"<p>The baseline follows the GPT/Llama paradigm: - Autoregressive generation: Predicts next token based on previous tokens - Causal attention: Each position can only attend to previous positions - Standard training: Cross-entropy loss on next-token prediction</p>"},{"location":"architecture/baseline/#2-modern-optimizations","title":"2. Modern Optimizations","text":""},{"location":"architecture/baseline/#rmsnorm","title":"RMSNorm","text":"<p>Instead of LayerNorm, uses Root Mean Square Layer Normalization: <pre><code>def rms_norm(x, weight, eps=1e-6):\n    variance = x.pow(2).mean(-1, keepdim=True)\n    x = x * torch.rsqrt(variance + eps)\n    return weight * x\n</code></pre></p> <p>Benefits: - More stable training - Better performance on large models - Simpler computation</p>"},{"location":"architecture/baseline/#swiglu-activation","title":"SwiGLU Activation","text":"<p>Uses SwiGLU instead of standard ReLU/GELU: <pre><code>def swiglu(x):\n    x, gate = x.chunk(2, dim=-1)\n    return F.silu(gate) * x\n</code></pre></p> <p>Benefits: - Better performance than ReLU/GELU - Gating mechanism improves expressivity - Used in modern large language models</p>"},{"location":"architecture/baseline/#rotary-position-embedding-rope","title":"Rotary Position Embedding (RoPE)","text":"<p>Encodes position information through rotation: <pre><code>def apply_rope(q, k, cos, sin):\n    q_rot = (q * cos) + (rotate_half(q) * sin)\n    k_rot = (k * cos) + (rotate_half(k) * sin)\n    return q_rot, k_rot\n</code></pre></p> <p>Benefits: - Better length extrapolation - Relative position encoding - No learned position embeddings needed</p>"},{"location":"architecture/baseline/#grouped-query-attention-gqa","title":"Grouped-Query Attention (GQA)","text":"<p>Reduces memory usage while maintaining performance: <pre><code># Standard: num_heads key-value heads\n# GQA: num_key_value_heads &lt; num_heads\n# Queries are grouped to share key-value pairs\n</code></pre></p> <p>Benefits: - Reduced memory usage - Faster inference - Minimal performance loss</p>"},{"location":"architecture/baseline/#model-architecture","title":"Model Architecture","text":"<pre><code>graph TB\n    A[Input Tokens] --&gt; B[Token Embeddings]\n    B --&gt; C[Decoder Block 1]\n    C --&gt; D[Decoder Block 2]\n    D --&gt; E[...]\n    E --&gt; F[Decoder Block L]\n    F --&gt; G[RMSNorm]\n    G --&gt; H[Language Modeling Head]\n    H --&gt; I[Output Logits]\n\n    subgraph \"Decoder Block\"\n        J[Input] --&gt; K[RMSNorm]\n        K --&gt; L[Multi-Head Attention]\n        L --&gt; M[Residual Connection]\n        J --&gt; M\n        M --&gt; N[RMSNorm]\n        N --&gt; O[SwiGLU FFN]\n        O --&gt; P[Residual Connection]\n        M --&gt; P\n        P --&gt; Q[Output]\n    end</code></pre>"},{"location":"architecture/baseline/#implementation-details","title":"Implementation Details","text":""},{"location":"architecture/baseline/#attention-mechanism","title":"Attention Mechanism","text":"<pre><code>class GroupedQueryAttention(nn.Module):\n    def __init__(self, config):\n        self.num_heads = config.num_heads\n        self.num_key_value_heads = config.num_key_value_heads\n        self.head_dim = config.hidden_dim // config.num_heads\n\n        self.q_proj = nn.Linear(config.hidden_dim, config.num_heads * self.head_dim)\n        self.k_proj = nn.Linear(config.hidden_dim, config.num_key_value_heads * self.head_dim)\n        self.v_proj = nn.Linear(config.hidden_dim, config.num_key_value_heads * self.head_dim)\n        self.o_proj = nn.Linear(config.num_heads * self.head_dim, config.hidden_dim)\n\n    def forward(self, hidden_states, attention_mask=None):\n        # Compute queries, keys, values\n        queries = self.q_proj(hidden_states)\n        keys = self.k_proj(hidden_states)\n        values = self.v_proj(hidden_states)\n\n        # Reshape and apply attention\n        # ... (standard attention computation)\n\n        return output\n</code></pre>"},{"location":"architecture/baseline/#feed-forward-network","title":"Feed-Forward Network","text":"<pre><code>class SwiGLU(nn.Module):\n    def __init__(self, config):\n        self.gate_proj = nn.Linear(config.hidden_dim, config.intermediate_size)\n        self.up_proj = nn.Linear(config.hidden_dim, config.intermediate_size)\n        self.down_proj = nn.Linear(config.intermediate_size, config.hidden_dim)\n\n    def forward(self, x):\n        gate = self.gate_proj(x)\n        up = self.up_proj(x)\n        return self.down_proj(F.silu(gate) * up)\n</code></pre>"},{"location":"architecture/baseline/#training-characteristics","title":"Training Characteristics","text":""},{"location":"architecture/baseline/#loss-function","title":"Loss Function","text":"<p>Standard cross-entropy loss for next-token prediction: <pre><code>def compute_loss(logits, targets):\n    shift_logits = logits[..., :-1, :].contiguous()\n    shift_labels = targets[..., 1:].contiguous()\n\n    loss = F.cross_entropy(\n        shift_logits.view(-1, shift_logits.size(-1)),\n        shift_labels.view(-1),\n        ignore_index=-100\n    )\n    return loss\n</code></pre></p>"},{"location":"architecture/baseline/#training-dynamics","title":"Training Dynamics","text":"<ul> <li>Stable training: Modern components provide stable gradients</li> <li>Efficient scaling: Works well with large models and datasets</li> <li>Fast convergence: Good optimization properties</li> </ul>"},{"location":"architecture/baseline/#performance-characteristics","title":"Performance Characteristics","text":""},{"location":"architecture/baseline/#memory-usage","title":"Memory Usage","text":"<ul> <li>Parameters: ~7N parameters for hidden dimension N</li> <li>Activation memory: O(batch_size \u00d7 seq_len \u00d7 hidden_dim)</li> <li>Attention memory: O(batch_size \u00d7 num_heads \u00d7 seq_len\u00b2)</li> </ul>"},{"location":"architecture/baseline/#computational-complexity","title":"Computational Complexity","text":"<ul> <li>Forward pass: O(seq_len \u00d7 hidden_dim\u00b2) per layer</li> <li>Attention: O(seq_len\u00b2 \u00d7 hidden_dim) per layer</li> <li>Total: O(num_layers \u00d7 seq_len \u00d7 hidden_dim\u00b2)</li> </ul>"},{"location":"architecture/baseline/#comparison-with-free-transformer","title":"Comparison with Free Transformer","text":"Aspect Baseline Transformer Free Transformer Architecture Decoder-only Decoder + Encoder Parameters ~7N (for hidden_dim N) ~7N + encoder params Training Loss Cross-entropy Cross-entropy + KL Generation Autoregressive Plan-conditioned Memory (Training) Baseline +30-40% Memory (Inference) Baseline +10-15% Speed (Training) Baseline -20-30% Speed (Inference) Baseline -5-10%"},{"location":"architecture/baseline/#use-cases","title":"Use Cases","text":""},{"location":"architecture/baseline/#when-to-use-baseline","title":"When to Use Baseline","text":"<ul> <li>Maximum efficiency: When computational resources are limited</li> <li>Simple tasks: Short sequences, simple patterns</li> <li>Established workflows: When using existing training pipelines</li> <li>Benchmarking: As a control for evaluating improvements</li> </ul>"},{"location":"architecture/baseline/#limitations","title":"Limitations","text":"<ul> <li>Local coherence: Struggles with long-range dependencies</li> <li>Limited controllability: Hard to guide generation</li> <li>Reactive generation: No explicit planning mechanism</li> </ul>"},{"location":"architecture/baseline/#configuration","title":"Configuration","text":""},{"location":"architecture/baseline/#model-configuration","title":"Model Configuration","text":"<pre><code>model:\n  vocab_size: 50000\n  hidden_dim: 512\n  num_layers: 12\n  num_heads: 8\n  num_key_value_heads: 2\n  intermediate_size: 2048\n  max_seq_len: 1024\n  dropout: 0.1\n  rms_norm_eps: 1e-6\n  rope_theta: 10000.0\n</code></pre>"},{"location":"architecture/baseline/#training-configuration","title":"Training Configuration","text":"<pre><code>training:\n  batch_size: 32\n  learning_rate: 1e-4\n  num_epochs: 10\n  weight_decay: 0.01\n  warmup_steps: 1000\n  gradient_clip_norm: 1.0\n</code></pre>"},{"location":"architecture/baseline/#implementation-example","title":"Implementation Example","text":"<pre><code>from free_transformer import BaselineTransformer, ModelConfig\n\n# Create configuration\nconfig = ModelConfig(\n    vocab_size=50000,\n    hidden_dim=512,\n    num_layers=12,\n    num_heads=8,\n    max_seq_len=1024\n)\n\n# Initialize model\nmodel = BaselineTransformer(config)\n\n# Training\nmodel.train()\nlogits = model(input_ids)\nloss = F.cross_entropy(logits.view(-1, config.vocab_size), targets.view(-1))\n\n# Generation\nmodel.eval()\ngenerated = model.generate(prompt, max_new_tokens=100)\n</code></pre>"},{"location":"architecture/baseline/#next-steps","title":"Next Steps","text":"<ul> <li>Free Transformer: Compare with the enhanced architecture</li> <li>Training Guide: Learn how to train both models</li> <li>Examples: See practical usage examp</li> </ul>"},{"location":"architecture/free-transformer/","title":"Free Transformer Architecture","text":"<p>This page provides a detailed technical explanation of the Free Transformer architecture and its components.</p>"},{"location":"architecture/free-transformer/#core-architecture","title":"Core Architecture","text":"<p>The Free Transformer extends the standard decoder-only Transformer with a latent planning mechanism. The key innovation is the introduction of an explicit latent variable <code>Z</code> that represents an abstract \"plan\" for the entire sequence.</p> <pre><code>graph TB\n    subgraph \"Input Processing\"\n        A[Input Tokens] --&gt; B[Token Embeddings + Position Encoding]\n    end\n\n    subgraph \"Early Decoder Layers\"\n        B --&gt; C[Decoder Block 1]\n        C --&gt; D[Decoder Block 2]\n        D --&gt; E[...]\n        E --&gt; F[Decoder Block L/2]\n    end\n\n    subgraph \"Latent Planning System\"\n        F --&gt; G[Non-Causal Encoder]\n        G --&gt; H[Latent Variable Z]\n        H --&gt; I[Plan Injection Layer]\n    end\n\n    subgraph \"Late Decoder Layers\"\n        F --&gt; J[Decoder Block L/2+1]\n        I --&gt; J\n        J --&gt; K[Decoder Block L/2+2]\n        K --&gt; L[...]\n        L --&gt; M[Decoder Block L]\n    end\n\n    subgraph \"Output\"\n        M --&gt; N[Language Modeling Head]\n        N --&gt; O[Token Logits]\n        H --&gt; P[Latent Logits]\n    end</code></pre>"},{"location":"architecture/free-transformer/#detailed-components","title":"Detailed Components","text":""},{"location":"architecture/free-transformer/#1-decoder-backbone","title":"1. Decoder Backbone","text":"<p>The Free Transformer uses a Llama-style decoder architecture with modern optimizations:</p>"},{"location":"architecture/free-transformer/#layer-structure","title":"Layer Structure","text":"<pre><code>class DecoderBlock(nn.Module):\n    def __init__(self, config):\n        self.attention = GroupedQueryAttention(config)\n        self.feed_forward = SwiGLU(config)\n        self.attention_norm = RMSNorm(config.hidden_dim)\n        self.ffn_norm = RMSNorm(config.hidden_dim)\n\n    def forward(self, x, attention_mask=None):\n        # Pre-norm attention\n        normed_x = self.attention_norm(x)\n        attn_out = self.attention(normed_x, attention_mask)\n        x = x + attn_out\n\n        # Pre-norm feed-forward\n        normed_x = self.ffn_norm(x)\n        ffn_out = self.feed_forward(normed_x)\n        x = x + ffn_out\n\n        return x\n</code></pre>"},{"location":"architecture/free-transformer/#key-features","title":"Key Features","text":"<ul> <li>RMSNorm: More stable than LayerNorm, especially for large models</li> <li>SwiGLU: Gated linear unit activation that outperforms ReLU/GELU</li> <li>Grouped-Query Attention: Reduces memory usage while maintaining performance</li> <li>RoPE: Rotary Position Embedding for better length extrapolation</li> </ul>"},{"location":"architecture/free-transformer/#2-non-causal-encoder","title":"2. Non-Causal Encoder","text":"<p>The encoder is the key component that enables latent planning:</p> <pre><code>class NonCausalEncoder(nn.Module):\n    def __init__(self, config):\n        self.layers = nn.ModuleList([\n            EncoderBlock(config) for _ in range(config.encoder_layers)\n        ])\n        self.learned_query = nn.Parameter(torch.randn(config.hidden_dim))\n        self.readout = nn.Linear(config.hidden_dim, config.hidden_dim)\n\n    def forward(self, hidden_states):\n        # Use learned query to aggregate information\n        query = self.learned_query.unsqueeze(0).expand(\n            hidden_states.size(0), 1, -1\n        )\n\n        # Non-causal attention over entire sequence\n        for layer in self.layers:\n            query = layer(query, hidden_states)\n\n        # Readout to latent space\n        latent_repr = self.readout(query.squeeze(1))\n        return latent_repr\n</code></pre>"},{"location":"architecture/free-transformer/#design-principles","title":"Design Principles","text":"<ul> <li>Non-causal: Can attend to the entire sequence, not just previous tokens</li> <li>Learned query: Single learnable vector that aggregates sequence information</li> <li>Separate parameters: Independent from decoder to avoid interference</li> </ul>"},{"location":"architecture/free-transformer/#3-binary-latent-mapping","title":"3. Binary Latent Mapping","text":"<p>The continuous encoder output is mapped to a discrete binary plan:</p> <pre><code>class BinaryMapper(nn.Module):\n    def __init__(self, config):\n        self.projection = nn.Linear(config.hidden_dim, config.latent_dim)\n        self.temperature = config.gumbel_temperature\n\n    def forward(self, encoder_output, training=True):\n        # Project to latent dimension\n        logits = self.projection(encoder_output)\n\n        if training:\n            # Gumbel-Softmax for differentiable sampling\n            binary_soft = F.gumbel_softmax(\n                torch.stack([logits, -logits], dim=-1),\n                tau=self.temperature,\n                hard=True\n            )\n            return binary_soft[..., 0], logits\n        else:\n            # Hard sampling for inference\n            binary_hard = (logits &gt; 0).float()\n            return binary_hard, logits\n</code></pre>"},{"location":"architecture/free-transformer/#key-features_1","title":"Key Features","text":"<ul> <li>Gumbel-Softmax: Enables gradient flow through discrete sampling</li> <li>Hard sampling: Uses actual binary values during inference</li> <li>Temperature control: Balances between discrete and continuous</li> </ul>"},{"location":"architecture/free-transformer/#4-plan-injection-mechanism","title":"4. Plan Injection Mechanism","text":"<p>The binary plan is injected into the decoder representations:</p> <pre><code>class PlanInjection(nn.Module):\n    def __init__(self, config):\n        self.plan_projection = nn.Linear(\n            config.latent_dim, \n            config.hidden_dim\n        )\n        self.gate = nn.Linear(config.hidden_dim, config.hidden_dim)\n\n    def forward(self, decoder_hidden, binary_plan):\n        # Project plan to hidden dimension\n        plan_repr = self.plan_projection(binary_plan)\n\n        # Expand to sequence length\n        seq_len = decoder_hidden.size(1)\n        plan_repr = plan_repr.unsqueeze(1).expand(-1, seq_len, -1)\n\n        # Gated injection\n        gate_values = torch.sigmoid(self.gate(decoder_hidden))\n        injected = decoder_hidden + gate_values * plan_repr\n\n        return injected\n</code></pre>"},{"location":"architecture/free-transformer/#injection-strategies","title":"Injection Strategies","text":"<ol> <li>Additive: <code>hidden + plan_projection(z)</code></li> <li>Gated: <code>hidden + gate * plan_projection(z)</code></li> <li>Concatenation: <code>concat(hidden, plan_projection(z))</code></li> <li>Cross-attention: Plan as keys/values in attention</li> </ol>"},{"location":"architecture/free-transformer/#training-dynamics","title":"Training Dynamics","text":""},{"location":"architecture/free-transformer/#forward-pass-flow","title":"Forward Pass Flow","text":"<ol> <li>Input Processing: Tokenize and embed input sequence</li> <li>Early Decoding: Process through first L/2 decoder layers</li> <li>Encoding: Non-causal encoder creates sequence representation</li> <li>Latent Mapping: Map to binary plan Z</li> <li>Plan Injection: Inject plan into decoder hidden states</li> <li>Late Decoding: Process through remaining L/2 decoder layers</li> <li>Output: Generate token logits and latent logits</li> </ol>"},{"location":"architecture/free-transformer/#loss-computation","title":"Loss Computation","text":"<p>The model is trained with a composite VAE loss:</p> <pre><code>def compute_loss(logits, z_logits, targets, config):\n    # Reconstruction loss (standard language modeling)\n    recon_loss = F.cross_entropy(\n        logits.view(-1, config.vocab_size),\n        targets.view(-1),\n        ignore_index=-100\n    )\n\n    # KL divergence loss (regularization)\n    # Assuming uniform prior p(z) = 0.5 for each bit\n    posterior = torch.sigmoid(z_logits)\n    prior = torch.full_like(posterior, 0.5)\n\n    kl_loss = F.kl_div(\n        torch.log(posterior + 1e-8),\n        prior,\n        reduction='batchmean'\n    )\n\n    # Free bits regularization\n    kl_loss = torch.clamp(kl_loss, min=config.free_bits)\n\n    # Total loss\n    total_loss = recon_loss + config.kl_weight * kl_loss\n\n    return {\n        'total_loss': total_loss,\n        'recon_loss': recon_loss,\n        'kl_loss': kl_loss\n    }\n</code></pre>"},{"location":"architecture/free-transformer/#inference-mode","title":"Inference Mode","text":"<p>During inference, the model operates differently:</p> <ol> <li>No Encoder: The encoder is not used during generation</li> <li>Prior Sampling: Sample Z from uniform prior distribution</li> <li>Plan Injection: Inject sampled plan into decoder</li> <li>Generation: Standard autoregressive generation</li> </ol> <pre><code>def inference_forward(self, input_ids, max_new_tokens=100):\n    batch_size = input_ids.size(0)\n\n    # Sample latent plan from prior\n    z_sample = torch.bernoulli(\n        torch.full((batch_size, self.config.latent_dim), 0.5)\n    ).to(input_ids.device)\n\n    # Process through early layers\n    hidden = self.embed_tokens(input_ids)\n    for layer in self.early_layers:\n        hidden = layer(hidden)\n\n    # Inject plan\n    hidden = self.plan_injection(hidden, z_sample)\n\n    # Process through late layers\n    for layer in self.late_layers:\n        hidden = layer(hidden)\n\n    # Generate tokens autoregressively\n    return self.generate_tokens(hidden, max_new_tokens)\n</code></pre>"},{"location":"architecture/free-transformer/#architectural-variants","title":"Architectural Variants","text":""},{"location":"architecture/free-transformer/#1-multi-layer-injection","title":"1. Multi-Layer Injection","text":"<p>Instead of single injection point, inject at multiple layers:</p> <pre><code>class MultiLayerInjection(nn.Module):\n    def __init__(self, config):\n        self.injection_layers = nn.ModuleList([\n            PlanInjection(config) \n            for _ in range(config.num_injection_layers)\n        ])\n\n    def forward(self, hidden_states_list, binary_plan):\n        injected_states = []\n        for hidden, injection in zip(hidden_states_list, self.injection_layers):\n            injected = injection(hidden, binary_plan)\n            injected_states.append(injected)\n        return injected_states\n</code></pre>"},{"location":"architecture/free-transformer/#2-hierarchical-plans","title":"2. Hierarchical Plans","text":"<p>Use multiple latent variables at different levels:</p> <pre><code>class HierarchicalPlanner(nn.Module):\n    def __init__(self, config):\n        self.global_encoder = NonCausalEncoder(config)\n        self.local_encoders = nn.ModuleList([\n            NonCausalEncoder(config) \n            for _ in range(config.num_local_plans)\n        ])\n\n    def forward(self, hidden_states):\n        # Global plan for entire sequence\n        global_plan = self.global_encoder(hidden_states)\n\n        # Local plans for subsequences\n        local_plans = []\n        seq_len = hidden_states.size(1)\n        chunk_size = seq_len // len(self.local_encoders)\n\n        for i, encoder in enumerate(self.local_encoders):\n            start_idx = i * chunk_size\n            end_idx = (i + 1) * chunk_size\n            chunk = hidden_states[:, start_idx:end_idx]\n            local_plan = encoder(chunk)\n            local_plans.append(local_plan)\n\n        return global_plan, local_plans\n</code></pre>"},{"location":"architecture/free-transformer/#3-continuous-latents","title":"3. Continuous Latents","text":"<p>Alternative to binary plans using continuous latent variables:</p> <pre><code>class ContinuousMapper(nn.Module):\n    def __init__(self, config):\n        self.mean_projection = nn.Linear(config.hidden_dim, config.latent_dim)\n        self.logvar_projection = nn.Linear(config.hidden_dim, config.latent_dim)\n\n    def forward(self, encoder_output, training=True):\n        mean = self.mean_projection(encoder_output)\n        logvar = self.logvar_projection(encoder_output)\n\n        if training:\n            # Reparameterization trick\n            std = torch.exp(0.5 * logvar)\n            eps = torch.randn_like(std)\n            z = mean + eps * std\n            return z, mean, logvar\n        else:\n            # Use mean during inference\n            return mean, mean, logvar\n</code></pre>"},{"location":"architecture/free-transformer/#performance-considerations","title":"Performance Considerations","text":""},{"location":"architecture/free-transformer/#memory-usage","title":"Memory Usage","text":"<ul> <li>Training: ~30-40% more memory than baseline due to encoder</li> <li>Inference: ~10-15% more memory due to latent computations</li> </ul>"},{"location":"architecture/free-transformer/#computational-overhead","title":"Computational Overhead","text":"<ul> <li>Training: ~20-30% slower due to encoder and VAE loss</li> <li>Inference: ~5-10% slower due to plan injection</li> </ul>"},{"location":"architecture/free-transformer/#optimization-strategies","title":"Optimization Strategies","text":"<ol> <li>Gradient Checkpointing: Reduce memory at cost of computation</li> <li>Mixed Precision: Use bfloat16 for faster training</li> <li>Efficient Attention: Flash Attention integration (planned)</li> <li>Model Parallelism: FSDP for large models</li> </ol>"},{"location":"architecture/free-transformer/#next-steps","title":"Next Steps","text":"<ul> <li>Latent Planning: Deep dive into the planning mechanism</li> <li>Baseline Comparison: Compare with standard Transformers</li> <li>Training Guide: How to train effectively</li> </ul>"},{"location":"architecture/latent-planning/","title":"Latent Planning Mechanism","text":"<p>This page provides an in-depth explanation of the latent planning mechanism that makes the Free Transformer unique.</p>"},{"location":"architecture/latent-planning/#core-concept","title":"Core Concept","text":"<p>Traditional autoregressive models generate tokens sequentially, making decisions based only on the tokens seen so far. The Free Transformer introduces explicit latent planning - the model first creates an abstract \"plan\" for the entire sequence, then generates tokens to fulfill that plan.</p>"},{"location":"architecture/latent-planning/#planning-vs-reactive-generation","title":"Planning vs Reactive Generation","text":""},{"location":"architecture/latent-planning/#reactive-generation-standard-transformers","title":"Reactive Generation (Standard Transformers)","text":"<pre><code>Token 1 \u2192 Token 2 \u2192 Token 3 \u2192 ... \u2192 Token N\n   \u2191        \u2191        \u2191              \u2191\n   |        |        |              |\nContext  Context  Context       Context\n</code></pre> <p>Each token depends only on previous tokens, leading to: - Local coherence but potential global inconsistency - Difficulty with long-range planning - Limited controllability</p>"},{"location":"architecture/latent-planning/#plan-based-generation-free-transformer","title":"Plan-Based Generation (Free Transformer)","text":"<pre><code>Full Context \u2192 Abstract Plan Z \u2192 Token 1, Token 2, ..., Token N\n                     \u2193              \u2191       \u2191            \u2191\n                     \u2514\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2534\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2500\u2518\n</code></pre> <p>The model first creates a plan, then generates all tokens conditioned on that plan: - Global coherence through explicit planning - Better long-range dependencies - Controllable generation via plan manipulation</p>"},{"location":"architecture/latent-planning/#mathematical-formulation","title":"Mathematical Formulation","text":""},{"location":"architecture/latent-planning/#standard-autoregressive-model","title":"Standard Autoregressive Model","text":"\\[P(x_1, ..., x_T) = \\prod_{t=1}^T P(x_t | x_{&lt;t})\\]"},{"location":"architecture/latent-planning/#free-transformer-conditional-vae","title":"Free Transformer (Conditional VAE)","text":"\\[P(x_1, ..., x_T) = \\int P(x_1, ..., x_T | z) P(z) dz\\] <p>Where: - \\(z\\) is the latent plan variable - \\(P(z)\\) is the prior distribution (uniform for binary plans) - \\(P(x_1, ..., x_T | z)\\) is the conditional generation model</p>"},{"location":"architecture/latent-planning/#plan-representation","title":"Plan Representation","text":""},{"location":"architecture/latent-planning/#binary-plans","title":"Binary Plans","text":"<p>The Free Transformer uses binary latent variables: \\(\\(z \\in \\{0, 1\\}^d\\)\\)</p> <p>Where \\(d\\) is the latent dimension (typically 16-64).</p>"},{"location":"architecture/latent-planning/#why-binary","title":"Why Binary?","text":"<ol> <li>Interpretability: Each bit can represent a discrete choice</li> <li>Efficiency: Compact representation</li> <li>Controllability: Easy to manipulate specific aspects</li> <li>Stability: Avoids posterior collapse issues common with continuous latents</li> </ol>"},{"location":"architecture/latent-planning/#plan-semantics","title":"Plan Semantics","text":"<p>Each bit in the plan can potentially encode: - Style: Formal vs informal, technical vs casual - Structure: Narrative vs expository, linear vs non-linear - Content: Topic focus, emotional tone - Length: Short vs long form content</p>"},{"location":"architecture/latent-planning/#architecture-components","title":"Architecture Components","text":""},{"location":"architecture/latent-planning/#1-non-causal-encoder","title":"1. Non-Causal Encoder","text":"<p>The encoder creates the latent plan from the full sequence:</p> <pre><code>class NonCausalEncoder(nn.Module):\n    def __init__(self, config):\n        self.attention_layers = nn.ModuleList([\n            NonCausalAttention(config) for _ in range(config.encoder_layers)\n        ])\n        self.learned_query = nn.Parameter(torch.randn(config.hidden_dim))\n\n    def forward(self, hidden_states):\n        # Use learned query to aggregate sequence information\n        query = self.learned_query.expand(hidden_states.size(0), 1, -1)\n\n        # Non-causal attention over entire sequence\n        for layer in self.attention_layers:\n            query = layer(query, hidden_states, hidden_states)\n\n        return query.squeeze(1)\n</code></pre> <p>Key Features: - Non-causal attention: Can see the entire sequence - Learned query: Single vector that aggregates information - Separate parameters: Independent from decoder</p>"},{"location":"architecture/latent-planning/#2-binary-mapping","title":"2. Binary Mapping","text":"<p>Converts continuous encoder output to discrete binary plan:</p> <pre><code>class BinaryMapper(nn.Module):\n    def __init__(self, config):\n        self.projection = nn.Linear(config.hidden_dim, config.latent_dim)\n        self.temperature = config.gumbel_temperature\n\n    def forward(self, encoder_output, training=True):\n        logits = self.projection(encoder_output)\n\n        if training:\n            # Gumbel-Softmax for differentiable sampling\n            binary_soft = F.gumbel_softmax(\n                torch.stack([logits, -logits], dim=-1),\n                tau=self.temperature,\n                hard=True\n            )\n            return binary_soft[..., 0], logits\n        else:\n            # Hard binary sampling\n            return (logits &gt; 0).float(), logits\n</code></pre> <p>Gumbel-Softmax Trick: - Enables gradient flow through discrete sampling - Temperature controls discreteness vs continuity - Hard sampling during forward, soft during backward</p>"},{"location":"architecture/latent-planning/#3-plan-injection","title":"3. Plan Injection","text":"<p>Integrates the binary plan into decoder representations:</p> <pre><code>class PlanInjection(nn.Module):\n    def __init__(self, config):\n        self.plan_projection = nn.Linear(config.latent_dim, config.hidden_dim)\n        self.gate = nn.Linear(config.hidden_dim, config.hidden_dim)\n\n    def forward(self, decoder_hidden, binary_plan):\n        # Project plan to hidden dimension\n        plan_repr = self.plan_projection(binary_plan)\n\n        # Broadcast to sequence length\n        plan_repr = plan_repr.unsqueeze(1).expand(-1, decoder_hidden.size(1), -1)\n\n        # Gated injection\n        gate_values = torch.sigmoid(self.gate(decoder_hidden))\n        return decoder_hidden + gate_values * plan_repr\n</code></pre> <p>Injection Strategies: 1. Additive: <code>hidden + plan_projection(z)</code> 2. Gated: <code>hidden + gate * plan_projection(z)</code> (used) 3. Concatenation: <code>concat(hidden, plan_projection(z))</code> 4. Cross-attention: Plan as keys/values</p>"},{"location":"architecture/latent-planning/#training-dynamics","title":"Training Dynamics","text":""},{"location":"architecture/latent-planning/#variational-objective","title":"Variational Objective","text":"<p>The model is trained to maximize the Evidence Lower Bound (ELBO):</p> \\[\\mathcal{L} = \\mathbb{E}_{q(z|x)}[\\log p(x|z)] - \\beta \\cdot KL(q(z|x) || p(z))\\] <p>Where: - Reconstruction term: \\(\\mathbb{E}_{q(z|x)}[\\log p(x|z)]\\) - how well the model generates given the plan - Regularization term: \\(KL(q(z|x) || p(z))\\) - keeps posterior close to prior - \u03b2-VAE weight: Controls trade-off between reconstruction and regularization</p>"},{"location":"architecture/latent-planning/#free-bits-regularization","title":"Free Bits Regularization","text":"<p>To prevent posterior collapse, we use free bits:</p> \\[KL_{regularized} = \\max(KL(q(z|x) || p(z)), \\text{free\\_bits})\\] <p>This ensures the model uses at least <code>free_bits</code> nats of information in the latent variable.</p>"},{"location":"architecture/latent-planning/#training-vs-inference","title":"Training vs Inference","text":"<p>Training Mode: 1. Encode full sequence \u2192 latent plan 2. Inject plan into decoder 3. Optimize reconstruction + KL loss</p> <p>Inference Mode: 1. Sample plan from prior: \\(z \\sim p(z) = \\text{Uniform}(\\{0,1\\}^d)\\) 2. Inject sampled plan into decoder 3. Generate autoregressively</p>"},{"location":"architecture/latent-planning/#plan-analysis-and-interpretation","title":"Plan Analysis and Interpretation","text":""},{"location":"architecture/latent-planning/#plan-utilization","title":"Plan Utilization","text":"<p>Monitor whether the model actually uses the latent variable:</p> <pre><code>def analyze_plan_usage(model, dataloader):\n    \"\"\"Analyze how much the model uses the latent plan.\"\"\"\n    kl_divergences = []\n\n    for batch in dataloader:\n        with torch.no_grad():\n            _, z_logits = model(batch['input_ids'], mode='training')\n\n            # Compute KL divergence for each sample\n            posterior = torch.sigmoid(z_logits)\n            prior = torch.full_like(posterior, 0.5)\n\n            kl = F.kl_div(torch.log(posterior + 1e-8), prior, reduction='none')\n            kl_divergences.append(kl.sum(dim=-1))\n\n    kl_divergences = torch.cat(kl_divergences)\n\n    print(f\"Mean KL divergence: {kl_divergences.mean():.4f}\")\n    print(f\"Std KL divergence: {kl_divergences.std():.4f}\")\n    print(f\"Min KL divergence: {kl_divergences.min():.4f}\")\n    print(f\"Max KL divergence: {kl_divergences.max():.4f}\")\n\n    return kl_divergences\n</code></pre>"},{"location":"architecture/latent-planning/#plan-interpolation","title":"Plan Interpolation","text":"<p>Explore the latent space by interpolating between plans:</p> <pre><code>def interpolate_plans(model, prompt, plan1, plan2, steps=5):\n    \"\"\"Generate text with interpolated plans.\"\"\"\n    generations = []\n\n    for i in range(steps):\n        alpha = i / (steps - 1)\n        interpolated_plan = (1 - alpha) * plan1 + alpha * plan2\n        interpolated_plan = (interpolated_plan &gt; 0.5).float()\n\n        # Generate with interpolated plan\n        with torch.no_grad():\n            generated = model.generate_with_plan(\n                prompt, \n                interpolated_plan,\n                max_new_tokens=50\n            )\n        generations.append(generated)\n\n    return generations\n</code></pre>"},{"location":"architecture/latent-planning/#plan-manipulation","title":"Plan Manipulation","text":"<p>Control generation by modifying specific plan bits:</p> <pre><code>def manipulate_plan(model, prompt, bit_index, value):\n    \"\"\"Generate text with specific plan bit set to value.\"\"\"\n    # Sample random plan\n    plan = torch.bernoulli(torch.full((1, model.config.latent_dim), 0.5))\n\n    # Set specific bit\n    plan[0, bit_index] = value\n\n    # Generate with modified plan\n    with torch.no_grad():\n        generated = model.generate_with_plan(prompt, plan, max_new_tokens=100)\n\n    return generated\n\n# Example: Compare generations with bit 5 set to 0 vs 1\ngen_0 = manipulate_plan(model, prompt, bit_index=5, value=0)\ngen_1 = manipulate_plan(model, prompt, bit_index=5, value=1)\n</code></pre>"},{"location":"architecture/latent-planning/#advanced-planning-techniques","title":"Advanced Planning Techniques","text":""},{"location":"architecture/latent-planning/#hierarchical-planning","title":"Hierarchical Planning","text":"<p>Use multiple latent variables at different levels:</p> <pre><code>class HierarchicalPlanner(nn.Module):\n    def __init__(self, config):\n        self.global_encoder = NonCausalEncoder(config)\n        self.local_encoders = nn.ModuleList([\n            NonCausalEncoder(config) for _ in range(config.num_local_levels)\n        ])\n\n    def forward(self, hidden_states):\n        # Global plan for entire sequence\n        global_plan = self.global_encoder(hidden_states)\n\n        # Local plans for subsequences\n        local_plans = []\n        chunk_size = hidden_states.size(1) // len(self.local_encoders)\n\n        for i, encoder in enumerate(self.local_encoders):\n            start = i * chunk_size\n            end = (i + 1) * chunk_size\n            chunk = hidden_states[:, start:end]\n            local_plan = encoder(chunk)\n            local_plans.append(local_plan)\n\n        return global_plan, local_plans\n</code></pre>"},{"location":"architecture/latent-planning/#conditional-planning","title":"Conditional Planning","text":"<p>Condition plans on external information:</p> <pre><code>class ConditionalPlanner(nn.Module):\n    def __init__(self, config):\n        self.encoder = NonCausalEncoder(config)\n        self.condition_projection = nn.Linear(config.condition_dim, config.hidden_dim)\n\n    def forward(self, hidden_states, condition):\n        # Project condition to hidden space\n        condition_repr = self.condition_projection(condition)\n\n        # Add condition to hidden states\n        conditioned_hidden = hidden_states + condition_repr.unsqueeze(1)\n\n        # Encode with condition\n        plan = self.encoder(conditioned_hidden)\n        return plan\n</code></pre>"},{"location":"architecture/latent-planning/#troubleshooting-planning-issues","title":"Troubleshooting Planning Issues","text":""},{"location":"architecture/latent-planning/#posterior-collapse","title":"Posterior Collapse","text":"<p>Symptoms: KL loss drops to zero, model ignores latent variable Solutions: - Increase free bits threshold - Reduce KL weight (\u03b2) - Use KL annealing - Check encoder capacity</p>"},{"location":"architecture/latent-planning/#plan-underutilization","title":"Plan Underutilization","text":"<p>Symptoms: Low KL divergence, similar generations Solutions: - Increase latent dimension - Improve encoder architecture - Use stronger regularization - Check injection mechanism</p>"},{"location":"architecture/latent-planning/#training-instability","title":"Training Instability","text":"<p>Symptoms: Loss spikes, gradient explosions Solutions: - Gradient clipping - Lower learning rate - Reduce Gumbel temperature - Use warmup schedule</p>"},{"location":"architecture/latent-planning/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"architecture/latent-planning/#plan-quality-metrics","title":"Plan Quality Metrics","text":"<ol> <li>KL Divergence: Measures plan utilization</li> <li>Mutual Information: I(X; Z) between input and plan</li> <li>Plan Consistency: Similarity of plans for similar inputs</li> <li>Generation Diversity: Variety in outputs for different plans</li> </ol>"},{"location":"architecture/latent-planning/#implementation","title":"Implementation","text":"<pre><code>def evaluate_planning(model, dataloader):\n    \"\"\"Comprehensive evaluation of planning mechanism.\"\"\"\n    metrics = {\n        'kl_divergence': [],\n        'plan_entropy': [],\n        'generation_diversity': []\n    }\n\n    for batch in dataloader:\n        with torch.no_grad():\n            logits, z_logits = model(batch['input_ids'], mode='training')\n\n            # KL divergence\n            posterior = torch.sigmoid(z_logits)\n            kl = compute_kl_divergence(posterior)\n            metrics['kl_divergence'].append(kl)\n\n            # Plan entropy\n            entropy = -posterior * torch.log(posterior + 1e-8) - (1 - posterior) * torch.log(1 - posterior + 1e-8)\n            metrics['plan_entropy'].append(entropy.sum(dim=-1))\n\n    return {k: torch.cat(v).mean().item() for k, v in metrics.items()}\n</code></pre>"},{"location":"architecture/latent-planning/#next-steps","title":"Next Steps","text":"<ul> <li>Free Transformer Architecture: Complete architecture overview</li> <li>Training Guide: How to train with latent planning</li> <li>Examples: Practical usage examples</li> </ul>"},{"location":"architecture/overview/","title":"Architecture Overview","text":"<p>The Free Transformer introduces a novel approach to sequence modeling by incorporating explicit latent planning into the traditional autoregressive generation process.</p>"},{"location":"architecture/overview/#core-concept","title":"Core Concept","text":"<p>Traditional autoregressive Transformers generate tokens sequentially, conditioning only on previously generated tokens. This \"reactive\" approach can lead to:</p> <ul> <li>Local coherence but global inconsistency</li> <li>Difficulty with long-range planning</li> <li>Limited controllability in generation</li> </ul> <p>The Free Transformer addresses these limitations by introducing a latent planning mechanism that:</p> <ol> <li>First creates an abstract plan <code>Z</code> for the entire sequence</li> <li>Then generates tokens conditioned on both the history and the plan</li> </ol>"},{"location":"architecture/overview/#high-level-architecture","title":"High-Level Architecture","text":"<pre><code>graph TB\n    subgraph \"Input Processing\"\n        A[Input Tokens] --&gt; B[Token Embeddings]\n        B --&gt; C[Positional Encoding]\n    end\n\n    subgraph \"Early Layers (Context Building)\"\n        C --&gt; D[Decoder Blocks 1...L/2]\n    end\n\n    subgraph \"Latent Planning (Training Only)\"\n        D --&gt; E[Non-Causal Encoder]\n        E --&gt; F[Latent Variable Z]\n        F --&gt; G[Plan Injection]\n    end\n\n    subgraph \"Late Layers (Plan-Conditioned Generation)\"\n        D --&gt; H[Decoder Blocks L/2+1...L]\n        G --&gt; H\n        H --&gt; I[Output Logits]\n    end\n\n    subgraph \"Inference Mode\"\n        J[Random Z Sampling] --&gt; K[Plan Injection]\n        K --&gt; H\n    end</code></pre>"},{"location":"architecture/overview/#key-components","title":"Key Components","text":""},{"location":"architecture/overview/#1-decoder-backbone","title":"1. Decoder Backbone","text":"<p>Based on the Llama architecture with modern optimizations:</p> <ul> <li>RMSNorm: More stable than LayerNorm</li> <li>SwiGLU Activation: Better than ReLU/GELU</li> <li>RoPE: Rotary Position Embedding for better length generalization</li> <li>Grouped-Query Attention (GQA): Efficient multi-head attention</li> </ul>"},{"location":"architecture/overview/#2-latent-planning-system","title":"2. Latent Planning System","text":"<p>The core innovation consists of three components:</p>"},{"location":"architecture/overview/#encoder-block","title":"Encoder Block","text":"<ul> <li>Non-causal attention: Can attend to the entire sequence</li> <li>Learned query vector \u03b6: Aggregates sequence information</li> <li>Separate from decoder: Doesn't interfere with autoregressive flow</li> </ul>"},{"location":"architecture/overview/#binary-mapper","title":"Binary Mapper","text":"<ul> <li>Differentiable discretization: Converts continuous representations to binary plans</li> <li>Gumbel-Softmax: Enables gradient flow through discrete sampling</li> <li>Configurable dimensionality: Latent plan size <code>Z \u2208 {0,1}^d</code></li> </ul>"},{"location":"architecture/overview/#plan-injection","title":"Plan Injection","text":"<ul> <li>Post-sampler FC layer: Integrates plan into decoder representations</li> <li>Residual connections: Preserves original information flow</li> <li>Layer-wise injection: Plan influences multiple decoder layers</li> </ul>"},{"location":"architecture/overview/#3-conditional-vae-framework","title":"3. Conditional VAE Framework","text":"<p>The model is trained as a conditional Variational Autoencoder:</p>"},{"location":"architecture/overview/#training-mode","title":"Training Mode","text":"<pre><code>p(x|z) * p(z|x) = Reconstruction * Posterior\n</code></pre> <ul> <li>Reconstruction Loss: Standard language modeling loss</li> <li>KL Divergence: Regularizes latent space toward uniform prior</li> <li>Free Bits: Prevents posterior collapse</li> </ul>"},{"location":"architecture/overview/#inference-mode","title":"Inference Mode","text":"<pre><code>p(x|z) * p(z) = Generation * Prior\n</code></pre> <ul> <li>Prior Sampling: Sample <code>z</code> from uniform distribution</li> <li>Conditional Generation: Generate tokens given the sampled plan</li> </ul>"},{"location":"architecture/overview/#training-vs-inference-modes","title":"Training vs Inference Modes","text":""},{"location":"architecture/overview/#training-mode-flow","title":"Training Mode Flow","text":"<ol> <li>Forward Pass: Input \u2192 Early Layers \u2192 Encoder \u2192 Latent Z</li> <li>Plan Injection: Z \u2192 Late Layers \u2192 Output Logits</li> <li>Loss Computation: Reconstruction + KL Divergence</li> <li>Backward Pass: Gradients flow through differentiable components</li> </ol>"},{"location":"architecture/overview/#inference-mode-flow","title":"Inference Mode Flow","text":"<ol> <li>Context Processing: Prompt \u2192 Early Layers</li> <li>Plan Sampling: Sample Z from uniform prior</li> <li>Plan Injection: Z \u2192 Late Layers</li> <li>Generation: Autoregressive token generation</li> </ol>"},{"location":"architecture/overview/#mathematical-formulation","title":"Mathematical Formulation","text":""},{"location":"architecture/overview/#encoder","title":"Encoder","text":"<p>The encoder produces a latent representation from the full sequence:</p> \\[h_{enc} = \\text{Encoder}(X, \\zeta)\\] <p>where \\(\\zeta\\) is a learned query vector and \\(X\\) is the input sequence.</p>"},{"location":"architecture/overview/#binary-mapping","title":"Binary Mapping","text":"<p>The continuous representation is mapped to a binary plan:</p> \\[Z = \\text{BinaryMapper}(h_{enc})\\] <p>using differentiable binary encoding (e.g., Gumbel-Softmax).</p>"},{"location":"architecture/overview/#plan-injection_1","title":"Plan Injection","text":"<p>The binary plan is injected into the decoder:</p> \\[h_{inj} = h_{dec} + \\text{FC}(Z)\\] <p>where \\(h_{dec}\\) comes from the early decoder layers.</p>"},{"location":"architecture/overview/#loss-function","title":"Loss Function","text":"<p>The total loss combines reconstruction and regularization:</p> \\[\\mathcal{L} = \\mathcal{L}_{recon} + \\beta \\cdot \\text{KL}(q(Z|X) || p(Z))\\] <p>with free bits regularization to prevent collapse.</p>"},{"location":"architecture/overview/#design-principles","title":"Design Principles","text":""},{"location":"architecture/overview/#1-modularity","title":"1. Modularity","text":"<ul> <li>Separate components: Encoder, decoder, and injection are independent</li> <li>Configurable: Easy to modify latent dimensions, injection points</li> <li>Extensible: Can add new components without major changes</li> </ul>"},{"location":"architecture/overview/#2-efficiency","title":"2. Efficiency","text":"<ul> <li>Shared backbone: Reuses decoder architecture</li> <li>Minimal overhead: Encoder only active during training</li> <li>Memory efficient: Gradient checkpointing and optimized attention</li> </ul>"},{"location":"architecture/overview/#3-compatibility","title":"3. Compatibility","text":"<ul> <li>Standard interfaces: Compatible with HuggingFace ecosystem</li> <li>Flexible training: Works with existing training pipelines</li> <li>Easy deployment: Standard PyTorch model for inference</li> </ul>"},{"location":"architecture/overview/#comparison-with-baselines","title":"Comparison with Baselines","text":"Aspect Standard Transformer Free Transformer Planning Implicit, reactive Explicit, proactive Coherence Local Global + Local Controllability Limited High (via plan manipulation) Training Language modeling Conditional VAE Inference Autoregressive Plan-conditioned autoregressive Complexity O(n\u00b2) attention O(n\u00b2) + O(d) latent"},{"location":"architecture/overview/#next-steps","title":"Next Steps","text":"<ul> <li>Free Transformer Details: Deep dive into the model</li> <li>Latent Planning: Understanding the planning mechanism</li> <li>Training Guide: How to train the model effectively</li> </ul>"},{"location":"development/contributing/","title":"Contributing to Free Transformer","text":"<p>We welcome contributions to the Free Transformer project! This guide will help you get started with contributing code, documentation, or other improvements.</p>"},{"location":"development/contributing/#getting-started","title":"Getting Started","text":""},{"location":"development/contributing/#1-fork-and-clone","title":"1. Fork and Clone","text":"<pre><code># Fork the repository on GitHub, then clone your fork\ngit clone https://github.com/YOUR_USERNAME/free-transformer.git\ncd free-transformer\n\n# Add upstream remote\ngit remote add upstream https://github.com/udapy/free-transformer.git\n</code></pre>"},{"location":"development/contributing/#2-set-up-development-environment","title":"2. Set Up Development Environment","text":"<pre><code># Create virtual environment\nuv venv --python 3.12\nsource .venv/bin/activate\n\n# Install development dependencies\nuv pip install -e \".[dev]\"\n\n# Install pre-commit hooks (optional but recommended)\npre-commit install\n</code></pre>"},{"location":"development/contributing/#3-verify-setup","title":"3. Verify Setup","text":"<pre><code># Run tests to ensure everything works\nmake test\n\n# Run quality checks\nmake quality\n\n# Generate synthetic data and run demo\nmake demo\n</code></pre>"},{"location":"development/contributing/#development-workflow","title":"Development Workflow","text":""},{"location":"development/contributing/#1-create-a-feature-branch","title":"1. Create a Feature Branch","text":"<pre><code># Update your main branch\ngit checkout main\ngit pull upstream main\n\n# Create feature branch\ngit checkout -b feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#2-make-changes","title":"2. Make Changes","text":"<p>Follow these guidelines when making changes:</p> <ul> <li>Code Style: Follow PEP 8 and use the provided formatters</li> <li>Type Hints: Add type hints to all new functions</li> <li>Documentation: Update docstrings and documentation</li> <li>Tests: Add tests for new functionality</li> </ul>"},{"location":"development/contributing/#3-test-your-changes","title":"3. Test Your Changes","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test file\npytest tests/test_model.py -v\n\n# Run quality checks\nmake quality\n\n# Test with different configurations\npython examples/train_free.py --config configs/small.yaml\n</code></pre>"},{"location":"development/contributing/#4-commit-and-push","title":"4. Commit and Push","text":"<pre><code># Stage your changes\ngit add .\n\n# Commit with descriptive message\ngit commit -m \"feat: add support for custom attention patterns\"\n\n# Push to your fork\ngit push origin feature/your-feature-name\n</code></pre>"},{"location":"development/contributing/#5-create-pull-request","title":"5. Create Pull Request","text":"<ol> <li>Go to GitHub and create a pull request</li> <li>Fill out the PR template</li> <li>Link any related issues</li> <li>Wait for review and address feedback</li> </ol>"},{"location":"development/contributing/#code-style-guidelines","title":"Code Style Guidelines","text":""},{"location":"development/contributing/#python-code-style","title":"Python Code Style","text":"<p>We use several tools to maintain code quality:</p> <pre><code># Format code\nblack src/ tests/ examples/\nisort src/ tests/ examples/\n\n# Lint code\nflake8 src/ tests/ examples/\nruff check src/ tests/ examples/\n\n# Type checking\nmypy src/\n</code></pre>"},{"location":"development/contributing/#code-organization","title":"Code Organization","text":"<pre><code>src/free_transformer/\n\u251c\u2500\u2500 __init__.py          # Public API exports\n\u251c\u2500\u2500 model.py             # Main model classes\n\u251c\u2500\u2500 baseline.py          # Baseline Transformer\n\u251c\u2500\u2500 encoder.py           # Non-causal encoder\n\u251c\u2500\u2500 latent.py           # Latent variable components\n\u251c\u2500\u2500 injection.py        # Plan injection mechanisms\n\u251c\u2500\u2500 losses.py           # Loss functions\n\u251c\u2500\u2500 config.py           # Configuration classes\n\u251c\u2500\u2500 train_utils.py      # Training utilities\n\u2514\u2500\u2500 synthetic_data.py   # Data generation\n</code></pre>"},{"location":"development/contributing/#naming-conventions","title":"Naming Conventions","text":"<ul> <li>Classes: PascalCase (<code>FreeTransformer</code>, <code>ModelConfig</code>)</li> <li>Functions/Variables: snake_case (<code>compute_loss</code>, <code>hidden_dim</code>)</li> <li>Constants: UPPER_SNAKE_CASE (<code>DEFAULT_VOCAB_SIZE</code>)</li> <li>Private methods: Leading underscore (<code>_compute_attention</code>)</li> </ul>"},{"location":"development/contributing/#testing-guidelines","title":"Testing Guidelines","text":""},{"location":"development/contributing/#test-structure","title":"Test Structure","text":"<pre><code>tests/\n\u251c\u2500\u2500 unit/                # Unit tests for individual components\n\u2502   \u251c\u2500\u2500 test_model.py\n\u2502   \u251c\u2500\u2500 test_encoder.py\n\u2502   \u2514\u2500\u2500 test_losses.py\n\u251c\u2500\u2500 integration/         # Integration tests\n\u2502   \u251c\u2500\u2500 test_training.py\n\u2502   \u2514\u2500\u2500 test_generation.py\n\u2514\u2500\u2500 test_comparison.py   # Model comparison tests\n</code></pre>"},{"location":"development/contributing/#writing-tests","title":"Writing Tests","text":"<pre><code>import pytest\nimport torch\nfrom free_transformer import FreeTransformer, ModelConfig\n\nclass TestFreeTransformer:\n    @pytest.fixture\n    def config(self):\n        return ModelConfig(\n            vocab_size=1000,\n            hidden_dim=128,\n            num_layers=4,\n            num_heads=4,\n            latent_dim=8\n        )\n\n    @pytest.fixture\n    def model(self, config):\n        return FreeTransformer(config)\n\n    def test_forward_training_mode(self, model, config):\n        batch_size, seq_len = 2, 32\n        tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\n        logits, z_logits = model(tokens, mode='training')\n\n        assert logits.shape == (batch_size, seq_len, config.vocab_size)\n        assert z_logits.shape == (batch_size, config.latent_dim)\n\n    def test_generation(self, model, config):\n        prompt = torch.randint(0, config.vocab_size, (1, 10))\n\n        generated = model.generate(prompt, max_new_tokens=20)\n\n        assert generated.shape == (1, 30)  # 10 + 20\n        assert torch.all(generated &gt;= 0)\n        assert torch.all(generated &lt; config.vocab_size)\n</code></pre>"},{"location":"development/contributing/#test-coverage","title":"Test Coverage","text":"<p>Aim for high test coverage:</p> <pre><code># Run tests with coverage\npytest --cov=src/free_transformer --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html\n</code></pre>"},{"location":"development/contributing/#documentation-guidelines","title":"Documentation Guidelines","text":""},{"location":"development/contributing/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>def compute_loss(logits: torch.Tensor, targets: torch.Tensor, \n                 config: ModelConfig) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Compute the Free Transformer loss.\n\n    Args:\n        logits: Model output logits of shape (batch_size, seq_len, vocab_size).\n        targets: Target token IDs of shape (batch_size, seq_len).\n        config: Model configuration containing loss hyperparameters.\n\n    Returns:\n        Dictionary containing:\n            - total_loss: Combined reconstruction and KL loss\n            - recon_loss: Cross-entropy reconstruction loss\n            - kl_loss: KL divergence regularization loss\n\n    Raises:\n        ValueError: If logits and targets have incompatible shapes.\n\n    Example:\n        &gt;&gt;&gt; logits = torch.randn(2, 10, 1000)\n        &gt;&gt;&gt; targets = torch.randint(0, 1000, (2, 10))\n        &gt;&gt;&gt; loss_dict = compute_loss(logits, targets, config)\n        &gt;&gt;&gt; print(loss_dict['total_loss'])\n    \"\"\"\n</code></pre>"},{"location":"development/contributing/#documentation-updates","title":"Documentation Updates","text":"<p>When adding new features:</p> <ol> <li>Update API docs: Add docstrings to new classes/functions</li> <li>Update guides: Add examples to relevant guides</li> <li>Update README: If it affects installation or basic usage</li> <li>Add examples: Create example scripts if appropriate</li> </ol>"},{"location":"development/contributing/#types-of-contributions","title":"Types of Contributions","text":""},{"location":"development/contributing/#1-bug-fixes","title":"1. Bug Fixes","text":"<ul> <li>Small fixes: Can be submitted directly</li> <li>Large fixes: Please open an issue first to discuss</li> </ul> <p>Example bug fix PR: <pre><code>Title: Fix gradient flow in binary mapper\nDescription: The Gumbel-Softmax implementation was not properly \nhandling gradients in training mode. This PR fixes the issue by...\n</code></pre></p>"},{"location":"development/contributing/#2-new-features","title":"2. New Features","text":"<p>Please open an issue first to discuss new features:</p> <ul> <li>Architecture improvements: New attention mechanisms, injection strategies</li> <li>Training enhancements: New loss functions, optimization techniques</li> <li>Utility functions: Data processing, evaluation metrics</li> <li>Performance optimizations: Memory usage, speed improvements</li> </ul>"},{"location":"development/contributing/#3-documentation","title":"3. Documentation","text":"<ul> <li>API documentation: Improve docstrings and type hints</li> <li>Guides and tutorials: Add new examples or improve existing ones</li> <li>Architecture explanations: Help explain complex concepts</li> <li>FAQ updates: Add common questions and solutions</li> </ul>"},{"location":"development/contributing/#4-tests","title":"4. Tests","text":"<ul> <li>Unit tests: Test individual components</li> <li>Integration tests: Test component interactions</li> <li>Performance tests: Benchmark improvements</li> <li>Regression tests: Prevent known issues from reoccurring</li> </ul>"},{"location":"development/contributing/#review-process","title":"Review Process","text":""},{"location":"development/contributing/#what-we-look-for","title":"What We Look For","text":"<ol> <li>Correctness: Does the code work as intended?</li> <li>Style: Does it follow our coding standards?</li> <li>Tests: Are there adequate tests?</li> <li>Documentation: Is it properly documented?</li> <li>Performance: Does it maintain or improve performance?</li> </ol>"},{"location":"development/contributing/#review-timeline","title":"Review Timeline","text":"<ul> <li>Small fixes: Usually reviewed within 1-2 days</li> <li>Medium features: Usually reviewed within 3-5 days</li> <li>Large features: May take 1-2 weeks depending on complexity</li> </ul>"},{"location":"development/contributing/#addressing-feedback","title":"Addressing Feedback","text":"<ul> <li>Be responsive: Address feedback promptly</li> <li>Ask questions: If feedback is unclear, ask for clarification</li> <li>Make incremental changes: Small, focused commits are easier to review</li> <li>Update tests: Ensure tests pass after addressing feedback</li> </ul>"},{"location":"development/contributing/#release-process","title":"Release Process","text":""},{"location":"development/contributing/#version-numbering","title":"Version Numbering","text":"<p>We follow semantic versioning (SemVer):</p> <ul> <li>Major (1.0.0): Breaking changes</li> <li>Minor (0.1.0): New features, backward compatible</li> <li>Patch (0.0.1): Bug fixes, backward compatible</li> </ul>"},{"location":"development/contributing/#release-checklist","title":"Release Checklist","text":"<p>Before releasing:</p> <ol> <li>Update version: In <code>pyproject.toml</code> and <code>__init__.py</code></li> <li>Update CHANGELOG: Document all changes</li> <li>Run full test suite: Ensure everything passes</li> <li>Update documentation: Reflect any changes</li> <li>Create release notes: Summarize key changes</li> </ol>"},{"location":"development/contributing/#getting-help","title":"Getting Help","text":""},{"location":"development/contributing/#communication-channels","title":"Communication Channels","text":"<ul> <li>GitHub Issues: Bug reports, feature requests</li> <li>GitHub Discussions: General questions, ideas</li> <li>Pull Request Comments: Code-specific discussions</li> </ul>"},{"location":"development/contributing/#mentorship","title":"Mentorship","text":"<p>New contributors are welcome! If you're new to the project:</p> <ol> <li>Start small: Look for \"good first issue\" labels</li> <li>Ask questions: Don't hesitate to ask for help</li> <li>Read the code: Familiarize yourself with the codebase</li> <li>Join discussions: Participate in issue discussions</li> </ol>"},{"location":"development/contributing/#recognition","title":"Recognition","text":"<p>Contributors are recognized in several ways:</p> <ul> <li>CONTRIBUTORS.md: All contributors are listed</li> <li>Release notes: Significant contributions are highlighted</li> <li>GitHub: Contributions show up on your GitHub profile</li> </ul>"},{"location":"development/contributing/#code-of-conduct","title":"Code of Conduct","text":"<p>We are committed to providing a welcoming and inclusive environment:</p> <ul> <li>Be respectful: Treat all contributors with respect</li> <li>Be constructive: Provide helpful feedback</li> <li>Be patient: Remember that everyone is learning</li> <li>Be inclusive: Welcome contributors from all backgrounds</li> </ul>"},{"location":"development/contributing/#common-tasks","title":"Common Tasks","text":""},{"location":"development/contributing/#adding-a-new-model-component","title":"Adding a New Model Component","text":"<ol> <li>Create the module: Add to <code>src/free_transformer/</code></li> <li>Add tests: Create corresponding test file</li> <li>Update exports: Add to <code>__init__.py</code></li> <li>Add documentation: Include docstrings and examples</li> <li>Update configs: Add configuration options if needed</li> </ol>"},{"location":"development/contributing/#adding-a-new-loss-function","title":"Adding a New Loss Function","text":"<ol> <li>Implement in <code>losses.py</code>: Follow existing patterns</li> <li>Add unit tests: Test edge cases and gradients</li> <li>Update training scripts: Show how to use it</li> <li>Document parameters: Explain hyperparameters</li> <li>Add examples: Show typical usage</li> </ol>"},{"location":"development/contributing/#improving-performance","title":"Improving Performance","text":"<ol> <li>Profile first: Identify actual bottlenecks</li> <li>Benchmark changes: Measure improvements</li> <li>Maintain correctness: Ensure outputs don't change</li> <li>Update tests: Add performance regression tests</li> <li>Document changes: Explain the optimization</li> </ol> <p>Thank you for contributing to Free Transformer! \ud83d\ude80</p>"},{"location":"development/quality/","title":"Code Quality","text":"<p>This guide covers code quality tools, standards, and practices for the Free Transformer project.</p>"},{"location":"development/quality/#quality-tools","title":"Quality Tools","text":"<p>The project uses several tools to maintain code quality:</p> <ul> <li>Black: Code formatting</li> <li>isort: Import sorting</li> <li>Ruff: Fast Python linter</li> <li>MyPy: Static type checking</li> <li>Pytest: Testing framework</li> </ul>"},{"location":"development/quality/#running-quality-checks","title":"Running Quality Checks","text":""},{"location":"development/quality/#quick-commands","title":"Quick Commands","text":"<pre><code># Run all quality checks\nmake quality\n\n# Individual tools\nmake format          # Format code with black and isort\nmake format-check    # Check formatting without changes\nmake lint           # Run ruff linter\nmake type-check     # Run mypy type checker\n</code></pre>"},{"location":"development/quality/#pre-commit-hooks","title":"Pre-commit Hooks","text":"<p>Install pre-commit hooks to run quality checks automatically:</p> <pre><code># Install pre-commit\nuv pip install pre-commit\n\n# Install hooks\npre-commit install\n\n# Run hooks manually\npre-commit run --all-files\n</code></pre>"},{"location":"development/quality/#code-formatting","title":"Code Formatting","text":""},{"location":"development/quality/#black-configuration","title":"Black Configuration","text":"<p>Black is configured in <code>pyproject.toml</code>:</p> <pre><code>[tool.black]\nline-length = 100\ntarget-version = ['py312']\n</code></pre>"},{"location":"development/quality/#usage-examples","title":"Usage Examples","text":"<pre><code># Format all code\nblack src/ tests/ examples/\n\n# Check formatting without changes\nblack --check src/ tests/ examples/\n\n# Format specific file\nblack src/free_transformer/model.py\n</code></pre>"},{"location":"development/quality/#import-sorting-with-isort","title":"Import Sorting with isort","text":"<p>isort configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.isort]\nprofile = \"black\"\nline_length = 100\nmulti_line_output = 3\ninclude_trailing_comma = true\nforce_grid_wrap = 0\nuse_parentheses = true\nensure_newline_before_comments = true\n</code></pre> <p>Example usage:</p> <pre><code># Sort imports\nisort src/ tests/ examples/\n\n# Check import sorting\nisort --check-only src/ tests/ examples/\n</code></pre>"},{"location":"development/quality/#linting-with-ruff","title":"Linting with Ruff","text":""},{"location":"development/quality/#configuration","title":"Configuration","text":"<p>Ruff configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.ruff]\nline-length = 100\ntarget-version = \"py312\"\n\n[tool.ruff.lint]\nselect = [\n    \"E\",   # pycodestyle errors\n    \"W\",   # pycodestyle warnings\n    \"F\",   # pyflakes\n    \"I\",   # isort\n    \"B\",   # flake8-bugbear\n    \"C4\",  # flake8-comprehensions\n    \"UP\",  # pyupgrade\n]\nignore = [\n    \"E501\",  # line too long (handled by black)\n    \"B008\",  # do not perform function calls in argument defaults\n    \"C901\",  # too complex\n]\n\n[tool.ruff.lint.per-file-ignores]\n\"__init__.py\" = [\"F401\"]  # Allow unused imports in __init__.py\n\"tests/*\" = [\"B011\"]      # Allow assert False in tests\n</code></pre>"},{"location":"development/quality/#common-lint-issues-and-fixes","title":"Common Lint Issues and Fixes","text":"<p>Unused imports: <pre><code># Bad\nimport torch\nimport numpy as np  # Unused\n\ndef forward(x):\n    return torch.relu(x)\n\n# Good\nimport torch\n\ndef forward(x):\n    return torch.relu(x)\n</code></pre></p> <p>Long lines: <pre><code># Bad\ndef very_long_function_name(very_long_parameter_name, another_very_long_parameter_name, yet_another_parameter):\n    pass\n\n# Good\ndef very_long_function_name(\n    very_long_parameter_name,\n    another_very_long_parameter_name,\n    yet_another_parameter\n):\n    pass\n</code></pre></p> <p>Mutable default arguments: <pre><code># Bad\ndef process_data(data, config={}):\n    config['processed'] = True\n    return data\n\n# Good\ndef process_data(data, config=None):\n    if config is None:\n        config = {}\n    config['processed'] = True\n    return data\n</code></pre></p>"},{"location":"development/quality/#type-checking-with-mypy","title":"Type Checking with MyPy","text":""},{"location":"development/quality/#configuration_1","title":"Configuration","text":"<p>MyPy configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.mypy]\npython_version = \"3.12\"\nwarn_return_any = true\nwarn_unused_configs = true\ndisallow_untyped_defs = false\nignore_missing_imports = true\n\n[[tool.mypy.overrides]]\nmodule = \"tests.*\"\ndisallow_untyped_defs = false\n</code></pre>"},{"location":"development/quality/#type-annotation-examples","title":"Type Annotation Examples","text":"<p>Function annotations: <pre><code>from typing import Optional, List, Dict, Tuple, Union\nimport torch\n\ndef compute_loss(\n    logits: torch.Tensor,\n    targets: torch.Tensor,\n    weights: Optional[torch.Tensor] = None\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Compute loss with optional weights.\"\"\"\n    loss = F.cross_entropy(logits, targets, reduction='none')\n\n    if weights is not None:\n        loss = loss * weights\n\n    return {\n        'loss': loss.mean(),\n        'raw_loss': loss\n    }\n</code></pre></p> <p>Class annotations: <pre><code>from typing import List, Optional\nimport torch.nn as nn\n\nclass FreeTransformer(nn.Module):\n    \"\"\"Free Transformer model with type annotations.\"\"\"\n\n    def __init__(self, config: ModelConfig) -&gt; None:\n        super().__init__()\n        self.config = config\n        self.layers: List[TransformerBlock] = []\n        self._cache: Optional[Dict[str, torch.Tensor]] = None\n\n    def forward(\n        self, \n        tokens: torch.Tensor, \n        mode: str = 'training'\n    ) -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Forward pass with type hints.\"\"\"\n        # Implementation here\n        pass\n</code></pre></p> <p>Generic types: <pre><code>from typing import TypeVar, Generic, List\n\nT = TypeVar('T')\n\nclass DataLoader(Generic[T]):\n    \"\"\"Generic data loader.\"\"\"\n\n    def __init__(self, data: List[T]) -&gt; None:\n        self.data = data\n\n    def __getitem__(self, index: int) -&gt; T:\n        return self.data[index]\n</code></pre></p>"},{"location":"development/quality/#common-type-issues","title":"Common Type Issues","text":"<p>Missing return type: <pre><code># Bad\ndef process_tokens(tokens):\n    return tokens.long()\n\n# Good\ndef process_tokens(tokens: torch.Tensor) -&gt; torch.Tensor:\n    return tokens.long()\n</code></pre></p> <p>Any type usage: <pre><code># Bad\nfrom typing import Any\n\ndef process_data(data: Any) -&gt; Any:\n    return data.process()\n\n# Good\nfrom typing import Protocol\n\nclass Processable(Protocol):\n    def process(self) -&gt; 'Processable':\n        ...\n\ndef process_data(data: Processable) -&gt; Processable:\n    return data.process()\n</code></pre></p>"},{"location":"development/quality/#documentation-standards","title":"Documentation Standards","text":""},{"location":"development/quality/#docstring-format","title":"Docstring Format","text":"<p>Use Google-style docstrings:</p> <pre><code>def free_transformer_loss(\n    logits: torch.Tensor,\n    z_logits: torch.Tensor,\n    targets: torch.Tensor,\n    latent_dim: int,\n    kl_weight: float = 0.1,\n    free_bits: float = 0.5\n) -&gt; Dict[str, torch.Tensor]:\n    \"\"\"Compute Free Transformer loss with VAE components.\n\n    This function computes the total loss for the Free Transformer,\n    combining reconstruction loss and KL divergence with free bits\n    regularization.\n\n    Args:\n        logits: Model output logits of shape (batch_size, seq_len, vocab_size).\n        z_logits: Latent variable logits of shape (batch_size, latent_dim).\n        targets: Target token IDs of shape (batch_size, seq_len).\n        latent_dim: Dimension of the latent space.\n        kl_weight: Weight for KL divergence loss. Defaults to 0.1.\n        free_bits: Free bits threshold for KL regularization. Defaults to 0.5.\n\n    Returns:\n        Dictionary containing:\n            - total_loss: Combined reconstruction and KL loss\n            - recon_loss: Cross-entropy reconstruction loss\n            - kl_loss: KL divergence loss with free bits\n\n    Raises:\n        ValueError: If input tensors have incompatible shapes.\n\n    Example:\n        &gt;&gt;&gt; logits = torch.randn(2, 10, 1000)\n        &gt;&gt;&gt; z_logits = torch.randn(2, 16)\n        &gt;&gt;&gt; targets = torch.randint(0, 1000, (2, 10))\n        &gt;&gt;&gt; loss_dict = free_transformer_loss(logits, z_logits, targets, 16)\n        &gt;&gt;&gt; print(f\"Total loss: {loss_dict['total_loss']:.4f}\")\n    \"\"\"\n    # Implementation here\n    pass\n</code></pre>"},{"location":"development/quality/#class-documentation","title":"Class Documentation","text":"<pre><code>class FreeTransformer(nn.Module):\n    \"\"\"Free Transformer with explicit latent planning.\n\n    The Free Transformer extends standard autoregressive Transformers\n    with a latent planning mechanism. It first creates an abstract plan\n    for the entire sequence, then generates tokens conditioned on that plan.\n\n    Attributes:\n        config: Model configuration containing architecture parameters.\n        token_embedding: Token embedding layer.\n        encoder: Non-causal encoder for latent plan generation.\n        decoder_blocks: List of transformer decoder blocks.\n\n    Example:\n        &gt;&gt;&gt; config = ModelConfig(vocab_size=1000, hidden_dim=512)\n        &gt;&gt;&gt; model = FreeTransformer(config)\n        &gt;&gt;&gt; tokens = torch.randint(0, 1000, (2, 128))\n        &gt;&gt;&gt; logits, z_logits = model(tokens, mode='training')\n    \"\"\"\n\n    def __init__(self, config: ModelConfig) -&gt; None:\n        \"\"\"Initialize Free Transformer.\n\n        Args:\n            config: Model configuration with architecture parameters.\n        \"\"\"\n        super().__init__()\n        # Implementation here\n</code></pre>"},{"location":"development/quality/#code-organization","title":"Code Organization","text":""},{"location":"development/quality/#module-structure","title":"Module Structure","text":"<pre><code>\"\"\"Module for Free Transformer model implementation.\n\nThis module contains the main Free Transformer model class and related\ncomponents for latent planning and conditional generation.\n\"\"\"\n\n# Standard library imports\nimport math\nfrom typing import Dict, List, Optional, Tuple, Union\n\n# Third-party imports\nimport torch\nimport torch.nn as nn\nimport torch.nn.functional as F\n\n# Local imports\nfrom .config import ModelConfig\nfrom .encoder import NonCausalEncoder\nfrom .latent import BinaryMapper, PlanInjection\nfrom .losses import free_transformer_loss\n\n# Module-level constants\nDEFAULT_TEMPERATURE = 0.8\nDEFAULT_TOP_K = 40\n\n# Public API\n__all__ = [\n    'FreeTransformer',\n    'TransformerBlock',\n    'RMSNorm',\n]\n</code></pre>"},{"location":"development/quality/#function-organization","title":"Function Organization","text":"<pre><code>class FreeTransformer(nn.Module):\n    \"\"\"Free Transformer implementation.\"\"\"\n\n    # Public methods first\n    def __init__(self, config: ModelConfig) -&gt; None:\n        \"\"\"Initialize model.\"\"\"\n        pass\n\n    def forward(self, tokens: torch.Tensor, mode: str = 'training') -&gt; Union[torch.Tensor, Tuple[torch.Tensor, torch.Tensor]]:\n        \"\"\"Forward pass.\"\"\"\n        pass\n\n    def generate(self, prompt: torch.Tensor, max_new_tokens: int = 100, **kwargs) -&gt; torch.Tensor:\n        \"\"\"Generate text.\"\"\"\n        pass\n\n    # Private methods last\n    def _init_weights(self, module: nn.Module) -&gt; None:\n        \"\"\"Initialize weights.\"\"\"\n        pass\n\n    def _compute_attention_mask(self, seq_len: int) -&gt; torch.Tensor:\n        \"\"\"Compute causal attention mask.\"\"\"\n        pass\n</code></pre>"},{"location":"development/quality/#error-handling","title":"Error Handling","text":""},{"location":"development/quality/#exception-handling","title":"Exception Handling","text":"<pre><code>def validate_config(config: ModelConfig) -&gt; None:\n    \"\"\"Validate model configuration.\n\n    Args:\n        config: Model configuration to validate.\n\n    Raises:\n        ValueError: If configuration is invalid.\n    \"\"\"\n    if config.hidden_dim &lt;= 0:\n        raise ValueError(f\"hidden_dim must be positive, got {config.hidden_dim}\")\n\n    if config.hidden_dim % config.num_heads != 0:\n        raise ValueError(\n            f\"hidden_dim ({config.hidden_dim}) must be divisible by \"\n            f\"num_heads ({config.num_heads})\"\n        )\n\n    if config.latent_dim &lt;= 0:\n        raise ValueError(f\"latent_dim must be positive, got {config.latent_dim}\")\n\ndef safe_divide(a: torch.Tensor, b: torch.Tensor, eps: float = 1e-8) -&gt; torch.Tensor:\n    \"\"\"Safely divide tensors with epsilon for numerical stability.\n\n    Args:\n        a: Numerator tensor.\n        b: Denominator tensor.\n        eps: Small epsilon to prevent division by zero.\n\n    Returns:\n        Result of a / (b + eps).\n    \"\"\"\n    return a / (b + eps)\n</code></pre>"},{"location":"development/quality/#logging","title":"Logging","text":"<pre><code>import logging\n\nlogger = logging.getLogger(__name__)\n\ndef train_model(model: FreeTransformer, dataloader: DataLoader) -&gt; None:\n    \"\"\"Train model with proper logging.\"\"\"\n    logger.info(\"Starting training\")\n\n    for epoch in range(num_epochs):\n        logger.debug(f\"Starting epoch {epoch}\")\n\n        try:\n            epoch_loss = train_epoch(model, dataloader)\n            logger.info(f\"Epoch {epoch}: loss = {epoch_loss:.4f}\")\n        except Exception as e:\n            logger.error(f\"Training failed at epoch {epoch}: {e}\")\n            raise\n\n    logger.info(\"Training completed successfully\")\n</code></pre>"},{"location":"development/quality/#performance-guidelines","title":"Performance Guidelines","text":""},{"location":"development/quality/#memory-efficiency","title":"Memory Efficiency","text":"<pre><code># Use in-place operations when possible\ndef apply_activation_inplace(x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Apply activation in-place to save memory.\"\"\"\n    return F.relu_(x)  # In-place ReLU\n\n# Use context managers for temporary computations\ndef compute_attention_weights(query: torch.Tensor, key: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute attention weights efficiently.\"\"\"\n    with torch.cuda.amp.autocast():  # Mixed precision\n        scores = torch.matmul(query, key.transpose(-2, -1))\n        return F.softmax(scores, dim=-1)\n\n# Clear intermediate variables\ndef forward_with_cleanup(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass with memory cleanup.\"\"\"\n    intermediate = self.layer1(x)\n    x = self.layer2(intermediate)\n    del intermediate  # Free memory\n    return x\n</code></pre>"},{"location":"development/quality/#computational-efficiency","title":"Computational Efficiency","text":"<pre><code># Vectorize operations\ndef compute_distances_vectorized(embeddings: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Compute pairwise distances efficiently.\"\"\"\n    # Vectorized computation\n    return torch.cdist(embeddings, embeddings, p=2)\n\n# Use appropriate data types\ndef mixed_precision_forward(self, x: torch.Tensor) -&gt; torch.Tensor:\n    \"\"\"Forward pass with mixed precision.\"\"\"\n    with torch.cuda.amp.autocast():\n        return self.model(x.half())  # Use half precision\n</code></pre>"},{"location":"development/quality/#testing-quality","title":"Testing Quality","text":""},{"location":"development/quality/#test-organization","title":"Test Organization","text":"<pre><code># tests/unit/test_model.py\nclass TestFreeTransformer:\n    \"\"\"Test suite for FreeTransformer class.\"\"\"\n\n    @pytest.fixture\n    def config(self) -&gt; ModelConfig:\n        \"\"\"Standard test configuration.\"\"\"\n        return ModelConfig(\n            vocab_size=1000,\n            hidden_dim=128,\n            num_layers=4,\n            num_heads=4,\n            latent_dim=8\n        )\n\n    def test_initialization(self, config: ModelConfig) -&gt; None:\n        \"\"\"Test model initializes correctly.\"\"\"\n        model = FreeTransformer(config)\n        assert isinstance(model, FreeTransformer)\n        assert model.config == config\n\n    def test_forward_shapes(self, config: ModelConfig) -&gt; None:\n        \"\"\"Test forward pass produces correct shapes.\"\"\"\n        model = FreeTransformer(config)\n        tokens = torch.randint(0, config.vocab_size, (2, 16))\n\n        logits, z_logits = model(tokens, mode='training')\n\n        assert logits.shape == (2, 16, config.vocab_size)\n        assert z_logits.shape == (2, config.latent_dim)\n</code></pre>"},{"location":"development/quality/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/quality/#quality-checks-in-ci","title":"Quality Checks in CI","text":"<pre><code># .github/workflows/quality.yml\nname: Code Quality\n\non: [push, pull_request]\n\njobs:\n  quality:\n    runs-on: ubuntu-latest\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python\n      uses: actions/setup-python@v4\n      with:\n        python-version: 3.12\n\n    - name: Install dependencies\n      run: |\n        curl -LsSf https://astral.sh/uv/install.sh | sh\n        source $HOME/.cargo/env\n        uv venv\n        source .venv/bin/activate\n        uv pip install -e \".[dev]\"\n\n    - name: Check formatting\n      run: |\n        source .venv/bin/activate\n        black --check src/ tests/ examples/\n        isort --check-only src/ tests/ examples/\n\n    - name: Lint code\n      run: |\n        source .venv/bin/activate\n        ruff check src/ tests/ examples/\n\n    - name: Type check\n      run: |\n        source .venv/bin/activate\n        mypy src/\n\n    - name: Run tests\n      run: |\n        source .venv/bin/activate\n        pytest tests/ --cov=src/free_transformer\n</code></pre>"},{"location":"development/quality/#best-practices-summary","title":"Best Practices Summary","text":"<ol> <li>Formatting: Use Black and isort for consistent code style</li> <li>Linting: Fix all Ruff warnings and errors</li> <li>Type Hints: Add type annotations to all public functions</li> <li>Documentation: Write clear docstrings with examples</li> <li>Error Handling: Use appropriate exceptions and logging</li> <li>Testing: Maintain high test coverage with quality tests</li> <li>Performance: Consider memory and computational efficiency</li> <li>Organization: Structure code logically with clear imports</li> </ol>"},{"location":"development/quality/#next-steps","title":"Next Steps","text":"<ul> <li>Testing Guide: Comprehensive testing strategies</li> <li>Contributing: How to contribute to the project</li> <li>Training Guide: Training best practices</li> </ul>"},{"location":"development/testing/","title":"Testing Guide","text":"<p>This guide covers testing strategies and practices for the Free Transformer project.</p>"},{"location":"development/testing/#test-structure","title":"Test Structure","text":"<p>The test suite is organized into three main categories:</p> <pre><code>tests/\n\u251c\u2500\u2500 unit/                    # Unit tests for individual components\n\u2502   \u251c\u2500\u2500 test_model.py       # Model architecture tests\n\u2502   \u251c\u2500\u2500 test_encoder.py     # Encoder component tests\n\u2502   \u251c\u2500\u2500 test_latent.py      # Latent variable tests\n\u2502   \u251c\u2500\u2500 test_losses.py      # Loss function tests\n\u2502   \u2514\u2500\u2500 test_config.py      # Configuration tests\n\u251c\u2500\u2500 integration/            # Integration tests\n\u2502   \u251c\u2500\u2500 test_training.py    # Training pipeline tests\n\u2502   \u251c\u2500\u2500 test_generation.py  # Generation tests\n\u2502   \u2514\u2500\u2500 test_data.py        # Data loading tests\n\u2514\u2500\u2500 test_comparison.py      # Model comparison tests\n</code></pre>"},{"location":"development/testing/#running-tests","title":"Running Tests","text":""},{"location":"development/testing/#basic-test-commands","title":"Basic Test Commands","text":"<pre><code># Run all tests\nmake test\n\n# Run specific test categories\nmake test-unit\nmake test-integration\nmake test-comparison\n\n# Run fast tests (no coverage)\nmake test-fast\n\n# Run specific test file\npytest tests/unit/test_model.py -v\n\n# Run specific test function\npytest tests/unit/test_model.py::TestFreeTransformer::test_forward_training_mode -v\n</code></pre>"},{"location":"development/testing/#test-configuration","title":"Test Configuration","text":"<p>Tests use pytest with the following configuration in <code>pyproject.toml</code>:</p> <pre><code>[tool.pytest.ini_options]\ntestpaths = [\"tests\"]\npython_files = [\"test_*.py\"]\naddopts = \"--cov=src/free_transformer --cov-report=html --cov-report=term\"\n</code></pre>"},{"location":"development/testing/#unit-tests","title":"Unit Tests","text":""},{"location":"development/testing/#model-architecture-tests","title":"Model Architecture Tests","text":"<pre><code># tests/unit/test_model.py\nimport pytest\nimport torch\nfrom free_transformer import FreeTransformer, ModelConfig\n\nclass TestFreeTransformer:\n    @pytest.fixture\n    def config(self):\n        return ModelConfig(\n            vocab_size=1000,\n            hidden_dim=128,\n            num_layers=4,\n            num_heads=4,\n            latent_dim=8,\n            max_seq_len=256\n        )\n\n    @pytest.fixture\n    def model(self, config):\n        return FreeTransformer(config)\n\n    def test_model_initialization(self, model, config):\n        \"\"\"Test model initializes correctly.\"\"\"\n        assert model.config.vocab_size == config.vocab_size\n        assert model.config.hidden_dim == config.hidden_dim\n        assert model.config.latent_dim == config.latent_dim\n\n        # Check parameter count\n        total_params = sum(p.numel() for p in model.parameters())\n        assert total_params &gt; 0\n\n    def test_forward_training_mode(self, model, config):\n        \"\"\"Test forward pass in training mode.\"\"\"\n        batch_size, seq_len = 2, 32\n        tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\n        logits, z_logits = model(tokens, mode='training')\n\n        # Check output shapes\n        assert logits.shape == (batch_size, seq_len, config.vocab_size)\n        assert z_logits.shape == (batch_size, config.latent_dim)\n\n        # Check output types\n        assert isinstance(logits, torch.Tensor)\n        assert isinstance(z_logits, torch.Tensor)\n\n        # Check gradients can flow\n        loss = logits.sum() + z_logits.sum()\n        loss.backward()\n\n        # Check some parameters have gradients\n        has_gradients = any(p.grad is not None for p in model.parameters())\n        assert has_gradients\n\n    def test_forward_inference_mode(self, model, config):\n        \"\"\"Test forward pass in inference mode.\"\"\"\n        batch_size, seq_len = 2, 32\n        tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\n        model.eval()\n        with torch.no_grad():\n            logits = model(tokens, mode='inference')\n\n        # Check output shape\n        assert logits.shape == (batch_size, seq_len, config.vocab_size)\n        assert isinstance(logits, torch.Tensor)\n\n    def test_generation(self, model, config):\n        \"\"\"Test text generation.\"\"\"\n        prompt = torch.randint(0, config.vocab_size, (1, 10))\n        max_new_tokens = 20\n\n        model.eval()\n        with torch.no_grad():\n            generated = model.generate(prompt, max_new_tokens=max_new_tokens)\n\n        # Check output shape\n        expected_length = prompt.size(1) + max_new_tokens\n        assert generated.shape == (1, expected_length)\n\n        # Check tokens are valid\n        assert torch.all(generated &gt;= 0)\n        assert torch.all(generated &lt; config.vocab_size)\n\n        # Check prompt is preserved\n        assert torch.equal(generated[:, :prompt.size(1)], prompt)\n\n    def test_different_batch_sizes(self, model, config):\n        \"\"\"Test model works with different batch sizes.\"\"\"\n        for batch_size in [1, 2, 4, 8]:\n            tokens = torch.randint(0, config.vocab_size, (batch_size, 16))\n\n            logits, z_logits = model(tokens, mode='training')\n\n            assert logits.shape[0] == batch_size\n            assert z_logits.shape[0] == batch_size\n\n    def test_different_sequence_lengths(self, model, config):\n        \"\"\"Test model works with different sequence lengths.\"\"\"\n        batch_size = 2\n\n        for seq_len in [8, 16, 32, 64]:\n            if seq_len &lt;= config.max_seq_len:\n                tokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\n                logits, z_logits = model(tokens, mode='training')\n\n                assert logits.shape[1] == seq_len\n                assert z_logits.shape == (batch_size, config.latent_dim)\n\n    def test_model_device_compatibility(self, model, config):\n        \"\"\"Test model works on different devices.\"\"\"\n        tokens = torch.randint(0, config.vocab_size, (2, 16))\n\n        # CPU test\n        model_cpu = model.to('cpu')\n        tokens_cpu = tokens.to('cpu')\n\n        logits_cpu, z_logits_cpu = model_cpu(tokens_cpu, mode='training')\n        assert logits_cpu.device.type == 'cpu'\n        assert z_logits_cpu.device.type == 'cpu'\n\n        # GPU test (if available)\n        if torch.cuda.is_available():\n            model_gpu = model.to('cuda')\n            tokens_gpu = tokens.to('cuda')\n\n            logits_gpu, z_logits_gpu = model_gpu(tokens_gpu, mode='training')\n            assert logits_gpu.device.type == 'cuda'\n            assert z_logits_gpu.device.type == 'cuda'\n</code></pre>"},{"location":"development/testing/#loss-function-tests","title":"Loss Function Tests","text":"<pre><code># tests/unit/test_losses.py\nimport pytest\nimport torch\nfrom free_transformer.losses import free_transformer_loss\n\nclass TestLosses:\n    @pytest.fixture\n    def sample_data(self):\n        batch_size, seq_len, vocab_size, latent_dim = 2, 16, 1000, 8\n\n        logits = torch.randn(batch_size, seq_len, vocab_size)\n        z_logits = torch.randn(batch_size, latent_dim)\n        targets = torch.randint(0, vocab_size, (batch_size, seq_len))\n\n        return logits, z_logits, targets, latent_dim\n\n    def test_free_transformer_loss_basic(self, sample_data):\n        \"\"\"Test basic loss computation.\"\"\"\n        logits, z_logits, targets, latent_dim = sample_data\n\n        loss_dict = free_transformer_loss(\n            logits=logits,\n            z_logits=z_logits,\n            targets=targets,\n            latent_dim=latent_dim,\n            kl_weight=0.1,\n            free_bits=0.5\n        )\n\n        # Check all expected keys are present\n        expected_keys = ['total_loss', 'recon_loss', 'kl_loss']\n        for key in expected_keys:\n            assert key in loss_dict\n            assert isinstance(loss_dict[key], torch.Tensor)\n            assert loss_dict[key].dim() == 0  # Scalar\n\n        # Check loss values are reasonable\n        assert loss_dict['total_loss'] &gt; 0\n        assert loss_dict['recon_loss'] &gt; 0\n        assert loss_dict['kl_loss'] &gt;= 0  # Can be zero due to free bits\n\n    def test_loss_gradients(self, sample_data):\n        \"\"\"Test that loss enables gradient computation.\"\"\"\n        logits, z_logits, targets, latent_dim = sample_data\n\n        # Make tensors require gradients\n        logits.requires_grad_(True)\n        z_logits.requires_grad_(True)\n\n        loss_dict = free_transformer_loss(\n            logits=logits,\n            z_logits=z_logits,\n            targets=targets,\n            latent_dim=latent_dim,\n            kl_weight=0.1,\n            free_bits=0.5\n        )\n\n        # Backward pass\n        loss_dict['total_loss'].backward()\n\n        # Check gradients exist\n        assert logits.grad is not None\n        assert z_logits.grad is not None\n        assert not torch.all(logits.grad == 0)\n        assert not torch.all(z_logits.grad == 0)\n\n    def test_kl_weight_effect(self, sample_data):\n        \"\"\"Test that KL weight affects total loss.\"\"\"\n        logits, z_logits, targets, latent_dim = sample_data\n\n        # Compute loss with different KL weights\n        loss_low = free_transformer_loss(\n            logits=logits, z_logits=z_logits, targets=targets,\n            latent_dim=latent_dim, kl_weight=0.01, free_bits=0.0\n        )\n\n        loss_high = free_transformer_loss(\n            logits=logits, z_logits=z_logits, targets=targets,\n            latent_dim=latent_dim, kl_weight=1.0, free_bits=0.0\n        )\n\n        # Higher KL weight should increase total loss (if KL &gt; 0)\n        if loss_low['kl_loss'] &gt; 0:\n            assert loss_high['total_loss'] &gt; loss_low['total_loss']\n\n    def test_free_bits_effect(self, sample_data):\n        \"\"\"Test that free bits affects KL loss.\"\"\"\n        logits, z_logits, targets, latent_dim = sample_data\n\n        # Compute loss with different free bits\n        loss_no_free = free_transformer_loss(\n            logits=logits, z_logits=z_logits, targets=targets,\n            latent_dim=latent_dim, kl_weight=0.1, free_bits=0.0\n        )\n\n        loss_with_free = free_transformer_loss(\n            logits=logits, z_logits=z_logits, targets=targets,\n            latent_dim=latent_dim, kl_weight=0.1, free_bits=1.0\n        )\n\n        # Free bits should increase KL loss (clamping effect)\n        assert loss_with_free['kl_loss'] &gt;= loss_no_free['kl_loss']\n</code></pre>"},{"location":"development/testing/#integration-tests","title":"Integration Tests","text":""},{"location":"development/testing/#training-pipeline-tests","title":"Training Pipeline Tests","text":"<pre><code># tests/integration/test_training.py\nimport pytest\nimport torch\nfrom torch.utils.data import DataLoader, TensorDataset\nfrom free_transformer import FreeTransformer, ModelConfig\nfrom free_transformer.losses import free_transformer_loss\n\nclass TestTrainingPipeline:\n    @pytest.fixture\n    def config(self):\n        return ModelConfig(\n            vocab_size=100,\n            hidden_dim=64,\n            num_layers=2,\n            num_heads=2,\n            latent_dim=4,\n            max_seq_len=32\n        )\n\n    @pytest.fixture\n    def model(self, config):\n        return FreeTransformer(config)\n\n    @pytest.fixture\n    def dataloader(self, config):\n        # Create synthetic data\n        num_samples = 50\n        seq_len = 16\n\n        data = torch.randint(0, config.vocab_size, (num_samples, seq_len))\n        dataset = TensorDataset(data)\n\n        return DataLoader(dataset, batch_size=8, shuffle=True)\n\n    def test_training_step(self, model, config, dataloader):\n        \"\"\"Test a single training step.\"\"\"\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-3)\n\n        model.train()\n\n        for batch in dataloader:\n            tokens = batch[0]\n\n            # Forward pass\n            logits, z_logits = model(tokens, mode='training')\n\n            # Compute loss\n            loss_dict = free_transformer_loss(\n                logits=logits,\n                z_logits=z_logits,\n                targets=tokens,\n                latent_dim=config.latent_dim,\n                kl_weight=0.1,\n                free_bits=0.5\n            )\n\n            loss = loss_dict['total_loss']\n\n            # Backward pass\n            optimizer.zero_grad()\n            loss.backward()\n            optimizer.step()\n\n            # Check loss is finite\n            assert torch.isfinite(loss)\n\n            # Only test one batch\n            break\n\n    def test_training_convergence(self, model, config, dataloader):\n        \"\"\"Test that model can overfit to small dataset.\"\"\"\n        optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n\n        model.train()\n\n        initial_loss = None\n        final_loss = None\n\n        # Train for several epochs\n        for epoch in range(10):\n            epoch_losses = []\n\n            for batch in dataloader:\n                tokens = batch[0]\n\n                logits, z_logits = model(tokens, mode='training')\n\n                loss_dict = free_transformer_loss(\n                    logits=logits,\n                    z_logits=z_logits,\n                    targets=tokens,\n                    latent_dim=config.latent_dim,\n                    kl_weight=0.01,  # Low KL weight for easier overfitting\n                    free_bits=0.1\n                )\n\n                loss = loss_dict['total_loss']\n                epoch_losses.append(loss.item())\n\n                optimizer.zero_grad()\n                loss.backward()\n                optimizer.step()\n\n            avg_loss = sum(epoch_losses) / len(epoch_losses)\n\n            if epoch == 0:\n                initial_loss = avg_loss\n            if epoch == 9:\n                final_loss = avg_loss\n\n        # Model should overfit (loss should decrease)\n        assert final_loss &lt; initial_loss\n        assert final_loss &lt; 5.0  # Reasonable threshold\n\n    def test_evaluation_mode(self, model, config, dataloader):\n        \"\"\"Test evaluation mode doesn't update parameters.\"\"\"\n        # Get initial parameters\n        initial_params = {name: param.clone() for name, param in model.named_parameters()}\n\n        model.eval()\n\n        with torch.no_grad():\n            for batch in dataloader:\n                tokens = batch[0]\n\n                # Forward pass in training mode (for loss computation)\n                logits, z_logits = model(tokens, mode='training')\n\n                # Compute loss (but don't backward)\n                loss_dict = free_transformer_loss(\n                    logits=logits,\n                    z_logits=z_logits,\n                    targets=tokens,\n                    latent_dim=config.latent_dim,\n                    kl_weight=0.1,\n                    free_bits=0.5\n                )\n\n                # Only test one batch\n                break\n\n        # Check parameters haven't changed\n        for name, param in model.named_parameters():\n            assert torch.equal(param, initial_params[name])\n</code></pre>"},{"location":"development/testing/#generation-tests","title":"Generation Tests","text":"<pre><code># tests/integration/test_generation.py\nimport pytest\nimport torch\nfrom free_transformer import FreeTransformer, ModelConfig\n\nclass TestGeneration:\n    @pytest.fixture\n    def config(self):\n        return ModelConfig(\n            vocab_size=100,\n            hidden_dim=64,\n            num_layers=2,\n            num_heads=2,\n            latent_dim=4,\n            max_seq_len=64\n        )\n\n    @pytest.fixture\n    def model(self, config):\n        model = FreeTransformer(config)\n        model.eval()\n        return model\n\n    def test_basic_generation(self, model, config):\n        \"\"\"Test basic generation functionality.\"\"\"\n        prompt = torch.randint(0, config.vocab_size, (1, 5))\n        max_new_tokens = 10\n\n        with torch.no_grad():\n            generated = model.generate(prompt, max_new_tokens=max_new_tokens)\n\n        # Check output properties\n        assert generated.shape == (1, 5 + max_new_tokens)\n        assert torch.all(generated &gt;= 0)\n        assert torch.all(generated &lt; config.vocab_size)\n        assert torch.equal(generated[:, :5], prompt)\n\n    def test_generation_determinism(self, model, config):\n        \"\"\"Test generation determinism with same seed.\"\"\"\n        prompt = torch.randint(0, config.vocab_size, (1, 5))\n\n        # Generate with same seed\n        torch.manual_seed(42)\n        gen1 = model.generate(prompt, max_new_tokens=10, temperature=0.0)\n\n        torch.manual_seed(42)\n        gen2 = model.generate(prompt, max_new_tokens=10, temperature=0.0)\n\n        # Should be identical with temperature=0\n        assert torch.equal(gen1, gen2)\n\n    def test_generation_diversity(self, model, config):\n        \"\"\"Test generation produces diverse outputs.\"\"\"\n        prompt = torch.randint(0, config.vocab_size, (1, 5))\n\n        generations = []\n        for _ in range(5):\n            with torch.no_grad():\n                gen = model.generate(\n                    prompt, \n                    max_new_tokens=20, \n                    temperature=1.0,\n                    do_sample=True\n                )\n            generations.append(gen)\n\n        # Check that not all generations are identical\n        all_same = all(torch.equal(generations[0], gen) for gen in generations[1:])\n        assert not all_same, \"All generations are identical - no diversity\"\n\n    def test_batch_generation(self, model, config):\n        \"\"\"Test generation with batch input.\"\"\"\n        batch_size = 3\n        prompt = torch.randint(0, config.vocab_size, (batch_size, 5))\n\n        with torch.no_grad():\n            generated = model.generate(prompt, max_new_tokens=10)\n\n        assert generated.shape == (batch_size, 15)\n\n        # Check each sequence in batch\n        for i in range(batch_size):\n            assert torch.equal(generated[i, :5], prompt[i])\n\n    def test_generation_parameters(self, model, config):\n        \"\"\"Test different generation parameters.\"\"\"\n        prompt = torch.randint(0, config.vocab_size, (1, 5))\n\n        # Test different temperatures\n        for temp in [0.1, 0.5, 1.0, 1.5]:\n            gen = model.generate(prompt, max_new_tokens=10, temperature=temp)\n            assert gen.shape == (1, 15)\n\n        # Test different top_k values\n        for top_k in [1, 5, 10, 20]:\n            gen = model.generate(prompt, max_new_tokens=10, top_k=top_k)\n            assert gen.shape == (1, 15)\n\n        # Test different top_p values\n        for top_p in [0.1, 0.5, 0.9, 1.0]:\n            gen = model.generate(prompt, max_new_tokens=10, top_p=top_p)\n            assert gen.shape == (1, 15)\n</code></pre>"},{"location":"development/testing/#performance-tests","title":"Performance Tests","text":""},{"location":"development/testing/#memory-and-speed-tests","title":"Memory and Speed Tests","text":"<pre><code># tests/test_performance.py\nimport pytest\nimport torch\nimport time\nimport psutil\nimport os\nfrom free_transformer import FreeTransformer, ModelConfig\n\nclass TestPerformance:\n    @pytest.fixture\n    def small_config(self):\n        return ModelConfig(\n            vocab_size=1000,\n            hidden_dim=128,\n            num_layers=4,\n            num_heads=4,\n            latent_dim=8\n        )\n\n    @pytest.fixture\n    def medium_config(self):\n        return ModelConfig(\n            vocab_size=10000,\n            hidden_dim=512,\n            num_layers=12,\n            num_heads=8,\n            latent_dim=32\n        )\n\n    def test_memory_usage(self, small_config):\n        \"\"\"Test memory usage is reasonable.\"\"\"\n        process = psutil.Process(os.getpid())\n        initial_memory = process.memory_info().rss / 1024 / 1024  # MB\n\n        model = FreeTransformer(small_config)\n\n        after_model_memory = process.memory_info().rss / 1024 / 1024  # MB\n        model_memory = after_model_memory - initial_memory\n\n        # Model should use reasonable amount of memory (less than 500MB for small model)\n        assert model_memory &lt; 500, f\"Model uses too much memory: {model_memory:.1f}MB\"\n\n    def test_forward_speed(self, small_config):\n        \"\"\"Test forward pass speed.\"\"\"\n        model = FreeTransformer(small_config)\n        model.eval()\n\n        batch_size, seq_len = 4, 128\n        tokens = torch.randint(0, small_config.vocab_size, (batch_size, seq_len))\n\n        # Warmup\n        with torch.no_grad():\n            for _ in range(5):\n                model(tokens, mode='training')\n\n        # Time forward passes\n        num_runs = 20\n        start_time = time.time()\n\n        with torch.no_grad():\n            for _ in range(num_runs):\n                model(tokens, mode='training')\n\n        end_time = time.time()\n        avg_time = (end_time - start_time) / num_runs\n\n        # Should be reasonably fast (less than 100ms for small model)\n        assert avg_time &lt; 0.1, f\"Forward pass too slow: {avg_time:.3f}s\"\n\n    def test_generation_speed(self, small_config):\n        \"\"\"Test generation speed.\"\"\"\n        model = FreeTransformer(small_config)\n        model.eval()\n\n        prompt = torch.randint(0, small_config.vocab_size, (1, 10))\n        max_new_tokens = 50\n\n        # Warmup\n        with torch.no_grad():\n            model.generate(prompt, max_new_tokens=10)\n\n        # Time generation\n        start_time = time.time()\n\n        with torch.no_grad():\n            generated = model.generate(prompt, max_new_tokens=max_new_tokens)\n\n        end_time = time.time()\n        generation_time = end_time - start_time\n        tokens_per_second = max_new_tokens / generation_time\n\n        # Should generate at reasonable speed (at least 10 tokens/sec)\n        assert tokens_per_second &gt; 10, f\"Generation too slow: {tokens_per_second:.1f} tokens/sec\"\n\n    @pytest.mark.skipif(not torch.cuda.is_available(), reason=\"CUDA not available\")\n    def test_gpu_utilization(self, small_config):\n        \"\"\"Test GPU utilization.\"\"\"\n        device = torch.device('cuda')\n        model = FreeTransformer(small_config).to(device)\n\n        batch_size, seq_len = 8, 256\n        tokens = torch.randint(0, small_config.vocab_size, (batch_size, seq_len)).to(device)\n\n        # Clear GPU cache\n        torch.cuda.empty_cache()\n        initial_memory = torch.cuda.memory_allocated(device)\n\n        # Forward pass\n        with torch.no_grad():\n            logits, z_logits = model(tokens, mode='training')\n\n        peak_memory = torch.cuda.max_memory_allocated(device)\n        memory_used = (peak_memory - initial_memory) / 1024 / 1024  # MB\n\n        # Should use reasonable GPU memory\n        assert memory_used &lt; 1000, f\"Uses too much GPU memory: {memory_used:.1f}MB\"\n</code></pre>"},{"location":"development/testing/#test-utilities","title":"Test Utilities","text":""},{"location":"development/testing/#common-test-fixtures","title":"Common Test Fixtures","text":"<pre><code># tests/conftest.py\nimport pytest\nimport torch\nfrom free_transformer import FreeTransformer, ModelConfig\n\n@pytest.fixture\ndef device():\n    \"\"\"Get available device.\"\"\"\n    return torch.device('cuda' if torch.cuda.is_available() else 'cpu')\n\n@pytest.fixture\ndef small_config():\n    \"\"\"Small model configuration for testing.\"\"\"\n    return ModelConfig(\n        vocab_size=100,\n        hidden_dim=64,\n        num_layers=2,\n        num_heads=2,\n        latent_dim=4,\n        max_seq_len=32\n    )\n\n@pytest.fixture\ndef medium_config():\n    \"\"\"Medium model configuration for testing.\"\"\"\n    return ModelConfig(\n        vocab_size=1000,\n        hidden_dim=256,\n        num_layers=6,\n        num_heads=4,\n        latent_dim=16,\n        max_seq_len=128\n    )\n\n@pytest.fixture\ndef sample_tokens(small_config):\n    \"\"\"Sample token sequences for testing.\"\"\"\n    return torch.randint(0, small_config.vocab_size, (4, 16))\n\n@pytest.fixture\ndef trained_model(small_config, sample_tokens):\n    \"\"\"A model that has been trained for a few steps.\"\"\"\n    model = FreeTransformer(small_config)\n    optimizer = torch.optim.Adam(model.parameters(), lr=1e-2)\n\n    model.train()\n\n    # Train for a few steps\n    for _ in range(10):\n        logits, z_logits = model(sample_tokens, mode='training')\n\n        from free_transformer.losses import free_transformer_loss\n        loss_dict = free_transformer_loss(\n            logits=logits,\n            z_logits=z_logits,\n            targets=sample_tokens,\n            latent_dim=small_config.latent_dim,\n            kl_weight=0.1,\n            free_bits=0.5\n        )\n\n        loss = loss_dict['total_loss']\n\n        optimizer.zero_grad()\n        loss.backward()\n        optimizer.step()\n\n    model.eval()\n    return model\n</code></pre>"},{"location":"development/testing/#continuous-integration","title":"Continuous Integration","text":""},{"location":"development/testing/#github-actions-configuration","title":"GitHub Actions Configuration","text":"<pre><code># .github/workflows/test.yml\nname: Tests\n\non:\n  push:\n    branches: [ main, develop ]\n  pull_request:\n    branches: [ main ]\n\njobs:\n  test:\n    runs-on: ubuntu-latest\n    strategy:\n      matrix:\n        python-version: [3.11, 3.12]\n\n    steps:\n    - uses: actions/checkout@v3\n\n    - name: Set up Python ${{ matrix.python-version }}\n      uses: actions/setup-python@v4\n      with:\n        python-version: ${{ matrix.python-version }}\n\n    - name: Install UV\n      run: |\n        curl -LsSf https://astral.sh/uv/install.sh | sh\n        echo \"$HOME/.cargo/bin\" &gt;&gt; $GITHUB_PATH\n\n    - name: Install dependencies\n      run: |\n        uv venv --python ${{ matrix.python-version }}\n        source .venv/bin/activate\n        uv pip install -e \".[dev]\"\n\n    - name: Run tests\n      run: |\n        source .venv/bin/activate\n        make test\n\n    - name: Run quality checks\n      run: |\n        source .venv/bin/activate\n        make quality\n\n    - name: Upload coverage\n      uses: codecov/codecov-action@v3\n      with:\n        file: ./coverage.xml\n        fail_ci_if_error: true\n</code></pre>"},{"location":"development/testing/#best-practices","title":"Best Practices","text":""},{"location":"development/testing/#writing-good-tests","title":"Writing Good Tests","text":"<ol> <li>Test one thing at a time: Each test should focus on a single behavior</li> <li>Use descriptive names: Test names should clearly describe what they test</li> <li>Arrange-Act-Assert: Structure tests with clear setup, execution, and verification</li> <li>Use fixtures: Share common setup code using pytest fixtures</li> <li>Test edge cases: Include tests for boundary conditions and error cases</li> <li>Mock external dependencies: Use mocks for external services or complex dependencies</li> </ol>"},{"location":"development/testing/#test-coverage","title":"Test Coverage","text":"<p>Aim for high test coverage but focus on quality over quantity:</p> <pre><code># Generate coverage report\npytest --cov=src/free_transformer --cov-report=html\n\n# View coverage report\nopen htmlcov/index.html\n</code></pre> <p>Target coverage levels: - Unit tests: 90%+ coverage of core components - Integration tests: Cover main user workflows - End-to-end tests: Test complete pipelines</p>"},{"location":"development/testing/#performance-testing","title":"Performance Testing","text":"<p>Include performance tests to catch regressions:</p> <pre><code>@pytest.mark.performance\ndef test_training_speed():\n    \"\"\"Test training speed doesn't regress.\"\"\"\n    # Implementation here\n    pass\n\n# Run performance tests separately\npytest -m performance\n</code></pre>"},{"location":"development/testing/#troubleshooting-tests","title":"Troubleshooting Tests","text":""},{"location":"development/testing/#common-issues","title":"Common Issues","text":"<p>Tests fail on CI but pass locally - Check Python version compatibility - Verify all dependencies are installed - Check for platform-specific issues</p> <p>Flaky tests - Use fixed random seeds - Mock time-dependent operations - Increase timeouts for slow operations</p> <p>Memory issues in tests - Use smaller models for testing - Clear GPU cache between tests - Use pytest-xdist for parallel execution</p> <p>Slow test suite - Profile tests to find bottlenecks - Use pytest-benchmark for performance tests - Consider test parallelization</p>"},{"location":"development/testing/#next-steps","title":"Next Steps","text":"<ul> <li>Code Quality: Code quality tools and practices</li> <li>Contributing: How to contribute to the project</li> <li>Training Guide: Training best practices</li> </ul>"},{"location":"examples/basic/","title":"Basic Usage Examples","text":"<p>This page provides practical examples for using the Free Transformer in various scenarios.</p>"},{"location":"examples/basic/#model-creation-and-basic-usage","title":"Model Creation and Basic Usage","text":""},{"location":"examples/basic/#creating-a-model","title":"Creating a Model","text":"<pre><code>import torch\nfrom free_transformer import FreeTransformer, ModelConfig\n\n# Create configuration\nconfig = ModelConfig(\n    vocab_size=50000,\n    hidden_dim=512,\n    num_layers=12,\n    num_heads=8,\n    latent_dim=32,\n    max_seq_len=1024\n)\n\n# Initialize model\nmodel = FreeTransformer(config)\nprint(f\"Model has {sum(p.numel() for p in model.parameters()):,} parameters\")\n</code></pre>"},{"location":"examples/basic/#training-mode","title":"Training Mode","text":"<pre><code># Prepare training data\nbatch_size, seq_len = 4, 128\ntokens = torch.randint(0, config.vocab_size, (batch_size, seq_len))\n\n# Forward pass in training mode\nmodel.train()\nlogits, z_logits = model(tokens, mode='training')\n\nprint(f\"Output logits shape: {logits.shape}\")  # [4, 128, 50000]\nprint(f\"Latent logits shape: {z_logits.shape}\")  # [4, 32]\n</code></pre>"},{"location":"examples/basic/#inference-mode","title":"Inference Mode","text":"<pre><code># Prepare prompt\nprompt = torch.randint(0, config.vocab_size, (1, 20))\n\n# Generate text\nmodel.eval()\nwith torch.no_grad():\n    generated = model.generate(\n        prompt,\n        max_new_tokens=100,\n        temperature=0.8,\n        top_k=40,\n        do_sample=True\n    )\n\nprint(f\"Generated sequence length: {generated.shape[1]}\")  # 120 (20 + 100)\n</code></pre>"},{"location":"examples/basic/#text-generation-examples","title":"Text Generation Examples","text":""},{"location":"examples/basic/#basic-generation","title":"Basic Generation","text":"<pre><code>def generate_text(model, tokenizer, prompt_text, max_length=100):\n    \"\"\"Generate text from a prompt string.\"\"\"\n    # Tokenize prompt\n    prompt_tokens = tokenizer.encode(prompt_text, return_tensors='pt')\n\n    # Generate\n    model.eval()\n    with torch.no_grad():\n        generated_tokens = model.generate(\n            prompt_tokens,\n            max_new_tokens=max_length,\n            temperature=0.8,\n            top_k=40,\n            pad_token_id=tokenizer.pad_token_id\n        )\n\n    # Decode\n    generated_text = tokenizer.decode(generated_tokens[0], skip_special_tokens=True)\n    return generated_text\n\n# Example usage\nprompt = \"The future of artificial intelligence\"\ngenerated = generate_text(model, tokenizer, prompt)\nprint(generated)\n</code></pre>"},{"location":"examples/basic/#controlled-generation-with-different-plans","title":"Controlled Generation with Different Plans","text":"<pre><code>def generate_with_different_plans(model, prompt, num_plans=5):\n    \"\"\"Generate multiple texts with different latent plans.\"\"\"\n    generations = []\n\n    model.eval()\n    with torch.no_grad():\n        for i in range(num_plans):\n            # Each generation will sample a different latent plan\n            generated = model.generate(\n                prompt,\n                max_new_tokens=50,\n                temperature=0.8,\n                top_k=40\n            )\n            generations.append(generated)\n\n    return generations\n\n# Example usage\nprompt = torch.randint(0, config.vocab_size, (1, 10))\ndifferent_generations = generate_with_different_plans(model, prompt)\nprint(f\"Generated {len(different_generations)} different continuations\")\n</code></pre>"},{"location":"examples/basic/#temperature-and-sampling-control","title":"Temperature and Sampling Control","text":"<pre><code>def compare_sampling_strategies(model, prompt):\n    \"\"\"Compare different sampling strategies.\"\"\"\n    strategies = [\n        {\"temperature\": 0.1, \"top_k\": None, \"name\": \"Low temperature\"},\n        {\"temperature\": 0.8, \"top_k\": 40, \"name\": \"Balanced\"},\n        {\"temperature\": 1.2, \"top_k\": 100, \"name\": \"High temperature\"},\n        {\"temperature\": 0.0, \"top_k\": None, \"name\": \"Greedy (deterministic)\"}\n    ]\n\n    results = {}\n    model.eval()\n\n    for strategy in strategies:\n        with torch.no_grad():\n            generated = model.generate(\n                prompt,\n                max_new_tokens=30,\n                temperature=strategy[\"temperature\"],\n                top_k=strategy[\"top_k\"],\n                do_sample=strategy[\"temperature\"] &gt; 0\n            )\n        results[strategy[\"name\"]] = generated\n\n    return results\n\n# Example usage\nprompt = torch.randint(0, config.vocab_size, (1, 15))\nsampling_results = compare_sampling_strategies(model, prompt)\n</code></pre>"},{"location":"examples/basic/#training-examples","title":"Training Examples","text":""},{"location":"examples/basic/#simple-training-loop","title":"Simple Training Loop","text":"<pre><code>import torch.nn.functional as F\nfrom free_transformer.losses import free_transformer_loss\n\ndef train_epoch(model, dataloader, optimizer, config):\n    \"\"\"Train for one epoch.\"\"\"\n    model.train()\n    total_loss = 0\n    num_batches = 0\n\n    for batch in dataloader:\n        tokens = batch['input_ids']\n\n        # Forward pass\n        logits, z_logits = model(tokens, mode='training')\n\n        # Compute loss\n        loss_dict = free_transformer_loss(\n            logits=logits,\n            z_logits=z_logits,\n            targets=tokens,\n            latent_dim=config.latent_dim,\n            kl_weight=0.1,\n            free_bits=0.5\n        )\n\n        loss = loss_dict['total_loss']\n\n        # Backward pass\n        optimizer.zero_grad()\n        loss.backward()\n        torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n        optimizer.step()\n\n        total_loss += loss.item()\n        num_batches += 1\n\n        if num_batches % 100 == 0:\n            print(f\"Batch {num_batches}, Loss: {loss.item():.4f}\")\n\n    return total_loss / num_batches\n\n# Example usage\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\navg_loss = train_epoch(model, train_dataloader, optimizer, config)\nprint(f\"Average loss: {avg_loss:.4f}\")\n</code></pre>"},{"location":"examples/basic/#training-with-validation","title":"Training with Validation","text":"<pre><code>def train_with_validation(model, train_loader, val_loader, num_epochs=5):\n    \"\"\"Training loop with validation.\"\"\"\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n    scheduler = torch.optim.lr_scheduler.CosineAnnealingLR(optimizer, num_epochs)\n\n    best_val_loss = float('inf')\n\n    for epoch in range(num_epochs):\n        # Training\n        train_loss = train_epoch(model, train_loader, optimizer, config)\n\n        # Validation\n        val_loss = evaluate_model(model, val_loader)\n\n        # Learning rate scheduling\n        scheduler.step()\n\n        print(f\"Epoch {epoch+1}/{num_epochs}\")\n        print(f\"Train Loss: {train_loss:.4f}, Val Loss: {val_loss:.4f}\")\n        print(f\"Learning Rate: {optimizer.param_groups[0]['lr']:.6f}\")\n\n        # Save best model\n        if val_loss &lt; best_val_loss:\n            best_val_loss = val_loss\n            torch.save(model.state_dict(), 'best_model.pt')\n            print(\"Saved new best model!\")\n\n        print(\"-\" * 50)\n\ndef evaluate_model(model, dataloader):\n    \"\"\"Evaluate model on validation set.\"\"\"\n    model.eval()\n    total_loss = 0\n    num_batches = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            tokens = batch['input_ids']\n            logits, z_logits = model(tokens, mode='training')\n\n            loss_dict = free_transformer_loss(\n                logits=logits,\n                z_logits=z_logits,\n                targets=tokens,\n                latent_dim=config.latent_dim,\n                kl_weight=0.1,\n                free_bits=0.5\n            )\n\n            total_loss += loss_dict['total_loss'].item()\n            num_batches += 1\n\n    return total_loss / num_batches\n</code></pre>"},{"location":"examples/basic/#model-comparison","title":"Model Comparison","text":""},{"location":"examples/basic/#compare-with-baseline","title":"Compare with Baseline","text":"<pre><code>from free_transformer import BaselineTransformer\n\ndef compare_models(free_model, baseline_model, test_data):\n    \"\"\"Compare Free Transformer with baseline.\"\"\"\n    results = {}\n\n    for name, model in [(\"Free Transformer\", free_model), (\"Baseline\", baseline_model)]:\n        model.eval()\n        total_loss = 0\n        num_samples = 0\n\n        with torch.no_grad():\n            for batch in test_data:\n                tokens = batch['input_ids']\n\n                if name == \"Free Transformer\":\n                    logits, _ = model(tokens, mode='training')\n                else:\n                    logits = model(tokens)\n\n                loss = F.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    tokens.view(-1),\n                    reduction='sum'\n                )\n\n                total_loss += loss.item()\n                num_samples += tokens.numel()\n\n        perplexity = torch.exp(torch.tensor(total_loss / num_samples))\n        results[name] = {\n            'loss': total_loss / num_samples,\n            'perplexity': perplexity.item()\n        }\n\n    return results\n\n# Example usage\nbaseline_config = ModelConfig(\n    vocab_size=config.vocab_size,\n    hidden_dim=config.hidden_dim,\n    num_layers=config.num_layers,\n    num_heads=config.num_heads,\n    max_seq_len=config.max_seq_len\n)\nbaseline_model = BaselineTransformer(baseline_config)\n\ncomparison_results = compare_models(model, baseline_model, test_dataloader)\nprint(\"Model Comparison Results:\")\nfor model_name, metrics in comparison_results.items():\n    print(f\"{model_name}: Perplexity = {metrics['perplexity']:.2f}\")\n</code></pre>"},{"location":"examples/basic/#utility-functions","title":"Utility Functions","text":""},{"location":"examples/basic/#model-information","title":"Model Information","text":"<pre><code>def model_info(model):\n    \"\"\"Print detailed model information.\"\"\"\n    total_params = sum(p.numel() for p in model.parameters())\n    trainable_params = sum(p.numel() for p in model.parameters() if p.requires_grad)\n\n    print(f\"Model: {model.__class__.__name__}\")\n    print(f\"Total parameters: {total_params:,}\")\n    print(f\"Trainable parameters: {trainable_params:,}\")\n    print(f\"Model size: {total_params * 4 / 1024**2:.1f} MB (float32)\")\n\n    # Layer breakdown\n    for name, module in model.named_modules():\n        if hasattr(module, 'weight') and module.weight is not None:\n            params = module.weight.numel()\n            if hasattr(module, 'bias') and module.bias is not None:\n                params += module.bias.numel()\n            print(f\"  {name}: {params:,} parameters\")\n\nmodel_info(model)\n</code></pre>"},{"location":"examples/basic/#save-and-load-models","title":"Save and Load Models","text":"<pre><code>def save_model(model, config, path):\n    \"\"\"Save model and configuration.\"\"\"\n    checkpoint = {\n        'model_state_dict': model.state_dict(),\n        'config': config.__dict__,\n        'model_class': model.__class__.__name__\n    }\n    torch.save(checkpoint, path)\n    print(f\"Model saved to {path}\")\n\ndef load_model(path):\n    \"\"\"Load model and configuration.\"\"\"\n    checkpoint = torch.load(path, map_location='cpu')\n\n    # Recreate config\n    config = ModelConfig(**checkpoint['config'])\n\n    # Recreate model\n    if checkpoint['model_class'] == 'FreeTransformer':\n        model = FreeTransformer(config)\n    else:\n        model = BaselineTransformer(config)\n\n    # Load weights\n    model.load_state_dict(checkpoint['model_state_dict'])\n\n    return model, config\n\n# Example usage\nsave_model(model, config, 'my_model.pt')\nloaded_model, loaded_config = load_model('my_model.pt')\n</code></pre>"},{"location":"examples/basic/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Training: Advanced training techniques</li> <li>Evaluation: Comprehensive model evaluation</li> <li>API Reference: Complete API documentation</li> </ul>"},{"location":"examples/custom-training/","title":"Custom Training Examples","text":"<p>This page provides advanced examples for customizing Free Transformer training.</p>"},{"location":"examples/custom-training/#custom-loss-functions","title":"Custom Loss Functions","text":""},{"location":"examples/custom-training/#adding-custom-loss-components","title":"Adding Custom Loss Components","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom free_transformer.losses import free_transformer_loss\n\ndef custom_free_transformer_loss(logits, z_logits, targets, config, **kwargs):\n    \"\"\"Custom loss function with additional components.\"\"\"\n\n    # Standard Free Transformer loss\n    base_loss = free_transformer_loss(\n        logits=logits,\n        z_logits=z_logits,\n        targets=targets,\n        latent_dim=config.latent_dim,\n        kl_weight=config.kl_weight,\n        free_bits=config.free_bits\n    )\n\n    # Custom loss components\n    custom_losses = {}\n\n    # 1. Diversity loss - encourage different plans for different sequences\n    if 'diversity_weight' in kwargs:\n        diversity_loss = compute_diversity_loss(z_logits)\n        custom_losses['diversity_loss'] = diversity_loss\n        base_loss['total_loss'] += kwargs['diversity_weight'] * diversity_loss\n\n    # 2. Consistency loss - similar inputs should have similar plans\n    if 'consistency_weight' in kwargs and 'similarity_matrix' in kwargs:\n        consistency_loss = compute_consistency_loss(z_logits, kwargs['similarity_matrix'])\n        custom_losses['consistency_loss'] = consistency_loss\n        base_loss['total_loss'] += kwargs['consistency_weight'] * consistency_loss\n\n    # 3. Sparsity loss - encourage sparse plan usage\n    if 'sparsity_weight' in kwargs:\n        sparsity_loss = compute_sparsity_loss(z_logits)\n        custom_losses['sparsity_loss'] = sparsity_loss\n        base_loss['total_loss'] += kwargs['sparsity_weight'] * sparsity_loss\n\n    # Combine all losses\n    base_loss.update(custom_losses)\n    return base_loss\n\ndef compute_diversity_loss(z_logits):\n    \"\"\"Encourage diversity in latent plans.\"\"\"\n    # Convert to probabilities\n    probs = torch.sigmoid(z_logits)\n\n    # Compute pairwise distances\n    batch_size = probs.size(0)\n    distances = torch.cdist(probs, probs, p=2)\n\n    # Encourage larger distances (more diversity)\n    diversity_loss = -distances.mean()\n    return diversity_loss\n\ndef compute_consistency_loss(z_logits, similarity_matrix):\n    \"\"\"Encourage consistent plans for similar inputs.\"\"\"\n    probs = torch.sigmoid(z_logits)\n\n    # Compute plan distances\n    plan_distances = torch.cdist(probs, probs, p=2)\n\n    # Consistency loss: similar inputs should have similar plans\n    consistency_loss = (similarity_matrix * plan_distances).mean()\n    return consistency_loss\n\ndef compute_sparsity_loss(z_logits):\n    \"\"\"Encourage sparse plan usage.\"\"\"\n    probs = torch.sigmoid(z_logits)\n\n    # L1 penalty on probabilities (encourages 0 or 1)\n    sparsity_loss = torch.abs(probs - 0.5).mean()\n    return -sparsity_loss  # Negative because we want to maximize sparsity\n</code></pre>"},{"location":"examples/custom-training/#using-custom-loss-in-training","title":"Using Custom Loss in Training","text":"<pre><code>def train_with_custom_loss(model, dataloader, config):\n    \"\"\"Training loop with custom loss function.\"\"\"\n    optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n\n    for epoch in range(config.num_epochs):\n        for batch_idx, batch in enumerate(dataloader):\n            tokens = batch['input_ids']\n\n            # Forward pass\n            logits, z_logits = model(tokens, mode='training')\n\n            # Compute similarity matrix (example: based on first few tokens)\n            similarity_matrix = compute_input_similarity(tokens)\n\n            # Custom loss with additional components\n            loss_dict = custom_free_transformer_loss(\n                logits=logits,\n                z_logits=z_logits,\n                targets=tokens,\n                config=config,\n                diversity_weight=0.1,\n                consistency_weight=0.05,\n                sparsity_weight=0.02,\n                similarity_matrix=similarity_matrix\n            )\n\n            total_loss = loss_dict['total_loss']\n\n            # Backward pass\n            optimizer.zero_grad()\n            total_loss.backward()\n            torch.nn.utils.clip_grad_norm_(model.parameters(), 1.0)\n            optimizer.step()\n\n            # Logging\n            if batch_idx % 100 == 0:\n                print(f\"Epoch {epoch}, Batch {batch_idx}\")\n                print(f\"  Total Loss: {total_loss.item():.4f}\")\n                print(f\"  Recon Loss: {loss_dict['recon_loss'].item():.4f}\")\n                print(f\"  KL Loss: {loss_dict['kl_loss'].item():.4f}\")\n                if 'diversity_loss' in loss_dict:\n                    print(f\"  Diversity Loss: {loss_dict['diversity_loss'].item():.4f}\")\n\ndef compute_input_similarity(tokens):\n    \"\"\"Compute similarity matrix based on input tokens.\"\"\"\n    batch_size = tokens.size(0)\n    similarity_matrix = torch.zeros(batch_size, batch_size)\n\n    for i in range(batch_size):\n        for j in range(batch_size):\n            # Simple similarity: overlap in first 10 tokens\n            overlap = (tokens[i, :10] == tokens[j, :10]).float().mean()\n            similarity_matrix[i, j] = overlap\n\n    return similarity_matrix.to(tokens.device)\n</code></pre>"},{"location":"examples/custom-training/#custom-training-schedules","title":"Custom Training Schedules","text":""},{"location":"examples/custom-training/#kl-annealing-schedule","title":"KL Annealing Schedule","text":"<pre><code>class KLAnnealingScheduler:\n    \"\"\"Custom KL weight annealing scheduler.\"\"\"\n\n    def __init__(self, initial_weight=0.0, final_weight=1.0, annealing_steps=10000, schedule_type='linear'):\n        self.initial_weight = initial_weight\n        self.final_weight = final_weight\n        self.annealing_steps = annealing_steps\n        self.schedule_type = schedule_type\n        self.step_count = 0\n\n    def get_kl_weight(self):\n        \"\"\"Get current KL weight.\"\"\"\n        if self.step_count &gt;= self.annealing_steps:\n            return self.final_weight\n\n        progress = self.step_count / self.annealing_steps\n\n        if self.schedule_type == 'linear':\n            weight = self.initial_weight + progress * (self.final_weight - self.initial_weight)\n        elif self.schedule_type == 'cosine':\n            weight = self.initial_weight + 0.5 * (self.final_weight - self.initial_weight) * (1 - torch.cos(torch.tensor(progress * 3.14159)))\n        elif self.schedule_type == 'exponential':\n            weight = self.initial_weight * (self.final_weight / self.initial_weight) ** progress\n        else:\n            raise ValueError(f\"Unknown schedule type: {self.schedule_type}\")\n\n        return float(weight)\n\n    def step(self):\n        \"\"\"Update step count.\"\"\"\n        self.step_count += 1\n\n# Usage in training loop\nkl_scheduler = KLAnnealingScheduler(\n    initial_weight=0.0,\n    final_weight=0.1,\n    annealing_steps=5000,\n    schedule_type='cosine'\n)\n\nfor batch in dataloader:\n    # Get current KL weight\n    current_kl_weight = kl_scheduler.get_kl_weight()\n\n    # Use in loss computation\n    loss_dict = free_transformer_loss(\n        logits=logits,\n        z_logits=z_logits,\n        targets=targets,\n        latent_dim=config.latent_dim,\n        kl_weight=current_kl_weight,\n        free_bits=config.free_bits\n    )\n\n    # Update scheduler\n    kl_scheduler.step()\n</code></pre>"},{"location":"examples/custom-training/#free-bits-scheduling","title":"Free Bits Scheduling","text":"<pre><code>class FreeBitsScheduler:\n    \"\"\"Dynamic free bits scheduling.\"\"\"\n\n    def __init__(self, initial_bits=2.0, final_bits=0.5, schedule_steps=8000):\n        self.initial_bits = initial_bits\n        self.final_bits = final_bits\n        self.schedule_steps = schedule_steps\n        self.step_count = 0\n        self.kl_history = []\n\n    def get_free_bits(self, current_kl=None):\n        \"\"\"Get current free bits threshold.\"\"\"\n        if current_kl is not None:\n            self.kl_history.append(current_kl)\n\n        # Adaptive scheduling based on KL history\n        if len(self.kl_history) &gt; 100:\n            recent_kl = torch.tensor(self.kl_history[-100:]).mean()\n\n            # If KL is too low, increase free bits\n            if recent_kl &lt; 0.1:\n                return min(self.initial_bits, self.get_scheduled_bits() + 0.5)\n            # If KL is too high, decrease free bits\n            elif recent_kl &gt; 2.0:\n                return max(self.final_bits, self.get_scheduled_bits() - 0.5)\n\n        return self.get_scheduled_bits()\n\n    def get_scheduled_bits(self):\n        \"\"\"Get scheduled free bits value.\"\"\"\n        if self.step_count &gt;= self.schedule_steps:\n            return self.final_bits\n\n        progress = self.step_count / self.schedule_steps\n        return self.initial_bits + progress * (self.final_bits - self.initial_bits)\n\n    def step(self):\n        \"\"\"Update step count.\"\"\"\n        self.step_count += 1\n</code></pre>"},{"location":"examples/custom-training/#custom-data-loading","title":"Custom Data Loading","text":""},{"location":"examples/custom-training/#advanced-data-pipeline","title":"Advanced Data Pipeline","text":"<pre><code>import torch\nfrom torch.utils.data import Dataset, DataLoader\nimport numpy as np\n\nclass AdvancedSyntheticDataset(Dataset):\n    \"\"\"Advanced synthetic dataset with custom features.\"\"\"\n\n    def __init__(self, sequences, config):\n        self.sequences = sequences\n        self.config = config\n        self.max_length = config.max_seq_len\n\n        # Precompute sequence features\n        self.sequence_features = self._compute_features()\n\n    def _compute_features(self):\n        \"\"\"Compute features for each sequence.\"\"\"\n        features = []\n        for seq in self.sequences:\n            feature = {\n                'length': len(seq),\n                'unique_tokens': len(set(seq)),\n                'repetition_ratio': self._compute_repetition_ratio(seq),\n                'entropy': self._compute_entropy(seq),\n            }\n            features.append(feature)\n        return features\n\n    def _compute_repetition_ratio(self, sequence):\n        \"\"\"Compute repetition ratio in sequence.\"\"\"\n        if len(sequence) &lt; 2:\n            return 0.0\n\n        repeats = sum(1 for i in range(1, len(sequence)) if sequence[i] == sequence[i-1])\n        return repeats / (len(sequence) - 1)\n\n    def _compute_entropy(self, sequence):\n        \"\"\"Compute entropy of sequence.\"\"\"\n        if not sequence:\n            return 0.0\n\n        from collections import Counter\n        counts = Counter(sequence)\n        total = len(sequence)\n\n        entropy = 0\n        for count in counts.values():\n            prob = count / total\n            entropy -= prob * np.log2(prob)\n\n        return entropy\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n        features = self.sequence_features[idx]\n\n        # Truncate or pad sequence\n        if len(sequence) &gt; self.max_length:\n            sequence = sequence[:self.max_length]\n        else:\n            # Pad with special token (assuming 0 is padding)\n            sequence = sequence + [0] * (self.max_length - len(sequence))\n\n        return {\n            'input_ids': torch.tensor(sequence, dtype=torch.long),\n            'attention_mask': torch.tensor([1] * len(self.sequences[idx]) + [0] * max(0, self.max_length - len(self.sequences[idx])), dtype=torch.long),\n            'features': features,\n            'original_length': len(self.sequences[idx])\n        }\n\nclass CurriculumDataLoader:\n    \"\"\"Data loader with curriculum learning.\"\"\"\n\n    def __init__(self, dataset, batch_size, curriculum_stages):\n        self.dataset = dataset\n        self.batch_size = batch_size\n        self.curriculum_stages = curriculum_stages\n        self.current_stage = 0\n        self.stage_step = 0\n\n    def get_current_dataloader(self):\n        \"\"\"Get data loader for current curriculum stage.\"\"\"\n        stage_config = self.curriculum_stages[self.current_stage]\n\n        # Filter dataset based on current stage criteria\n        filtered_indices = self._filter_indices(stage_config)\n\n        # Create subset\n        subset = torch.utils.data.Subset(self.dataset, filtered_indices)\n\n        return DataLoader(\n            subset,\n            batch_size=self.batch_size,\n            shuffle=True,\n            collate_fn=self._collate_fn\n        )\n\n    def _filter_indices(self, stage_config):\n        \"\"\"Filter dataset indices based on stage criteria.\"\"\"\n        indices = []\n\n        for idx, features in enumerate(self.dataset.sequence_features):\n            # Check if sequence meets stage criteria\n            if (features['length'] &gt;= stage_config.get('min_length', 0) and\n                features['length'] &lt;= stage_config.get('max_length', float('inf')) and\n                features['entropy'] &gt;= stage_config.get('min_entropy', 0) and\n                features['entropy'] &lt;= stage_config.get('max_entropy', float('inf'))):\n                indices.append(idx)\n\n        return indices\n\n    def _collate_fn(self, batch):\n        \"\"\"Custom collate function.\"\"\"\n        input_ids = torch.stack([item['input_ids'] for item in batch])\n        attention_mask = torch.stack([item['attention_mask'] for item in batch])\n\n        return {\n            'input_ids': input_ids,\n            'attention_mask': attention_mask,\n            'features': [item['features'] for item in batch],\n            'original_lengths': [item['original_length'] for item in batch]\n        }\n\n    def advance_stage(self):\n        \"\"\"Advance to next curriculum stage.\"\"\"\n        if self.current_stage &lt; len(self.curriculum_stages) - 1:\n            self.current_stage += 1\n            self.stage_step = 0\n            print(f\"Advanced to curriculum stage {self.current_stage}\")\n\n    def step(self):\n        \"\"\"Update step count and check for stage advancement.\"\"\"\n        self.stage_step += 1\n\n        stage_config = self.curriculum_stages[self.current_stage]\n        if self.stage_step &gt;= stage_config.get('steps', float('inf')):\n            self.advance_stage()\n\n# Usage example\ncurriculum_stages = [\n    {'min_length': 0, 'max_length': 128, 'min_entropy': 0, 'max_entropy': 2, 'steps': 2000},\n    {'min_length': 64, 'max_length': 256, 'min_entropy': 1, 'max_entropy': 4, 'steps': 3000},\n    {'min_length': 128, 'max_length': 512, 'min_entropy': 2, 'max_entropy': 8, 'steps': 5000},\n]\n\ndataset = AdvancedSyntheticDataset(sequences, config)\ncurriculum_loader = CurriculumDataLoader(dataset, batch_size=32, curriculum_stages=curriculum_stages)\n</code></pre>"},{"location":"examples/custom-training/#custom-model-components","title":"Custom Model Components","text":""},{"location":"examples/custom-training/#custom-injection-mechanisms","title":"Custom Injection Mechanisms","text":"<pre><code>import torch.nn as nn\n\nclass CrossAttentionInjection(nn.Module):\n    \"\"\"Inject plan using cross-attention mechanism.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_dim = config.hidden_dim\n        self.latent_dim = config.latent_dim\n        self.num_heads = config.num_heads\n\n        # Project plan to hidden dimension\n        self.plan_projection = nn.Linear(config.latent_dim, config.hidden_dim)\n\n        # Cross-attention layers\n        self.cross_attention = nn.MultiheadAttention(\n            embed_dim=config.hidden_dim,\n            num_heads=config.num_heads,\n            batch_first=True\n        )\n\n        # Layer norm and feedforward\n        self.norm = nn.LayerNorm(config.hidden_dim)\n        self.feedforward = nn.Sequential(\n            nn.Linear(config.hidden_dim, config.hidden_dim * 4),\n            nn.GELU(),\n            nn.Linear(config.hidden_dim * 4, config.hidden_dim)\n        )\n\n    def forward(self, decoder_hidden, binary_plan):\n        \"\"\"Apply cross-attention injection.\"\"\"\n        batch_size, seq_len, hidden_dim = decoder_hidden.shape\n\n        # Project plan to hidden dimension\n        plan_repr = self.plan_projection(binary_plan)  # [batch_size, hidden_dim]\n        plan_repr = plan_repr.unsqueeze(1)  # [batch_size, 1, hidden_dim]\n\n        # Cross-attention: decoder hidden as query, plan as key/value\n        attended, _ = self.cross_attention(\n            query=decoder_hidden,\n            key=plan_repr,\n            value=plan_repr\n        )\n\n        # Residual connection and normalization\n        output = self.norm(decoder_hidden + attended)\n\n        # Feedforward\n        ff_output = self.feedforward(output)\n        output = self.norm(output + ff_output)\n\n        return output\n\nclass MultiLevelInjection(nn.Module):\n    \"\"\"Inject plan at multiple levels with different transformations.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.num_levels = config.num_injection_levels\n\n        # Different projections for each level\n        self.level_projections = nn.ModuleList([\n            nn.Linear(config.latent_dim, config.hidden_dim)\n            for _ in range(self.num_levels)\n        ])\n\n        # Level-specific gates\n        self.level_gates = nn.ModuleList([\n            nn.Linear(config.hidden_dim, config.hidden_dim)\n            for _ in range(self.num_levels)\n        ])\n\n        # Attention weights for combining levels\n        self.level_attention = nn.Linear(config.hidden_dim, self.num_levels)\n\n    def forward(self, decoder_hidden, binary_plan, level_weights=None):\n        \"\"\"Apply multi-level injection.\"\"\"\n        batch_size, seq_len, hidden_dim = decoder_hidden.shape\n\n        # Generate level-specific representations\n        level_reprs = []\n        for i, (proj, gate) in enumerate(zip(self.level_projections, self.level_gates)):\n            # Project plan for this level\n            level_repr = proj(binary_plan)  # [batch_size, hidden_dim]\n            level_repr = level_repr.unsqueeze(1).expand(-1, seq_len, -1)\n\n            # Apply level-specific gating\n            gate_values = torch.sigmoid(gate(decoder_hidden))\n            gated_repr = gate_values * level_repr\n\n            level_reprs.append(gated_repr)\n\n        # Compute attention weights for combining levels\n        if level_weights is None:\n            attention_logits = self.level_attention(decoder_hidden)  # [batch_size, seq_len, num_levels]\n            level_weights = torch.softmax(attention_logits, dim=-1)\n\n        # Combine level representations\n        combined_repr = torch.zeros_like(decoder_hidden)\n        for i, level_repr in enumerate(level_reprs):\n            weight = level_weights[..., i:i+1]  # [batch_size, seq_len, 1]\n            combined_repr += weight * level_repr\n\n        return decoder_hidden + combined_repr\n</code></pre>"},{"location":"examples/custom-training/#custom-encoder-architectures","title":"Custom Encoder Architectures","text":"<pre><code>class HierarchicalEncoder(nn.Module):\n    \"\"\"Hierarchical encoder for multi-scale planning.\"\"\"\n\n    def __init__(self, config):\n        super().__init__()\n        self.hidden_dim = config.hidden_dim\n        self.num_levels = config.encoder_levels\n\n        # Encoders for different scales\n        self.local_encoder = self._build_encoder(config, scale='local')\n        self.global_encoder = self._build_encoder(config, scale='global')\n\n        # Fusion mechanism\n        self.fusion = nn.Sequential(\n            nn.Linear(config.hidden_dim * 2, config.hidden_dim),\n            nn.GELU(),\n            nn.Linear(config.hidden_dim, config.hidden_dim)\n        )\n\n        # Output projection\n        self.output_projection = nn.Linear(config.hidden_dim, config.latent_dim)\n\n    def _build_encoder(self, config, scale):\n        \"\"\"Build encoder for specific scale.\"\"\"\n        if scale == 'local':\n            # Local encoder: smaller receptive field\n            return nn.TransformerEncoder(\n                nn.TransformerEncoderLayer(\n                    d_model=config.hidden_dim,\n                    nhead=config.num_heads // 2,\n                    dim_feedforward=config.hidden_dim * 2,\n                    batch_first=True\n                ),\n                num_layers=2\n            )\n        else:\n            # Global encoder: larger receptive field\n            return nn.TransformerEncoder(\n                nn.TransformerEncoderLayer(\n                    d_model=config.hidden_dim,\n                    nhead=config.num_heads,\n                    dim_feedforward=config.hidden_dim * 4,\n                    batch_first=True\n                ),\n                num_layers=3\n            )\n\n    def forward(self, hidden_states):\n        \"\"\"Forward pass with hierarchical encoding.\"\"\"\n        batch_size, seq_len, hidden_dim = hidden_states.shape\n\n        # Local encoding: process overlapping windows\n        local_features = self._encode_local(hidden_states)\n\n        # Global encoding: process entire sequence\n        global_features = self._encode_global(hidden_states)\n\n        # Fuse local and global features\n        fused_features = self.fusion(torch.cat([local_features, global_features], dim=-1))\n\n        # Project to latent dimension\n        latent_repr = self.output_projection(fused_features)\n\n        return latent_repr\n\n    def _encode_local(self, hidden_states):\n        \"\"\"Encode local features using sliding windows.\"\"\"\n        batch_size, seq_len, hidden_dim = hidden_states.shape\n        window_size = min(64, seq_len // 4)\n        stride = window_size // 2\n\n        local_features = []\n        for start in range(0, seq_len - window_size + 1, stride):\n            end = start + window_size\n            window = hidden_states[:, start:end, :]\n\n            # Encode window\n            encoded_window = self.local_encoder(window)\n\n            # Pool to single representation\n            pooled = encoded_window.mean(dim=1)  # [batch_size, hidden_dim]\n            local_features.append(pooled)\n\n        # Combine local features\n        if local_features:\n            local_repr = torch.stack(local_features, dim=1).mean(dim=1)\n        else:\n            local_repr = hidden_states.mean(dim=1)\n\n        return local_repr\n\n    def _encode_global(self, hidden_states):\n        \"\"\"Encode global features using full sequence.\"\"\"\n        # Use learned query for global aggregation\n        global_query = nn.Parameter(torch.randn(1, 1, self.hidden_dim))\n        batch_size = hidden_states.size(0)\n\n        query = global_query.expand(batch_size, -1, -1)\n\n        # Concatenate query with hidden states\n        input_with_query = torch.cat([query, hidden_states], dim=1)\n\n        # Encode\n        encoded = self.global_encoder(input_with_query)\n\n        # Extract global representation (first token)\n        global_repr = encoded[:, 0, :]\n\n        return global_repr\n</code></pre>"},{"location":"examples/custom-training/#advanced-training-techniques","title":"Advanced Training Techniques","text":""},{"location":"examples/custom-training/#adversarial-training","title":"Adversarial Training","text":"<pre><code>class AdversarialTrainer:\n    \"\"\"Adversarial training for Free Transformer.\"\"\"\n\n    def __init__(self, model, discriminator, config):\n        self.model = model\n        self.discriminator = discriminator\n        self.config = config\n\n        # Optimizers\n        self.model_optimizer = torch.optim.AdamW(model.parameters(), lr=config.learning_rate)\n        self.disc_optimizer = torch.optim.AdamW(discriminator.parameters(), lr=config.disc_learning_rate)\n\n    def train_step(self, batch):\n        \"\"\"Single adversarial training step.\"\"\"\n        tokens = batch['input_ids']\n\n        # Train discriminator\n        disc_loss = self._train_discriminator(tokens)\n\n        # Train generator (model)\n        gen_loss = self._train_generator(tokens)\n\n        return {\n            'discriminator_loss': disc_loss,\n            'generator_loss': gen_loss\n        }\n\n    def _train_discriminator(self, real_tokens):\n        \"\"\"Train discriminator to distinguish real from generated sequences.\"\"\"\n        self.disc_optimizer.zero_grad()\n\n        # Real sequences\n        real_logits = self.discriminator(real_tokens)\n        real_loss = F.binary_cross_entropy_with_logits(\n            real_logits, \n            torch.ones_like(real_logits)\n        )\n\n        # Generated sequences\n        with torch.no_grad():\n            generated_tokens = self.model.generate(\n                real_tokens[:, :10], \n                max_new_tokens=real_tokens.size(1) - 10\n            )\n\n        fake_logits = self.discriminator(generated_tokens)\n        fake_loss = F.binary_cross_entropy_with_logits(\n            fake_logits,\n            torch.zeros_like(fake_logits)\n        )\n\n        # Total discriminator loss\n        disc_loss = (real_loss + fake_loss) / 2\n        disc_loss.backward()\n        self.disc_optimizer.step()\n\n        return disc_loss.item()\n\n    def _train_generator(self, tokens):\n        \"\"\"Train generator to fool discriminator.\"\"\"\n        self.model_optimizer.zero_grad()\n\n        # Standard Free Transformer loss\n        logits, z_logits = self.model(tokens, mode='training')\n        standard_loss = free_transformer_loss(\n            logits=logits,\n            z_logits=z_logits,\n            targets=tokens,\n            latent_dim=self.config.latent_dim,\n            kl_weight=self.config.kl_weight,\n            free_bits=self.config.free_bits\n        )\n\n        # Adversarial loss\n        generated_tokens = self.model.generate(\n            tokens[:, :10],\n            max_new_tokens=tokens.size(1) - 10\n        )\n\n        fake_logits = self.discriminator(generated_tokens)\n        adversarial_loss = F.binary_cross_entropy_with_logits(\n            fake_logits,\n            torch.ones_like(fake_logits)  # Want discriminator to think it's real\n        )\n\n        # Combined loss\n        total_loss = standard_loss['total_loss'] + self.config.adversarial_weight * adversarial_loss\n        total_loss.backward()\n        self.model_optimizer.step()\n\n        return total_loss.item()\n\nclass SimpleDiscriminator(nn.Module):\n    \"\"\"Simple discriminator for adversarial training.\"\"\"\n\n    def __init__(self, vocab_size, hidden_dim):\n        super().__init__()\n        self.embedding = nn.Embedding(vocab_size, hidden_dim)\n        self.encoder = nn.TransformerEncoder(\n            nn.TransformerEncoderLayer(\n                d_model=hidden_dim,\n                nhead=8,\n                dim_feedforward=hidden_dim * 4,\n                batch_first=True\n            ),\n            num_layers=4\n        )\n        self.classifier = nn.Linear(hidden_dim, 1)\n\n    def forward(self, tokens):\n        \"\"\"Forward pass.\"\"\"\n        embedded = self.embedding(tokens)\n        encoded = self.encoder(embedded)\n\n        # Global average pooling\n        pooled = encoded.mean(dim=1)\n\n        # Classification\n        logits = self.classifier(pooled)\n\n        return logits\n</code></pre>"},{"location":"examples/custom-training/#next-steps","title":"Next Steps","text":"<ul> <li>Evaluation Examples: Advanced evaluation techniques</li> <li>API Reference: Complete API documentation</li> <li>Training Guide: Standard training practices</li> </ul>"},{"location":"examples/evaluation/","title":"Evaluation Examples","text":"<p>This page provides comprehensive examples for evaluating Free Transformer models.</p>"},{"location":"examples/evaluation/#basic-evaluation","title":"Basic Evaluation","text":""},{"location":"examples/evaluation/#model-comparison","title":"Model Comparison","text":"<pre><code>import torch\nimport torch.nn.functional as F\nfrom free_transformer import FreeTransformer, TransformerBaseline, ModelConfig\nfrom free_transformer.losses import free_transformer_loss\n\ndef compare_models(free_model, baseline_model, test_dataloader, device='cuda'):\n    \"\"\"Compare Free Transformer with baseline model.\"\"\"\n\n    results = {\n        'free_transformer': {'perplexity': 0, 'loss': 0, 'samples': 0},\n        'baseline': {'perplexity': 0, 'loss': 0, 'samples': 0}\n    }\n\n    # Evaluate Free Transformer\n    free_model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        total_tokens = 0\n\n        for batch in test_dataloader:\n            tokens = batch['input_ids'].to(device)\n\n            # Forward pass\n            logits, z_logits = free_model(tokens, mode='training')\n\n            # Compute loss (only reconstruction part for fair comparison)\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                tokens.view(-1),\n                reduction='sum'\n            )\n\n            total_loss += loss.item()\n            total_tokens += tokens.numel()\n\n        avg_loss = total_loss / total_tokens\n        perplexity = torch.exp(torch.tensor(avg_loss))\n\n        results['free_transformer']['loss'] = avg_loss\n        results['free_transformer']['perplexity'] = perplexity.item()\n        results['free_transformer']['samples'] = total_tokens\n\n    # Evaluate Baseline\n    baseline_model.eval()\n    with torch.no_grad():\n        total_loss = 0\n        total_tokens = 0\n\n        for batch in test_dataloader:\n            tokens = batch['input_ids'].to(device)\n\n            # Forward pass\n            logits = baseline_model(tokens)\n\n            # Compute loss\n            loss = F.cross_entropy(\n                logits.view(-1, logits.size(-1)),\n                tokens.view(-1),\n                reduction='sum'\n            )\n\n            total_loss += loss.item()\n            total_tokens += tokens.numel()\n\n        avg_loss = total_loss / total_tokens\n        perplexity = torch.exp(torch.tensor(avg_loss))\n\n        results['baseline']['loss'] = avg_loss\n        results['baseline']['perplexity'] = perplexity.item()\n        results['baseline']['samples'] = total_tokens\n\n    return results\n\n# Example usage\nconfig = ModelConfig(vocab_size=10000, hidden_dim=512, num_layers=12)\nfree_model = FreeTransformer(config).to('cuda')\nbaseline_model = TransformerBaseline(config).to('cuda')\n\n# Load trained models\nfree_model.load_state_dict(torch.load('checkpoints/free/model.pt'))\nbaseline_model.load_state_dict(torch.load('checkpoints/baseline/model.pt'))\n\nresults = compare_models(free_model, baseline_model, test_dataloader)\nprint(f\"Free Transformer Perplexity: {results['free_transformer']['perplexity']:.2f}\")\nprint(f\"Baseline Perplexity: {results['baseline']['perplexity']:.2f}\")\n</code></pre>"},{"location":"examples/evaluation/#generation-quality-evaluation","title":"Generation Quality Evaluation","text":"<pre><code>def evaluate_generation_quality(model, prompts, max_length=100, num_samples=5):\n    \"\"\"Evaluate generation quality with multiple metrics.\"\"\"\n\n    model.eval()\n    results = {\n        'diversity': [],\n        'coherence': [],\n        'fluency': [],\n        'generations': []\n    }\n\n    for prompt in prompts:\n        prompt_results = {\n            'prompt': prompt,\n            'generations': [],\n            'diversity_score': 0,\n            'coherence_score': 0,\n            'fluency_score': 0\n        }\n\n        # Generate multiple samples for diversity measurement\n        generations = []\n        for _ in range(num_samples):\n            with torch.no_grad():\n                generated = model.generate(\n                    prompt,\n                    max_new_tokens=max_length,\n                    temperature=0.8,\n                    top_k=40,\n                    do_sample=True\n                )\n            generations.append(generated)\n\n        prompt_results['generations'] = generations\n\n        # Compute diversity (average pairwise distance)\n        diversity_score = compute_diversity(generations)\n        prompt_results['diversity_score'] = diversity_score\n        results['diversity'].append(diversity_score)\n\n        # Compute coherence (consistency within each generation)\n        coherence_scores = [compute_coherence(gen) for gen in generations]\n        avg_coherence = sum(coherence_scores) / len(coherence_scores)\n        prompt_results['coherence_score'] = avg_coherence\n        results['coherence'].append(avg_coherence)\n\n        # Compute fluency (language model score)\n        fluency_scores = [compute_fluency(gen) for gen in generations]\n        avg_fluency = sum(fluency_scores) / len(fluency_scores)\n        prompt_results['fluency_score'] = avg_fluency\n        results['fluency'].append(avg_fluency)\n\n        results['generations'].append(prompt_results)\n\n    # Aggregate results\n    results['avg_diversity'] = sum(results['diversity']) / len(results['diversity'])\n    results['avg_coherence'] = sum(results['coherence']) / len(results['coherence'])\n    results['avg_fluency'] = sum(results['fluency']) / len(results['fluency'])\n\n    return results\n\ndef compute_diversity(generations):\n    \"\"\"Compute diversity score for a set of generations.\"\"\"\n    if len(generations) &lt; 2:\n        return 0.0\n\n    # Convert to text (assuming we have a tokenizer)\n    # For now, compute token-level diversity\n    total_distance = 0\n    num_pairs = 0\n\n    for i in range(len(generations)):\n        for j in range(i + 1, len(generations)):\n            # Compute edit distance or Jaccard similarity\n            distance = compute_sequence_distance(generations[i], generations[j])\n            total_distance += distance\n            num_pairs += 1\n\n    return total_distance / num_pairs if num_pairs &gt; 0 else 0.0\n\ndef compute_sequence_distance(seq1, seq2):\n    \"\"\"Compute distance between two sequences.\"\"\"\n    # Simple Jaccard distance\n    set1 = set(seq1.flatten().tolist())\n    set2 = set(seq2.flatten().tolist())\n\n    intersection = len(set1.intersection(set2))\n    union = len(set1.union(set2))\n\n    return 1 - (intersection / union) if union &gt; 0 else 1.0\n\ndef compute_coherence(generation):\n    \"\"\"Compute coherence score for a single generation.\"\"\"\n    # Simple coherence: repetition ratio\n    tokens = generation.flatten().tolist()\n    if len(tokens) &lt; 2:\n        return 0.0\n\n    # Count adjacent repetitions\n    repetitions = sum(1 for i in range(1, len(tokens)) if tokens[i] == tokens[i-1])\n    return 1 - (repetitions / (len(tokens) - 1))\n\ndef compute_fluency(generation):\n    \"\"\"Compute fluency score using a reference language model.\"\"\"\n    # Placeholder - in practice, use a trained language model\n    # For now, return entropy-based score\n    tokens = generation.flatten().tolist()\n    if not tokens:\n        return 0.0\n\n    from collections import Counter\n    token_counts = Counter(tokens)\n    total_tokens = len(tokens)\n\n    entropy = 0\n    for count in token_counts.values():\n        prob = count / total_tokens\n        entropy -= prob * torch.log(torch.tensor(prob))\n\n    # Normalize entropy (higher entropy = more fluent, up to a point)\n    max_entropy = torch.log(torch.tensor(len(token_counts)))\n    normalized_entropy = entropy / max_entropy if max_entropy &gt; 0 else 0\n\n    return normalized_entropy.item()\n</code></pre>"},{"location":"examples/evaluation/#advanced-evaluation","title":"Advanced Evaluation","text":""},{"location":"examples/evaluation/#latent-space-analysis","title":"Latent Space Analysis","text":"<pre><code>def analyze_latent_space(model, dataloader, device='cuda'):\n    \"\"\"Analyze the learned latent space.\"\"\"\n\n    model.eval()\n    latent_codes = []\n    input_features = []\n\n    with torch.no_grad():\n        for batch in dataloader:\n            tokens = batch['input_ids'].to(device)\n\n            # Get latent codes\n            _, z_logits = model(tokens, mode='training')\n            z_probs = torch.sigmoid(z_logits)\n\n            latent_codes.append(z_probs.cpu())\n\n            # Extract input features for analysis\n            features = extract_input_features(tokens)\n            input_features.extend(features)\n\n    latent_codes = torch.cat(latent_codes, dim=0)\n\n    analysis_results = {\n        'latent_utilization': analyze_latent_utilization(latent_codes),\n        'latent_clustering': analyze_latent_clustering(latent_codes, input_features),\n        'latent_interpolation': analyze_latent_interpolation(model, latent_codes),\n        'bit_importance': analyze_bit_importance(latent_codes, input_features)\n    }\n\n    return analysis_results\n\ndef analyze_latent_utilization(latent_codes):\n    \"\"\"Analyze how much each latent dimension is used.\"\"\"\n    # Compute statistics for each dimension\n    utilization = {}\n\n    for dim in range(latent_codes.size(1)):\n        dim_values = latent_codes[:, dim]\n\n        utilization[f'dim_{dim}'] = {\n            'mean': dim_values.mean().item(),\n            'std': dim_values.std().item(),\n            'entropy': compute_entropy_continuous(dim_values),\n            'active_ratio': ((dim_values &gt; 0.1) &amp; (dim_values &lt; 0.9)).float().mean().item()\n        }\n\n    # Overall utilization metrics\n    utilization['overall'] = {\n        'avg_entropy': sum(d['entropy'] for d in utilization.values() if isinstance(d, dict)) / latent_codes.size(1),\n        'avg_active_ratio': sum(d['active_ratio'] for d in utilization.values() if isinstance(d, dict)) / latent_codes.size(1)\n    }\n\n    return utilization\n\ndef compute_entropy_continuous(values):\n    \"\"\"Compute entropy for continuous values using binning.\"\"\"\n    # Bin values and compute entropy\n    hist, _ = torch.histogram(values, bins=20, range=(0, 1))\n    probs = hist.float() / hist.sum()\n    probs = probs[probs &gt; 0]  # Remove zero probabilities\n\n    entropy = -(probs * torch.log2(probs)).sum()\n    return entropy.item()\n\ndef analyze_latent_clustering(latent_codes, input_features):\n    \"\"\"Analyze clustering in latent space.\"\"\"\n    from sklearn.cluster import KMeans\n    from sklearn.metrics import silhouette_score\n\n    # Convert to numpy for sklearn\n    latent_np = latent_codes.numpy()\n\n    clustering_results = {}\n\n    # Try different numbers of clusters\n    for n_clusters in [2, 4, 8, 16]:\n        kmeans = KMeans(n_clusters=n_clusters, random_state=42)\n        cluster_labels = kmeans.fit_predict(latent_np)\n\n        # Compute silhouette score\n        silhouette = silhouette_score(latent_np, cluster_labels)\n\n        clustering_results[f'k_{n_clusters}'] = {\n            'silhouette_score': silhouette,\n            'cluster_centers': kmeans.cluster_centers_.tolist(),\n            'cluster_labels': cluster_labels.tolist()\n        }\n\n    return clustering_results\n\ndef extract_input_features(tokens):\n    \"\"\"Extract features from input tokens for analysis.\"\"\"\n    features = []\n\n    for seq in tokens:\n        seq_features = {\n            'length': len(seq),\n            'unique_tokens': len(torch.unique(seq)),\n            'repetition_ratio': compute_repetition_ratio(seq),\n            'first_token': seq[0].item() if len(seq) &gt; 0 else -1,\n            'last_token': seq[-1].item() if len(seq) &gt; 0 else -1\n        }\n        features.append(seq_features)\n\n    return features\n\ndef compute_repetition_ratio(sequence):\n    \"\"\"Compute repetition ratio in sequence.\"\"\"\n    if len(sequence) &lt; 2:\n        return 0.0\n\n    repeats = sum(1 for i in range(1, len(sequence)) if sequence[i] == sequence[i-1])\n    return repeats / (len(sequence) - 1)\n</code></pre>"},{"location":"examples/evaluation/#plan-manipulation-experiments","title":"Plan Manipulation Experiments","text":"<pre><code>def experiment_plan_manipulation(model, test_prompts, device='cuda'):\n    \"\"\"Experiment with plan manipulation for controllable generation.\"\"\"\n\n    model.eval()\n    experiments = {}\n\n    # Experiment 1: Bit flipping\n    experiments['bit_flipping'] = bit_flipping_experiment(model, test_prompts, device)\n\n    # Experiment 2: Plan interpolation\n    experiments['plan_interpolation'] = plan_interpolation_experiment(model, test_prompts, device)\n\n    # Experiment 3: Plan conditioning\n    experiments['plan_conditioning'] = plan_conditioning_experiment(model, test_prompts, device)\n\n    return experiments\n\ndef bit_flipping_experiment(model, prompts, device):\n    \"\"\"Test effect of flipping individual bits in the plan.\"\"\"\n    results = []\n\n    for prompt in prompts:\n        prompt_tensor = prompt.to(device)\n\n        # Generate with random plan\n        with torch.no_grad():\n            baseline_gen = model.generate(prompt_tensor, max_new_tokens=50)\n\n        # Test flipping each bit\n        bit_effects = []\n        for bit_idx in range(model.config.latent_dim):\n            # Sample a plan\n            plan = torch.bernoulli(torch.full((1, model.config.latent_dim), 0.5)).to(device)\n\n            # Generate with original plan\n            original_gen = model.generate_with_plan(prompt_tensor, plan, max_new_tokens=50)\n\n            # Flip the bit and generate again\n            flipped_plan = plan.clone()\n            flipped_plan[0, bit_idx] = 1 - flipped_plan[0, bit_idx]\n            flipped_gen = model.generate_with_plan(prompt_tensor, flipped_plan, max_new_tokens=50)\n\n            # Measure difference\n            difference = compute_sequence_distance(original_gen, flipped_gen)\n\n            bit_effects.append({\n                'bit_index': bit_idx,\n                'difference': difference,\n                'original_generation': original_gen,\n                'flipped_generation': flipped_gen\n            })\n\n        results.append({\n            'prompt': prompt,\n            'baseline_generation': baseline_gen,\n            'bit_effects': bit_effects\n        })\n\n    return results\n\ndef plan_interpolation_experiment(model, prompts, device):\n    \"\"\"Test plan interpolation for smooth generation transitions.\"\"\"\n    results = []\n\n    for prompt in prompts:\n        prompt_tensor = prompt.to(device)\n\n        # Sample two random plans\n        plan1 = torch.bernoulli(torch.full((1, model.config.latent_dim), 0.5)).to(device)\n        plan2 = torch.bernoulli(torch.full((1, model.config.latent_dim), 0.5)).to(device)\n\n        # Generate interpolated plans\n        interpolation_steps = 5\n        interpolated_generations = []\n\n        for i in range(interpolation_steps):\n            alpha = i / (interpolation_steps - 1)\n\n            # Linear interpolation in probability space\n            plan1_probs = plan1.float()\n            plan2_probs = plan2.float()\n            interpolated_probs = (1 - alpha) * plan1_probs + alpha * plan2_probs\n\n            # Sample from interpolated probabilities\n            interpolated_plan = torch.bernoulli(interpolated_probs)\n\n            # Generate with interpolated plan\n            with torch.no_grad():\n                generation = model.generate_with_plan(\n                    prompt_tensor, \n                    interpolated_plan, \n                    max_new_tokens=50\n                )\n\n            interpolated_generations.append({\n                'alpha': alpha,\n                'plan': interpolated_plan,\n                'generation': generation\n            })\n\n        results.append({\n            'prompt': prompt,\n            'plan1': plan1,\n            'plan2': plan2,\n            'interpolations': interpolated_generations\n        })\n\n    return results\n\ndef plan_conditioning_experiment(model, prompts, device):\n    \"\"\"Test conditioning generation on specific plan patterns.\"\"\"\n    results = []\n\n    # Define specific plan patterns to test\n    patterns = {\n        'all_zeros': torch.zeros(1, model.config.latent_dim),\n        'all_ones': torch.ones(1, model.config.latent_dim),\n        'alternating': torch.tensor([[i % 2 for i in range(model.config.latent_dim)]]).float(),\n        'first_half_ones': torch.cat([\n            torch.ones(1, model.config.latent_dim // 2),\n            torch.zeros(1, model.config.latent_dim - model.config.latent_dim // 2)\n        ], dim=1)\n    }\n\n    for prompt in prompts:\n        prompt_tensor = prompt.to(device)\n        pattern_results = {}\n\n        for pattern_name, pattern_plan in patterns.items():\n            pattern_plan = pattern_plan.to(device)\n\n            # Generate multiple samples with this pattern\n            generations = []\n            for _ in range(3):\n                with torch.no_grad():\n                    generation = model.generate_with_plan(\n                        prompt_tensor,\n                        pattern_plan,\n                        max_new_tokens=50\n                    )\n                generations.append(generation)\n\n            pattern_results[pattern_name] = {\n                'plan': pattern_plan,\n                'generations': generations,\n                'diversity': compute_diversity(generations)\n            }\n\n        results.append({\n            'prompt': prompt,\n            'patterns': pattern_results\n        })\n\n    return results\n</code></pre>"},{"location":"examples/evaluation/#evaluation-metrics","title":"Evaluation Metrics","text":""},{"location":"examples/evaluation/#comprehensive-metrics-suite","title":"Comprehensive Metrics Suite","text":"<pre><code>class EvaluationSuite:\n    \"\"\"Comprehensive evaluation suite for Free Transformer.\"\"\"\n\n    def __init__(self, model, tokenizer=None):\n        self.model = model\n        self.tokenizer = tokenizer\n        self.metrics = {}\n\n    def evaluate_all(self, test_dataloader, generation_prompts=None):\n        \"\"\"Run all evaluation metrics.\"\"\"\n\n        # Language modeling metrics\n        self.metrics['language_modeling'] = self.evaluate_language_modeling(test_dataloader)\n\n        # Generation metrics\n        if generation_prompts:\n            self.metrics['generation'] = self.evaluate_generation(generation_prompts)\n\n        # Latent space metrics\n        self.metrics['latent_space'] = self.evaluate_latent_space(test_dataloader)\n\n        # Plan utilization metrics\n        self.metrics['plan_utilization'] = self.evaluate_plan_utilization(test_dataloader)\n\n        return self.metrics\n\n    def evaluate_language_modeling(self, dataloader):\n        \"\"\"Evaluate language modeling performance.\"\"\"\n        self.model.eval()\n\n        total_loss = 0\n        total_tokens = 0\n        total_kl = 0\n        num_batches = 0\n\n        with torch.no_grad():\n            for batch in dataloader:\n                tokens = batch['input_ids']\n\n                logits, z_logits = self.model(tokens, mode='training')\n\n                # Reconstruction loss\n                recon_loss = F.cross_entropy(\n                    logits.view(-1, logits.size(-1)),\n                    tokens.view(-1),\n                    reduction='sum'\n                )\n\n                # KL loss\n                posterior = torch.sigmoid(z_logits)\n                prior = torch.full_like(posterior, 0.5)\n                kl_loss = F.kl_div(\n                    torch.log(posterior + 1e-8),\n                    prior,\n                    reduction='sum'\n                )\n\n                total_loss += recon_loss.item()\n                total_tokens += tokens.numel()\n                total_kl += kl_loss.item()\n                num_batches += 1\n\n        avg_loss = total_loss / total_tokens\n        perplexity = torch.exp(torch.tensor(avg_loss))\n        avg_kl = total_kl / num_batches\n\n        return {\n            'perplexity': perplexity.item(),\n            'avg_loss': avg_loss,\n            'avg_kl_divergence': avg_kl,\n            'total_tokens': total_tokens\n        }\n\n    def evaluate_generation(self, prompts):\n        \"\"\"Evaluate generation quality.\"\"\"\n        self.model.eval()\n\n        all_generations = []\n        diversity_scores = []\n        coherence_scores = []\n\n        for prompt in prompts:\n            # Generate multiple samples\n            generations = []\n            for _ in range(5):\n                with torch.no_grad():\n                    generated = self.model.generate(\n                        prompt,\n                        max_new_tokens=100,\n                        temperature=0.8,\n                        top_k=40,\n                        do_sample=True\n                    )\n                generations.append(generated)\n\n            all_generations.extend(generations)\n\n            # Compute diversity for this prompt\n            diversity = compute_diversity(generations)\n            diversity_scores.append(diversity)\n\n            # Compute coherence for each generation\n            coherence = [compute_coherence(gen) for gen in generations]\n            coherence_scores.extend(coherence)\n\n        return {\n            'avg_diversity': sum(diversity_scores) / len(diversity_scores),\n            'avg_coherence': sum(coherence_scores) / len(coherence_scores),\n            'total_generations': len(all_generations)\n        }\n\n    def evaluate_latent_space(self, dataloader):\n        \"\"\"Evaluate latent space properties.\"\"\"\n        self.model.eval()\n\n        all_z_logits = []\n\n        with torch.no_grad():\n            for batch in dataloader:\n                tokens = batch['input_ids']\n                _, z_logits = self.model(tokens, mode='training')\n                all_z_logits.append(z_logits)\n\n        all_z_logits = torch.cat(all_z_logits, dim=0)\n        z_probs = torch.sigmoid(all_z_logits)\n\n        # Compute metrics\n        metrics = {}\n\n        # Utilization per dimension\n        dim_utilization = []\n        for dim in range(z_probs.size(1)):\n            dim_values = z_probs[:, dim]\n            utilization = ((dim_values &gt; 0.1) &amp; (dim_values &lt; 0.9)).float().mean()\n            dim_utilization.append(utilization.item())\n\n        metrics['avg_dimension_utilization'] = sum(dim_utilization) / len(dim_utilization)\n        metrics['dimension_utilization'] = dim_utilization\n\n        # Overall entropy\n        total_entropy = 0\n        for dim in range(z_probs.size(1)):\n            dim_entropy = compute_entropy_continuous(z_probs[:, dim])\n            total_entropy += dim_entropy\n\n        metrics['avg_entropy'] = total_entropy / z_probs.size(1)\n\n        return metrics\n\n    def evaluate_plan_utilization(self, dataloader):\n        \"\"\"Evaluate how well the model uses the latent plan.\"\"\"\n        self.model.eval()\n\n        # Test plan manipulation effects\n        manipulation_effects = []\n\n        with torch.no_grad():\n            for batch in dataloader:\n                tokens = batch['input_ids']\n                if tokens.size(0) == 0:\n                    continue\n\n                # Take first sequence as test case\n                test_tokens = tokens[0:1]\n\n                # Generate with original plan\n                _, z_logits = self.model(test_tokens, mode='training')\n                original_plan = torch.sigmoid(z_logits)\n\n                # Generate with modified plans\n                for bit_idx in range(min(8, self.model.config.latent_dim)):  # Test first 8 bits\n                    modified_plan = original_plan.clone()\n                    modified_plan[0, bit_idx] = 1 - modified_plan[0, bit_idx]\n\n                    # This would require a generate_with_plan method\n                    # For now, just measure the difference in representations\n                    # In practice, you'd compare generated sequences\n\n                    plan_diff = torch.abs(original_plan - modified_plan).sum().item()\n                    manipulation_effects.append(plan_diff)\n\n                # Only test a few batches for efficiency\n                if len(manipulation_effects) &gt; 50:\n                    break\n\n        return {\n            'avg_manipulation_effect': sum(manipulation_effects) / len(manipulation_effects) if manipulation_effects else 0,\n            'manipulation_effects': manipulation_effects\n        }\n\n# Usage example\ndef run_comprehensive_evaluation():\n    \"\"\"Run comprehensive evaluation on trained models.\"\"\"\n\n    # Load models\n    config = ModelConfig(vocab_size=10000, hidden_dim=512, num_layers=12)\n    model = FreeTransformer(config)\n    model.load_state_dict(torch.load('checkpoints/free/model.pt'))\n    model.eval()\n\n    # Create evaluation suite\n    evaluator = EvaluationSuite(model)\n\n    # Run evaluation\n    results = evaluator.evaluate_all(test_dataloader, generation_prompts)\n\n    # Print results\n    print(\"=== Free Transformer Evaluation Results ===\")\n    print(f\"Perplexity: {results['language_modeling']['perplexity']:.2f}\")\n    print(f\"Average KL Divergence: {results['language_modeling']['avg_kl_divergence']:.4f}\")\n    print(f\"Generation Diversity: {results['generation']['avg_diversity']:.4f}\")\n    print(f\"Generation Coherence: {results['generation']['avg_coherence']:.4f}\")\n    print(f\"Latent Dimension Utilization: {results['latent_space']['avg_dimension_utilization']:.4f}\")\n    print(f\"Latent Space Entropy: {results['latent_space']['avg_entropy']:.4f}\")\n\n    return results\n</code></pre>"},{"location":"examples/evaluation/#next-steps","title":"Next Steps","text":"<ul> <li>Custom Training: Advanced training techniques</li> <li>API Reference: Complete API documentation</li> <li>Training Guide: Standard training practices</li> </ul>"},{"location":"getting-started/docker/","title":"Docker Setup","text":"<p>Docker provides the fastest way to try Free Transformer without local installation.</p>"},{"location":"getting-started/docker/#quick-demo","title":"Quick Demo","text":"<p>Run the complete demo with one command:</p> <pre><code>git clone https://github.com/udapy/free-transformer.git\ncd free-transformer\ndocker-compose up free-transformer-demo\n</code></pre> <p>This will: 1. Generate synthetic training data 2. Train baseline and Free Transformer models 3. Compare their performance 4. Display results</p>"},{"location":"getting-started/docker/#docker-images","title":"Docker Images","text":""},{"location":"getting-started/docker/#gpu-version-recommended","title":"GPU Version (Recommended)","text":"<p>For NVIDIA GPUs with CUDA support:</p> <pre><code># Build the image\nmake docker-build\n\n# Run the demo\nmake docker-demo\n\n# Interactive development\nmake docker-interactive\n</code></pre>"},{"location":"getting-started/docker/#cpu-version","title":"CPU Version","text":"<p>For systems without GPU or CUDA:</p> <pre><code># Build CPU-only image\nmake docker-build-cpu\n\n# Run CPU demo\nmake docker-run-cpu\n</code></pre>"},{"location":"getting-started/docker/#manual-docker-commands","title":"Manual Docker Commands","text":""},{"location":"getting-started/docker/#build-images","title":"Build Images","text":"<pre><code># GPU version\ndocker build -t free-transformer:gpu .\n\n# CPU version  \ndocker build -f Dockerfile.cpu -t free-transformer:cpu .\n</code></pre>"},{"location":"getting-started/docker/#run-containers","title":"Run Containers","text":"<pre><code># GPU demo\ndocker run --gpus all -v $(pwd)/results:/app/results free-transformer:gpu\n\n# CPU demo\ndocker run -v $(pwd)/results:/app/results free-transformer:cpu\n\n# Interactive shell\ndocker run --gpus all -it -v $(pwd):/app free-transformer:gpu bash\n</code></pre>"},{"location":"getting-started/docker/#docker-compose","title":"Docker Compose","text":"<p>The <code>docker-compose.yml</code> provides several services:</p>"},{"location":"getting-started/docker/#demo-service","title":"Demo Service","text":"<pre><code>services:\n  free-transformer-demo:\n    build: .\n    volumes:\n      - ./results:/app/results\n      - ./checkpoints:/app/checkpoints\n    deploy:\n      resources:\n        reservations:\n          devices:\n            - driver: nvidia\n              count: all\n              capabilities: [gpu]\n</code></pre> <p>Run with: <pre><code>docker-compose up free-transformer-demo\n</code></pre></p>"},{"location":"getting-started/docker/#development-service","title":"Development Service","text":"<pre><code>  free-transformer-dev:\n    build: .\n    volumes:\n      - .:/app\n    stdin_open: true\n    tty: true\n    command: bash\n</code></pre> <p>Run with: <pre><code>docker-compose run free-transformer-dev\n</code></pre></p>"},{"location":"getting-started/docker/#volume-mounts","title":"Volume Mounts","text":"<p>Important directories to mount:</p> <ul> <li><code>./results:/app/results</code> - Evaluation results</li> <li><code>./checkpoints:/app/checkpoints</code> - Model checkpoints  </li> <li><code>./data:/app/data</code> - Training data</li> <li><code>.:/app</code> - Full source code (development)</li> </ul>"},{"location":"getting-started/docker/#environment-variables","title":"Environment Variables","text":"<p>Configure the container with environment variables:</p> <pre><code>docker run \\\n  --gpus all \\\n  -e CUDA_VISIBLE_DEVICES=0,1 \\\n  -e WANDB_API_KEY=your_key \\\n  -e TORCH_DISTRIBUTED_DEBUG=INFO \\\n  free-transformer:gpu\n</code></pre> <p>Common variables: - <code>CUDA_VISIBLE_DEVICES</code> - GPU selection - <code>WANDB_API_KEY</code> - Weights &amp; Biases logging - <code>TORCH_DISTRIBUTED_DEBUG</code> - Distributed training debug - <code>OMP_NUM_THREADS</code> - CPU thread count</p>"},{"location":"getting-started/docker/#gpu-requirements","title":"GPU Requirements","text":""},{"location":"getting-started/docker/#nvidia-docker-setup","title":"NVIDIA Docker Setup","text":"<ol> <li>Install NVIDIA drivers</li> <li>Install Docker</li> <li>Install NVIDIA Container Toolkit:</li> </ol> <pre><code># Ubuntu/Debian\ndistribution=$(. /etc/os-release;echo $ID$VERSION_ID)\ncurl -s -L https://nvidia.github.io/nvidia-docker/gpgkey | sudo apt-key add -\ncurl -s -L https://nvidia.github.io/nvidia-docker/$distribution/nvidia-docker.list | sudo tee /etc/apt/sources.list.d/nvidia-docker.list\n\nsudo apt-get update &amp;&amp; sudo apt-get install -y nvidia-docker2\nsudo systemctl restart docker\n</code></pre> <ol> <li>Test GPU access: <pre><code>docker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi\n</code></pre></li> </ol>"},{"location":"getting-started/docker/#customization","title":"Customization","text":""},{"location":"getting-started/docker/#custom-dockerfile","title":"Custom Dockerfile","text":"<p>Create your own Dockerfile for specific needs:</p> <pre><code>FROM free-transformer:gpu\n\n# Add custom dependencies\nRUN pip install your-package\n\n# Copy custom configs\nCOPY my-config.yaml /app/configs/\n\n# Set custom entrypoint\nENTRYPOINT [\"python\", \"my-script.py\"]\n</code></pre>"},{"location":"getting-started/docker/#custom-docker-compose","title":"Custom Docker Compose","text":"<pre><code>version: '3.8'\nservices:\n  my-training:\n    build: .\n    volumes:\n      - ./my-data:/app/data\n      - ./my-configs:/app/configs\n    environment:\n      - WANDB_PROJECT=my-project\n    command: python examples/train_free.py --config configs/my-config.yaml\n</code></pre>"},{"location":"getting-started/docker/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/docker/#gpu-not-detected","title":"GPU Not Detected","text":"<pre><code># Check NVIDIA Docker setup\ndocker run --rm --gpus all nvidia/cuda:11.8-base-ubuntu20.04 nvidia-smi\n\n# Check container GPU access\ndocker run --rm --gpus all free-transformer:gpu python -c \"import torch; print(torch.cuda.is_available())\"\n</code></pre>"},{"location":"getting-started/docker/#out-of-memory","title":"Out of Memory","text":"<p>Reduce batch size in configs or use CPU version:</p> <pre><code># Use CPU version\ndocker-compose -f docker-compose.yml up free-transformer-demo-cpu\n\n# Or modify config\ndocker run -v $(pwd)/configs:/app/configs free-transformer:gpu \\\n  python examples/train_free.py --config configs/small.yaml\n</code></pre>"},{"location":"getting-started/docker/#permission-issues","title":"Permission Issues","text":"<p>Fix volume mount permissions:</p> <pre><code># Create directories with correct permissions\nmkdir -p results checkpoints data\nchmod 777 results checkpoints data\n\n# Or run with user ID\ndocker run --user $(id -u):$(id -g) -v $(pwd):/app free-transformer:gpu\n</code></pre>"},{"location":"getting-started/docker/#performance-tips","title":"Performance Tips","text":"<ol> <li>Use GPU version for significant speedup</li> <li>Mount SSD storage for data directories</li> <li>Allocate sufficient memory (8GB+ recommended)</li> <li>Use multi-GPU for large models:    <pre><code>docker run --gpus all -e CUDA_VISIBLE_DEVICES=0,1,2,3 free-transformer:gpu\n</code></pre></li> </ol>"},{"location":"getting-started/docker/#next-steps","title":"Next Steps","text":"<ul> <li>Quick Start: Run your first training</li> <li>Training Guide: Advanced training setup</li> <li>Multi-GPU: Distributed training</li> </ul>"},{"location":"getting-started/installation/","title":"Installation","text":""},{"location":"getting-started/installation/#requirements","title":"Requirements","text":"<ul> <li>Python 3.12+</li> <li>PyTorch 2.0+</li> <li>CUDA-compatible GPU (recommended)</li> </ul>"},{"location":"getting-started/installation/#using-uv-recommended","title":"Using UV (Recommended)","text":"<p>UV is the fastest way to install Free Transformer:</p> <pre><code># Install UV if you haven't already\ncurl -LsSf https://astral.sh/uv/install.sh | sh\n\n# Create virtual environment\nuv venv --python 3.12\nsource .venv/bin/activate\n\n# Install Free Transformer with development dependencies\nuv pip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#using-pip","title":"Using pip","text":"<p>After the PyPI release:</p> <pre><code>pip install free-transformer\n</code></pre> <p>For development:</p> <pre><code>git clone https://github.com/udapy/free-transformer.git\ncd free-transformer\npip install -e \".[dev]\"\n</code></pre>"},{"location":"getting-started/installation/#optional-dependencies","title":"Optional Dependencies","text":""},{"location":"getting-started/installation/#documentation","title":"Documentation","text":"<pre><code>uv pip install -e \".[docs]\"\n</code></pre>"},{"location":"getting-started/installation/#deepspeed-future","title":"DeepSpeed (Future)","text":"<pre><code>uv pip install -e \".[deepspeed]\"\n</code></pre>"},{"location":"getting-started/installation/#verify-installation","title":"Verify Installation","text":"<p>Test your installation:</p> <pre><code>import torch\nfrom free_transformer import FreeTransformer, ModelConfig\n\n# Create a small model\nconfig = ModelConfig(\n    vocab_size=1000,\n    hidden_dim=128,\n    num_layers=4,\n    num_heads=4,\n    latent_dim=8,\n)\n\nmodel = FreeTransformer(config)\nprint(f\"Model created with {sum(p.numel() for p in model.parameters())} parameters\")\n</code></pre>"},{"location":"getting-started/installation/#gpu-setup","title":"GPU Setup","text":"<p>For CUDA support, ensure you have:</p> <ol> <li>NVIDIA drivers installed</li> <li>CUDA toolkit (11.8+ or 12.0+)</li> <li>PyTorch with CUDA support</li> </ol> <p>Verify GPU availability:</p> <pre><code>import torch\nprint(f\"CUDA available: {torch.cuda.is_available()}\")\nprint(f\"CUDA devices: {torch.cuda.device_count()}\")\n</code></pre>"},{"location":"getting-started/installation/#troubleshooting","title":"Troubleshooting","text":""},{"location":"getting-started/installation/#common-issues","title":"Common Issues","text":"<p>ImportError: No module named 'free_transformer' - Ensure you're in the correct virtual environment - Try reinstalling with <code>uv pip install -e .</code></p> <p>CUDA out of memory - Reduce batch size in config files - Enable gradient checkpointing - Use smaller model dimensions</p> <p>Slow training - Verify GPU is being used - Enable mixed precision training - Consider multi-GPU setup with FSDP</p>"},{"location":"getting-started/installation/#getting-help","title":"Getting Help","text":"<ul> <li>Check the FAQ</li> <li>Open an issue on GitHub</li> <li>Review the troubleshooting guide</li> </ul>"},{"location":"getting-started/quick-start/","title":"Quick Start","text":"<p>Get up and running with Free Transformer in minutes.</p>"},{"location":"getting-started/quick-start/#1-generate-synthetic-data","title":"1. Generate Synthetic Data","text":"<p>Start with small synthetic data for quick experimentation:</p> <pre><code>make generate-data-small\n</code></pre> <p>This creates training data in <code>data/synthetic/</code> with: - 1000 training sequences - 200 validation sequences - Sequence length: 128 tokens - Vocabulary size: 1000</p>"},{"location":"getting-started/quick-start/#2-train-baseline-model","title":"2. Train Baseline Model","text":"<p>Train a standard Transformer for comparison:</p> <pre><code>make train-baseline\n</code></pre> <p>This will: - Use configuration from <code>configs/baseline.yaml</code> - Save checkpoints to <code>checkpoints/baseline/</code> - Log training metrics to TensorBoard</p>"},{"location":"getting-started/quick-start/#3-train-free-transformer","title":"3. Train Free Transformer","text":"<p>Train the Free Transformer with latent planning:</p> <pre><code>make train-free\n</code></pre> <p>This will: - Use configuration from <code>configs/free_transformer.yaml</code> - Save checkpoints to <code>checkpoints/free/</code> - Include VAE loss components (reconstruction + KL)</p>"},{"location":"getting-started/quick-start/#4-compare-models","title":"4. Compare Models","text":"<p>Evaluate and compare both models:</p> <pre><code>make compare\n</code></pre> <p>Results saved to <code>results/comparison/results.json</code> with: - Perplexity scores - Generation quality metrics - Training efficiency comparisons</p>"},{"location":"getting-started/quick-start/#full-demo-pipeline","title":"Full Demo Pipeline","text":"<p>Run everything at once:</p> <pre><code>make demo\n</code></pre>"},{"location":"getting-started/quick-start/#python-api-quick-start","title":"Python API Quick Start","text":""},{"location":"getting-started/quick-start/#basic-usage","title":"Basic Usage","text":"<pre><code>import torch\nfrom free_transformer import FreeTransformer, ModelConfig\n\n# Create model configuration\nconfig = ModelConfig(\n    vocab_size=1000,\n    hidden_dim=256,\n    num_layers=6,\n    num_heads=8,\n    latent_dim=16,\n    max_seq_len=512\n)\n\n# Initialize model\nmodel = FreeTransformer(config)\n\n# Training mode - with latent encoding\ntokens = torch.randint(0, 1000, (2, 128))  # batch_size=2, seq_len=128\nlogits, z_logits = model(tokens, mode='training')\n\nprint(f\"Output logits shape: {logits.shape}\")  # [2, 128, 1000]\nprint(f\"Latent logits shape: {z_logits.shape}\")  # [2, 16]\n</code></pre>"},{"location":"getting-started/quick-start/#generation","title":"Generation","text":"<pre><code># Inference mode - generate new tokens\nprompt = torch.randint(0, 1000, (1, 10))  # batch_size=1, prompt_len=10\ngenerated = model.generate(\n    prompt, \n    max_new_tokens=50,\n    temperature=0.8,\n    top_k=40\n)\n\nprint(f\"Generated sequence shape: {generated.shape}\")  # [1, 60]\n</code></pre>"},{"location":"getting-started/quick-start/#custom-training-loop","title":"Custom Training Loop","text":"<pre><code>import torch.nn.functional as F\nfrom free_transformer.losses import free_transformer_loss\n\n# Training setup\noptimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\nmodel.train()\n\nfor batch in dataloader:\n    tokens = batch['input_ids']\n\n    # Forward pass\n    logits, z_logits = model(tokens, mode='training')\n\n    # Compute loss\n    loss_dict = free_transformer_loss(\n        logits=logits,\n        z_logits=z_logits,\n        targets=tokens,\n        latent_dim=config.latent_dim,\n        kl_weight=0.1,\n        free_bits=0.5\n    )\n\n    total_loss = loss_dict['total_loss']\n\n    # Backward pass\n    optimizer.zero_grad()\n    total_loss.backward()\n    optimizer.step()\n\n    print(f\"Loss: {total_loss.item():.4f}\")\n</code></pre>"},{"location":"getting-started/quick-start/#configuration","title":"Configuration","text":""},{"location":"getting-started/quick-start/#model-configuration","title":"Model Configuration","text":"<p>Key parameters in <code>ModelConfig</code>:</p> <pre><code>config = ModelConfig(\n    # Architecture\n    vocab_size=50000,          # Vocabulary size\n    hidden_dim=512,            # Hidden dimension\n    num_layers=12,             # Number of transformer layers\n    num_heads=8,               # Number of attention heads\n\n    # Free Transformer specific\n    latent_dim=32,             # Latent plan dimension\n    encoder_layers=2,          # Number of encoder layers\n\n    # Training\n    max_seq_len=1024,          # Maximum sequence length\n    dropout=0.1,               # Dropout rate\n)\n</code></pre>"},{"location":"getting-started/quick-start/#training-configuration","title":"Training Configuration","text":"<p>Edit YAML config files:</p> <pre><code># configs/free_transformer.yaml\nmodel:\n  vocab_size: 50000\n  hidden_dim: 512\n  num_layers: 12\n  latent_dim: 32\n\ntraining:\n  batch_size: 32\n  learning_rate: 1e-4\n  num_epochs: 10\n  kl_weight: 0.1\n  free_bits: 0.5\n\ndata:\n  max_seq_len: 512\n  dataset_name: \"synthetic\"\n</code></pre>"},{"location":"getting-started/quick-start/#next-steps","title":"Next Steps","text":"<ul> <li>Architecture Overview: Understand the model design</li> <li>Training Guide: Advanced training techniques</li> <li>API Reference: Complete API documentation</li> <li>Examples: More detailed examples</li> </ul>"},{"location":"training/configuration/","title":"Configuration Reference","text":"<p>This page provides detailed information about configuring Free Transformer models and training.</p>"},{"location":"training/configuration/#configuration-files","title":"Configuration Files","text":"<p>Free Transformer uses YAML configuration files to manage model and training parameters. The main configuration files are:</p> <ul> <li><code>configs/free_transformer.yaml</code> - Free Transformer model configuration</li> <li><code>configs/baseline.yaml</code> - Baseline Transformer configuration</li> </ul>"},{"location":"training/configuration/#model-configuration","title":"Model Configuration","text":""},{"location":"training/configuration/#basic-model-parameters","title":"Basic Model Parameters","text":"<pre><code>model:\n  # Architecture basics\n  vocab_size: 50000           # Vocabulary size\n  hidden_dim: 512             # Hidden dimension (d_model)\n  num_layers: 12              # Number of transformer layers\n  num_heads: 8                # Number of attention heads\n  max_seq_len: 1024           # Maximum sequence length\n\n  # Free Transformer specific\n  latent_dim: 32              # Latent plan dimension\n  encoder_layers: 2           # Number of encoder layers\n\n  # Architecture details\n  intermediate_size: 2048     # FFN intermediate size (usually 4 * hidden_dim)\n  dropout: 0.1                # Dropout rate\n  attention_dropout: 0.1      # Attention dropout rate\n\n  # Position encoding\n  rope_theta: 10000.0         # RoPE base frequency\n\n  # Grouped-Query Attention\n  num_key_value_heads: 2      # Number of KV heads (for GQA)\n</code></pre>"},{"location":"training/configuration/#advanced-model-parameters","title":"Advanced Model Parameters","text":"<pre><code>model:\n  # Normalization\n  rms_norm_eps: 1e-6          # RMSNorm epsilon\n\n  # Activation functions\n  hidden_act: \"silu\"          # Activation function (silu, gelu, relu)\n\n  # Initialization\n  initializer_range: 0.02     # Weight initialization range\n\n  # Binary mapping\n  gumbel_temperature: 1.0     # Gumbel-Softmax temperature\n  binary_threshold: 0.5       # Binary threshold for inference\n\n  # Plan injection\n  injection_method: \"additive\" # additive, gated, concat, cross_attention\n  injection_layers: [6, 8, 10] # Which layers to inject plan (if multi-layer)\n</code></pre>"},{"location":"training/configuration/#training-configuration","title":"Training Configuration","text":""},{"location":"training/configuration/#basic-training-parameters","title":"Basic Training Parameters","text":"<pre><code>training:\n  # Optimization\n  batch_size: 32              # Training batch size\n  eval_batch_size: 64         # Evaluation batch size\n  learning_rate: 1e-4         # Peak learning rate\n  num_epochs: 10              # Number of training epochs\n  max_steps: null             # Maximum training steps (overrides epochs)\n\n  # Regularization\n  weight_decay: 0.01          # Weight decay coefficient\n  dropout: 0.1                # Dropout rate\n  gradient_clip_norm: 1.0     # Gradient clipping norm\n\n  # Free Transformer specific\n  kl_weight: 0.1              # KL divergence loss weight\n  free_bits: 0.5              # Free bits threshold\n\n  # Evaluation\n  eval_steps: 500             # Steps between evaluations\n  save_steps: 1000            # Steps between checkpoints\n  logging_steps: 100          # Steps between log outputs\n</code></pre>"},{"location":"training/configuration/#advanced-training-parameters","title":"Advanced Training Parameters","text":"<pre><code>training:\n  # Learning rate scheduling\n  warmup_steps: 1000          # Warmup steps\n  warmup_ratio: 0.1           # Warmup ratio (alternative to warmup_steps)\n  lr_scheduler_type: \"cosine\" # cosine, linear, constant\n  min_lr_ratio: 0.1           # Minimum LR as ratio of peak LR\n\n  # KL annealing\n  kl_annealing: true          # Enable KL weight annealing\n  kl_annealing_steps: 5000    # Steps to anneal KL weight\n  initial_kl_weight: 1.0      # Initial KL weight\n  final_kl_weight: 0.1        # Final KL weight\n\n  # Free bits scheduling\n  free_bits_annealing: true   # Enable free bits annealing\n  initial_free_bits: 2.0      # Initial free bits\n  final_free_bits: 0.5        # Final free bits\n\n  # Mixed precision\n  fp16: false                 # Use FP16 mixed precision\n  bf16: true                  # Use BF16 mixed precision\n\n  # Memory optimization\n  gradient_checkpointing: true # Enable gradient checkpointing\n  dataloader_num_workers: 4   # Number of data loading workers\n  dataloader_pin_memory: true # Pin memory for data loading\n</code></pre>"},{"location":"training/configuration/#optimizer-configuration","title":"Optimizer Configuration","text":"<pre><code>optimizer:\n  type: \"adamw\"               # Optimizer type\n  betas: [0.9, 0.95]         # Adam beta parameters\n  eps: 1e-8                  # Adam epsilon\n  weight_decay: 0.01         # Weight decay\n\n  # Alternative optimizers\n  # type: \"sgd\"\n  # momentum: 0.9\n  # nesterov: true\n</code></pre>"},{"location":"training/configuration/#data-configuration","title":"Data Configuration","text":"<pre><code>data:\n  # Dataset\n  dataset_name: \"synthetic\"   # Dataset name or path\n  dataset_config: null        # Dataset configuration\n\n  # Processing\n  max_seq_len: 512           # Maximum sequence length for training\n  tokenizer_name: null       # Tokenizer name (if using real data)\n\n  # Synthetic data (if using synthetic dataset)\n  num_train_samples: 10000   # Number of training samples\n  num_val_samples: 1000      # Number of validation samples\n  vocab_size: 50000          # Vocabulary size for synthetic data\n\n  # Data loading\n  shuffle: true              # Shuffle training data\n  drop_last: true            # Drop last incomplete batch\n</code></pre>"},{"location":"training/configuration/#distributed-training-configuration","title":"Distributed Training Configuration","text":"<pre><code>distributed:\n  # FSDP (Fully Sharded Data Parallel)\n  use_fsdp: false            # Enable FSDP\n  fsdp_sharding_strategy: \"full_shard\" # full_shard, shard_grad_op, no_shard\n  fsdp_backward_prefetch: \"backward_pre\" # backward_pre, backward_post\n  fsdp_forward_prefetch: false # Enable forward prefetch\n\n  # Model wrapping\n  fsdp_auto_wrap_policy: \"transformer_auto_wrap\" # Auto-wrap policy\n  fsdp_min_num_params: 1e6   # Minimum parameters for wrapping\n\n  # Checkpointing\n  fsdp_state_dict_type: \"full_state_dict\" # full_state_dict, local_state_dict, sharded_state_dict\n</code></pre>"},{"location":"training/configuration/#logging-and-monitoring","title":"Logging and Monitoring","text":"<pre><code>logging:\n  # Output directories\n  output_dir: \"./checkpoints\" # Checkpoint output directory\n  logging_dir: \"./logs\"      # Logging directory\n\n  # Weights &amp; Biases\n  use_wandb: false           # Enable W&amp;B logging\n  wandb_project: \"free-transformer\" # W&amp;B project name\n  wandb_run_name: null       # W&amp;B run name\n  wandb_tags: []             # W&amp;B tags\n\n  # TensorBoard\n  use_tensorboard: true      # Enable TensorBoard logging\n\n  # Console logging\n  log_level: \"info\"          # Logging level\n  disable_tqdm: false        # Disable progress bars\n</code></pre>"},{"location":"training/configuration/#environment-configuration","title":"Environment Configuration","text":"<pre><code>environment:\n  # Device\n  device: \"auto\"             # Device (auto, cpu, cuda, cuda:0)\n\n  # Random seeds\n  seed: 42                   # Random seed\n\n  # CUDA settings\n  cuda_deterministic: false  # Enable CUDA deterministic operations\n  cuda_benchmark: true       # Enable CUDA benchmark mode\n\n  # Memory\n  empty_cache_steps: 1000    # Steps between cache clearing\n</code></pre>"},{"location":"training/configuration/#configuration-examples","title":"Configuration Examples","text":""},{"location":"training/configuration/#small-model-for-testing","title":"Small Model for Testing","text":"<pre><code># configs/small.yaml\nmodel:\n  vocab_size: 1000\n  hidden_dim: 128\n  num_layers: 4\n  num_heads: 4\n  latent_dim: 8\n  max_seq_len: 256\n\ntraining:\n  batch_size: 8\n  learning_rate: 1e-3\n  num_epochs: 5\n  kl_weight: 0.1\n  free_bits: 0.5\n\ndata:\n  dataset_name: \"synthetic\"\n  num_train_samples: 1000\n  num_val_samples: 200\n  max_seq_len: 128\n</code></pre>"},{"location":"training/configuration/#large-model-for-production","title":"Large Model for Production","text":"<pre><code># configs/large.yaml\nmodel:\n  vocab_size: 50000\n  hidden_dim: 1024\n  num_layers: 24\n  num_heads: 16\n  latent_dim: 64\n  max_seq_len: 2048\n  num_key_value_heads: 4\n\ntraining:\n  batch_size: 16\n  learning_rate: 5e-5\n  num_epochs: 3\n  warmup_steps: 2000\n  gradient_checkpointing: true\n  bf16: true\n  kl_weight: 0.05\n  free_bits: 1.0\n\ndistributed:\n  use_fsdp: true\n  fsdp_sharding_strategy: \"full_shard\"\n\nlogging:\n  use_wandb: true\n  wandb_project: \"free-transformer-large\"\n</code></pre>"},{"location":"training/configuration/#curriculum-learning-configuration","title":"Curriculum Learning Configuration","text":"<pre><code># configs/curriculum.yaml\nmodel:\n  vocab_size: 32000\n  hidden_dim: 512\n  num_layers: 12\n  num_heads: 8\n  latent_dim: 32\n\ntraining:\n  # Phase 1: Short sequences, high KL weight\n  phase1:\n    max_seq_len: 128\n    batch_size: 64\n    kl_weight: 1.0\n    free_bits: 2.0\n    num_epochs: 3\n\n  # Phase 2: Medium sequences, medium KL weight\n  phase2:\n    max_seq_len: 256\n    batch_size: 32\n    kl_weight: 0.5\n    free_bits: 1.0\n    num_epochs: 3\n\n  # Phase 3: Long sequences, low KL weight\n  phase3:\n    max_seq_len: 512\n    batch_size: 16\n    kl_weight: 0.1\n    free_bits: 0.5\n    num_epochs: 4\n</code></pre>"},{"location":"training/configuration/#using-configurations","title":"Using Configurations","text":""},{"location":"training/configuration/#command-line","title":"Command Line","text":"<pre><code># Use specific config file\npython examples/train_free.py --config configs/small.yaml\n\n# Override specific parameters\npython examples/train_free.py \\\n  --config configs/free_transformer.yaml \\\n  --batch-size 16 \\\n  --learning-rate 5e-5 \\\n  --kl-weight 0.05\n</code></pre>"},{"location":"training/configuration/#python-api","title":"Python API","text":"<pre><code>import yaml\nfrom free_transformer import ModelConfig, TrainingConfig\n\n# Load from YAML\nwith open('configs/free_transformer.yaml', 'r') as f:\n    config_dict = yaml.safe_load(f)\n\nmodel_config = ModelConfig(**config_dict['model'])\ntraining_config = TrainingConfig(**config_dict['training'])\n\n# Create model\nmodel = FreeTransformer(model_config)\n</code></pre>"},{"location":"training/configuration/#dynamic-configuration","title":"Dynamic Configuration","text":"<pre><code>def create_config_for_size(model_size: str):\n    \"\"\"Create configuration based on model size.\"\"\"\n    if model_size == \"small\":\n        return ModelConfig(\n            vocab_size=1000,\n            hidden_dim=128,\n            num_layers=4,\n            latent_dim=8\n        )\n    elif model_size == \"medium\":\n        return ModelConfig(\n            vocab_size=32000,\n            hidden_dim=512,\n            num_layers=12,\n            latent_dim=32\n        )\n    elif model_size == \"large\":\n        return ModelConfig(\n            vocab_size=50000,\n            hidden_dim=1024,\n            num_layers=24,\n            latent_dim=64\n        )\n    else:\n        raise ValueError(f\"Unknown model size: {model_size}\")\n\n# Usage\nconfig = create_config_for_size(\"medium\")\nmodel = FreeTransformer(config)\n</code></pre>"},{"location":"training/configuration/#configuration-validation","title":"Configuration Validation","text":"<p>The configuration system includes validation to catch common errors:</p> <pre><code>from free_transformer.config import validate_config\n\n# This will raise an error if configuration is invalid\nvalidate_config(config_dict)\n\n# Common validation checks:\n# - hidden_dim must be divisible by num_heads\n# - latent_dim must be positive\n# - batch_size must be positive\n# - learning_rate must be positive\n# - kl_weight must be non-negative\n</code></pre>"},{"location":"training/configuration/#best-practices","title":"Best Practices","text":"<ol> <li>Start small: Begin with small configurations for testing</li> <li>Use templates: Copy and modify existing configurations</li> <li>Version control: Keep configuration files in version control</li> <li>Document changes: Comment important configuration choices</li> <li>Validate early: Check configurations before long training runs</li> <li>Monitor resources: Ensure configurations fit your hardware</li> </ol>"},{"location":"training/configuration/#next-steps","title":"Next Steps","text":"<ul> <li>Training Guide: Learn how to use these configurations</li> <li>Multi-GPU Training: Distributed training setup</li> <li>Examples: See configurations in action</li> </ul>"},{"location":"training/guide/","title":"Training Guide","text":"<p>This guide covers training Free Transformer models from basic setups to advanced distributed training.</p>"},{"location":"training/guide/#basic-training","title":"Basic Training","text":""},{"location":"training/guide/#single-gpu-training","title":"Single GPU Training","text":"<pre><code># Train with default config\npython examples/train_free.py --config configs/free_transformer.yaml\n\n# Train with custom parameters\npython examples/train_free.py \\\n  --config configs/free_transformer.yaml \\\n  --batch-size 16 \\\n  --learning-rate 1e-4 \\\n  --num-epochs 5\n</code></pre>"},{"location":"training/guide/#configuration-files","title":"Configuration Files","text":"<p>Training configurations are defined in YAML files:</p> <pre><code># configs/free_transformer.yaml\nmodel:\n  vocab_size: 50000\n  hidden_dim: 512\n  num_layers: 12\n  num_heads: 8\n  latent_dim: 32\n  max_seq_len: 1024\n\ntraining:\n  batch_size: 32\n  learning_rate: 1e-4\n  num_epochs: 10\n  warmup_steps: 1000\n  weight_decay: 0.01\n\n  # Free Transformer specific\n  kl_weight: 0.1\n  free_bits: 0.5\n\noptimizer:\n  type: \"adamw\"\n  betas: [0.9, 0.95]\n  eps: 1e-8\n\nscheduler:\n  type: \"cosine\"\n  min_lr: 1e-6\n\ndata:\n  dataset_name: \"synthetic\"\n  max_seq_len: 512\n  num_workers: 4\n</code></pre>"},{"location":"training/guide/#loss-components","title":"Loss Components","text":"<p>The Free Transformer uses a composite loss function:</p>"},{"location":"training/guide/#reconstruction-loss","title":"Reconstruction Loss","text":"<p>Standard cross-entropy loss for token prediction: <pre><code>recon_loss = F.cross_entropy(logits.view(-1, vocab_size), targets.view(-1))\n</code></pre></p>"},{"location":"training/guide/#kl-divergence-loss","title":"KL Divergence Loss","text":"<p>Regularizes the latent space: <pre><code>kl_loss = kl_divergence(z_logits, uniform_prior)\n</code></pre></p>"},{"location":"training/guide/#free-bits-regularization","title":"Free Bits Regularization","text":"<p>Prevents posterior collapse: <pre><code>kl_loss = torch.clamp(kl_loss, min=free_bits)\n</code></pre></p>"},{"location":"training/guide/#total-loss","title":"Total Loss","text":"<pre><code>total_loss = recon_loss + kl_weight * kl_loss\n</code></pre>"},{"location":"training/guide/#training-strategies","title":"Training Strategies","text":""},{"location":"training/guide/#1-curriculum-learning","title":"1. Curriculum Learning","text":"<p>Start with simpler tasks and gradually increase complexity:</p> <pre><code># Phase 1: Small sequences, high KL weight\nconfig.max_seq_len = 128\nconfig.kl_weight = 1.0\n\n# Phase 2: Medium sequences, medium KL weight  \nconfig.max_seq_len = 256\nconfig.kl_weight = 0.5\n\n# Phase 3: Full sequences, low KL weight\nconfig.max_seq_len = 512\nconfig.kl_weight = 0.1\n</code></pre>"},{"location":"training/guide/#2-kl-annealing","title":"2. KL Annealing","text":"<p>Gradually reduce KL weight during training:</p> <pre><code>def get_kl_weight(step, total_steps, initial_weight=1.0, final_weight=0.1):\n    progress = step / total_steps\n    return initial_weight * (1 - progress) + final_weight * progress\n</code></pre>"},{"location":"training/guide/#3-free-bits-scheduling","title":"3. Free Bits Scheduling","text":"<p>Adjust free bits threshold over time:</p> <pre><code>def get_free_bits(step, total_steps, initial_bits=2.0, final_bits=0.5):\n    progress = step / total_steps\n    return initial_bits * (1 - progress) + final_bits * progress\n</code></pre>"},{"location":"training/guide/#advanced-training","title":"Advanced Training","text":""},{"location":"training/guide/#mixed-precision-training","title":"Mixed Precision Training","text":"<p>Enable automatic mixed precision for faster training:</p> <pre><code>from torch.cuda.amp import GradScaler, autocast\n\nscaler = GradScaler()\n\nfor batch in dataloader:\n    with autocast():\n        logits, z_logits = model(batch['input_ids'], mode='training')\n        loss = compute_loss(logits, z_logits, batch['input_ids'])\n\n    scaler.scale(loss).backward()\n    scaler.step(optimizer)\n    scaler.update()\n</code></pre>"},{"location":"training/guide/#gradient-checkpointing","title":"Gradient Checkpointing","text":"<p>Reduce memory usage at the cost of computation:</p> <pre><code>model = FreeTransformer(config)\nmodel.gradient_checkpointing_enable()\n</code></pre>"},{"location":"training/guide/#learning-rate-scheduling","title":"Learning Rate Scheduling","text":"<p>Use cosine annealing with warmup:</p> <pre><code>from torch.optim.lr_scheduler import CosineAnnealingLR\nfrom transformers import get_cosine_schedule_with_warmup\n\nscheduler = get_cosine_schedule_with_warmup(\n    optimizer,\n    num_warmup_steps=1000,\n    num_training_steps=total_steps\n)\n</code></pre>"},{"location":"training/guide/#monitoring-and-logging","title":"Monitoring and Logging","text":""},{"location":"training/guide/#tensorboard-logging","title":"TensorBoard Logging","text":"<pre><code>from torch.utils.tensorboard import SummaryWriter\n\nwriter = SummaryWriter('runs/free_transformer')\n\n# Log losses\nwriter.add_scalar('Loss/Reconstruction', recon_loss, step)\nwriter.add_scalar('Loss/KL', kl_loss, step)\nwriter.add_scalar('Loss/Total', total_loss, step)\n\n# Log learning rate\nwriter.add_scalar('Learning_Rate', optimizer.param_groups[0]['lr'], step)\n</code></pre>"},{"location":"training/guide/#weights-biases-integration","title":"Weights &amp; Biases Integration","text":"<pre><code>import wandb\n\nwandb.init(project=\"free-transformer\")\nwandb.config.update(config)\n\n# Log metrics\nwandb.log({\n    'loss/reconstruction': recon_loss,\n    'loss/kl': kl_loss,\n    'loss/total': total_loss,\n    'learning_rate': lr\n})\n</code></pre>"},{"location":"training/guide/#evaluation-during-training","title":"Evaluation During Training","text":""},{"location":"training/guide/#perplexity-calculation","title":"Perplexity Calculation","text":"<pre><code>def calculate_perplexity(model, dataloader):\n    model.eval()\n    total_loss = 0\n    total_tokens = 0\n\n    with torch.no_grad():\n        for batch in dataloader:\n            logits, _ = model(batch['input_ids'], mode='training')\n            loss = F.cross_entropy(\n                logits.view(-1, vocab_size), \n                batch['input_ids'].view(-1),\n                reduction='sum'\n            )\n            total_loss += loss.item()\n            total_tokens += batch['input_ids'].numel()\n\n    return torch.exp(torch.tensor(total_loss / total_tokens))\n</code></pre>"},{"location":"training/guide/#generation-quality","title":"Generation Quality","text":"<pre><code>def evaluate_generation(model, prompts, max_length=100):\n    model.eval()\n    generations = []\n\n    for prompt in prompts:\n        with torch.no_grad():\n            generated = model.generate(\n                prompt,\n                max_new_tokens=max_length,\n                temperature=0.8,\n                top_k=40\n            )\n            generations.append(generated)\n\n    return generations\n</code></pre>"},{"location":"training/guide/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/guide/#common-issues","title":"Common Issues","text":"<p>Posterior Collapse - Symptoms: KL loss drops to zero, model ignores latent variable - Solutions: Increase free bits, reduce KL weight, use KL annealing</p> <p>Training Instability - Symptoms: Loss spikes, gradient explosions - Solutions: Gradient clipping, lower learning rate, warmup</p> <p>Poor Generation Quality - Symptoms: Repetitive or incoherent text - Solutions: Adjust temperature, top-k sampling, increase model size</p>"},{"location":"training/guide/#debugging-tips","title":"Debugging Tips","text":"<ol> <li>Monitor KL loss: Should be positive and stable</li> <li>Check latent utilization: Verify Z is being used</li> <li>Validate gradients: Ensure gradients flow through all components</li> <li>Compare with baseline: Train standard Transformer for comparison</li> </ol>"},{"location":"training/guide/#best-practices","title":"Best Practices","text":"<ol> <li>Start small: Begin with small models and datasets</li> <li>Use curriculum learning: Gradually increase complexity</li> <li>Monitor closely: Watch for posterior collapse</li> <li>Regular evaluation: Check generation quality frequently</li> <li>Save checkpoints: Regular saves for recovery</li> <li>Ablation studies: Test different hyperparameters</li> </ol>"},{"location":"training/guide/#next-steps","title":"Next Steps","text":"<ul> <li>Multi-GPU Training: Scale to multiple GPUs</li> <li>Configuration: Detailed config options</li> <li>Synthetic Data: Generate training data</li> </ul>"},{"location":"training/multi-gpu/","title":"Multi-GPU Training","text":"<p>This guide covers distributed training of Free Transformer models using PyTorch's Fully Sharded Data Parallel (FSDP).</p>"},{"location":"training/multi-gpu/#overview","title":"Overview","text":"<p>Multi-GPU training enables: - Larger models: Train models that don't fit on a single GPU - Faster training: Parallel computation across multiple devices - Better throughput: Higher effective batch sizes</p> <p>Free Transformer supports FSDP for efficient distributed training.</p>"},{"location":"training/multi-gpu/#fsdp-fully-sharded-data-parallel","title":"FSDP (Fully Sharded Data Parallel)","text":"<p>FSDP shards model parameters, gradients, and optimizer states across GPUs:</p> <ul> <li>Parameter sharding: Each GPU holds a subset of parameters</li> <li>Gradient sharding: Gradients are distributed across devices</li> <li>Optimizer sharding: Optimizer states are distributed</li> <li>Communication: All-gather for forward pass, reduce-scatter for backward pass</li> </ul>"},{"location":"training/multi-gpu/#quick-start","title":"Quick Start","text":""},{"location":"training/multi-gpu/#using-makefile","title":"Using Makefile","text":"<pre><code># Train Free Transformer with FSDP (auto-detects GPUs)\nmake train-free-fsdp\n\n# Train baseline with FSDP\nmake train-baseline-fsdp\n</code></pre>"},{"location":"training/multi-gpu/#using-torchrun","title":"Using torchrun","text":"<pre><code># Auto-detect number of GPUs\ntorchrun --nproc_per_node=auto examples/train_free.py \\\n  --config configs/free_transformer.yaml \\\n  --use-fsdp\n\n# Specify number of GPUs\ntorchrun --nproc_per_node=4 examples/train_free.py \\\n  --config configs/free_transformer.yaml \\\n  --use-fsdp\n</code></pre>"},{"location":"training/multi-gpu/#configuration","title":"Configuration","text":""},{"location":"training/multi-gpu/#fsdp-configuration","title":"FSDP Configuration","text":"<p>Add FSDP settings to your YAML config:</p> <pre><code>distributed:\n  # Enable FSDP\n  use_fsdp: true\n\n  # Sharding strategy\n  fsdp_sharding_strategy: \"full_shard\"  # full_shard, shard_grad_op, no_shard\n\n  # Backward prefetch\n  fsdp_backward_prefetch: \"backward_pre\"  # backward_pre, backward_post\n\n  # Forward prefetch\n  fsdp_forward_prefetch: false\n\n  # Auto-wrap policy\n  fsdp_auto_wrap_policy: \"transformer_auto_wrap\"\n  fsdp_min_num_params: 1000000  # Minimum parameters for wrapping\n\n  # State dict type for checkpointing\n  fsdp_state_dict_type: \"full_state_dict\"  # full_state_dict, local_state_dict, sharded_state_dict\n\n  # Mixed precision\n  fsdp_mixed_precision: true\n  fsdp_param_dtype: \"bfloat16\"\n  fsdp_reduce_dtype: \"float32\"\n  fsdp_buffer_dtype: \"float32\"\n</code></pre>"},{"location":"training/multi-gpu/#training-configuration","title":"Training Configuration","text":"<p>Adjust training settings for multi-GPU:</p> <pre><code>training:\n  # Increase batch size for multiple GPUs\n  batch_size: 64  # Per GPU batch size\n\n  # Adjust learning rate for larger effective batch size\n  learning_rate: 2e-4  # Scale with number of GPUs\n\n  # Enable gradient checkpointing for memory efficiency\n  gradient_checkpointing: true\n\n  # Use mixed precision\n  bf16: true\n\n  # Gradient clipping\n  gradient_clip_norm: 1.0\n</code></pre>"},{"location":"training/multi-gpu/#sharding-strategies","title":"Sharding Strategies","text":""},{"location":"training/multi-gpu/#1-full-shard-recommended","title":"1. Full Shard (Recommended)","text":"<pre><code>fsdp_sharding_strategy: \"full_shard\"\n</code></pre> <ul> <li>Memory: Lowest memory usage</li> <li>Communication: Highest communication overhead</li> <li>Use case: Large models, memory-constrained</li> </ul>"},{"location":"training/multi-gpu/#2-shard-grad-op","title":"2. Shard Grad Op","text":"<pre><code>fsdp_sharding_strategy: \"shard_grad_op\"\n</code></pre> <ul> <li>Memory: Medium memory usage</li> <li>Communication: Medium communication overhead</li> <li>Use case: Balance between memory and communication</li> </ul>"},{"location":"training/multi-gpu/#3-no-shard","title":"3. No Shard","text":"<pre><code>fsdp_sharding_strategy: \"no_shard\"\n</code></pre> <ul> <li>Memory: Highest memory usage (like DDP)</li> <li>Communication: Lowest communication overhead</li> <li>Use case: Small models, communication-constrained</li> </ul>"},{"location":"training/multi-gpu/#auto-wrap-policies","title":"Auto-Wrap Policies","text":""},{"location":"training/multi-gpu/#transformer-auto-wrap","title":"Transformer Auto-Wrap","text":"<pre><code>fsdp_auto_wrap_policy: \"transformer_auto_wrap\"\nfsdp_min_num_params: 1000000\n</code></pre> <p>Automatically wraps transformer layers with sufficient parameters.</p>"},{"location":"training/multi-gpu/#size-based-auto-wrap","title":"Size-Based Auto-Wrap","text":"<pre><code>fsdp_auto_wrap_policy: \"size_based_auto_wrap\"\nfsdp_min_num_params: 1000000\n</code></pre> <p>Wraps any module with more than <code>min_num_params</code> parameters.</p>"},{"location":"training/multi-gpu/#custom-auto-wrap","title":"Custom Auto-Wrap","text":"<pre><code>from torch.distributed.fsdp.wrap import ModuleWrapPolicy\nfrom free_transformer.model import DecoderBlock\n\ndef get_custom_wrap_policy():\n    return ModuleWrapPolicy({DecoderBlock})\n</code></pre>"},{"location":"training/multi-gpu/#memory-optimization","title":"Memory Optimization","text":""},{"location":"training/multi-gpu/#mixed-precision","title":"Mixed Precision","text":"<pre><code>fsdp_mixed_precision: true\nfsdp_param_dtype: \"bfloat16\"    # Parameter dtype\nfsdp_reduce_dtype: \"float32\"    # Gradient reduction dtype\nfsdp_buffer_dtype: \"float32\"    # Buffer dtype\n</code></pre>"},{"location":"training/multi-gpu/#activation-checkpointing","title":"Activation Checkpointing","text":"<pre><code>training:\n  gradient_checkpointing: true\n</code></pre> <p>Trades computation for memory by recomputing activations during backward pass.</p>"},{"location":"training/multi-gpu/#cpu-offloading","title":"CPU Offloading","text":"<pre><code>from torch.distributed.fsdp import CPUOffload\n\nfsdp_config = {\n    \"cpu_offload\": CPUOffload(offload_params=True)\n}\n</code></pre> <p>Offloads parameters to CPU when not in use.</p>"},{"location":"training/multi-gpu/#checkpointing","title":"Checkpointing","text":""},{"location":"training/multi-gpu/#full-state-dict-recommended","title":"Full State Dict (Recommended)","text":"<pre><code>fsdp_state_dict_type: \"full_state_dict\"\n</code></pre> <ul> <li>Pros: Compatible with single-GPU loading, easy to use</li> <li>Cons: Requires gathering all parameters on rank 0</li> </ul>"},{"location":"training/multi-gpu/#sharded-state-dict","title":"Sharded State Dict","text":"<pre><code>fsdp_state_dict_type: \"sharded_state_dict\"\n</code></pre> <ul> <li>Pros: Memory efficient, faster saving/loading</li> <li>Cons: Requires same number of GPUs to load</li> </ul>"},{"location":"training/multi-gpu/#local-state-dict","title":"Local State Dict","text":"<pre><code>fsdp_state_dict_type: \"local_state_dict\"\n</code></pre> <ul> <li>Pros: Each rank saves its own shard</li> <li>Cons: Complex to manage, requires custom loading logic</li> </ul>"},{"location":"training/multi-gpu/#example-training-script","title":"Example Training Script","text":"<pre><code>import torch\nimport torch.distributed as dist\nfrom torch.distributed.fsdp import FullyShardedDataParallel as FSDP\nfrom torch.distributed.fsdp.wrap import ModuleWrapPolicy\nfrom free_transformer import FreeTransformer, ModelConfig\nfrom free_transformer.model import DecoderBlock\n\ndef setup_distributed():\n    \"\"\"Initialize distributed training.\"\"\"\n    dist.init_process_group(backend=\"nccl\")\n    torch.cuda.set_device(int(os.environ[\"LOCAL_RANK\"]))\n\ndef get_fsdp_config():\n    \"\"\"Get FSDP configuration.\"\"\"\n    return {\n        \"auto_wrap_policy\": ModuleWrapPolicy({DecoderBlock}),\n        \"sharding_strategy\": ShardingStrategy.FULL_SHARD,\n        \"backward_prefetch\": BackwardPrefetch.BACKWARD_PRE,\n        \"mixed_precision\": MixedPrecision(\n            param_dtype=torch.bfloat16,\n            reduce_dtype=torch.float32,\n            buffer_dtype=torch.float32,\n        ),\n        \"device_id\": torch.cuda.current_device(),\n    }\n\ndef main():\n    setup_distributed()\n\n    # Create model\n    config = ModelConfig(\n        vocab_size=50000,\n        hidden_dim=1024,\n        num_layers=24,\n        num_heads=16,\n        latent_dim=64\n    )\n\n    model = FreeTransformer(config)\n\n    # Wrap with FSDP\n    model = FSDP(model, **get_fsdp_config())\n\n    # Training loop\n    optimizer = torch.optim.AdamW(model.parameters(), lr=1e-4)\n\n    for batch in dataloader:\n        optimizer.zero_grad()\n\n        logits, z_logits = model(batch['input_ids'], mode='training')\n        loss = compute_loss(logits, z_logits, batch['input_ids'])\n\n        loss.backward()\n        optimizer.step()\n\n        if dist.get_rank() == 0:\n            print(f\"Loss: {loss.item():.4f}\")\n\nif __name__ == \"__main__\":\n    main()\n</code></pre>"},{"location":"training/multi-gpu/#performance-tuning","title":"Performance Tuning","text":""},{"location":"training/multi-gpu/#batch-size-scaling","title":"Batch Size Scaling","text":"<p>Scale batch size with number of GPUs:</p> <pre><code># Effective batch size = batch_size * num_gpus * gradient_accumulation_steps\nnum_gpus = torch.cuda.device_count()\neffective_batch_size = 256\nbatch_size_per_gpu = effective_batch_size // num_gpus\n</code></pre>"},{"location":"training/multi-gpu/#learning-rate-scaling","title":"Learning Rate Scaling","text":"<p>Scale learning rate with effective batch size:</p> <pre><code># Linear scaling rule\nbase_lr = 1e-4\nbase_batch_size = 32\neffective_batch_size = batch_size_per_gpu * num_gpus\nscaled_lr = base_lr * (effective_batch_size / base_batch_size)\n</code></pre>"},{"location":"training/multi-gpu/#communication-optimization","title":"Communication Optimization","text":"<pre><code># Reduce communication frequency\nfsdp_backward_prefetch: \"backward_pre\"  # Prefetch parameters\nfsdp_forward_prefetch: true             # Prefetch for forward pass\n\n# Use efficient data types\nfsdp_param_dtype: \"bfloat16\"           # Reduce parameter size\nfsdp_reduce_dtype: \"float32\"           # Maintain precision for gradients\n</code></pre>"},{"location":"training/multi-gpu/#monitoring-and-debugging","title":"Monitoring and Debugging","text":""},{"location":"training/multi-gpu/#memory-usage","title":"Memory Usage","text":"<pre><code>def print_memory_stats():\n    \"\"\"Print GPU memory statistics.\"\"\"\n    if torch.cuda.is_available():\n        for i in range(torch.cuda.device_count()):\n            allocated = torch.cuda.memory_allocated(i) / 1024**3\n            reserved = torch.cuda.memory_reserved(i) / 1024**3\n            print(f\"GPU {i}: {allocated:.2f}GB allocated, {reserved:.2f}GB reserved\")\n</code></pre>"},{"location":"training/multi-gpu/#communication-profiling","title":"Communication Profiling","text":"<pre><code>import torch.profiler\n\nwith torch.profiler.profile(\n    activities=[\n        torch.profiler.ProfilerActivity.CPU,\n        torch.profiler.ProfilerActivity.CUDA,\n    ],\n    record_shapes=True,\n    with_stack=True,\n) as prof:\n    # Training step\n    pass\n\nprof.export_chrome_trace(\"trace.json\")\n</code></pre>"},{"location":"training/multi-gpu/#fsdp-debug-mode","title":"FSDP Debug Mode","text":"<pre><code>export TORCH_DISTRIBUTED_DEBUG=DETAIL\ntorchrun --nproc_per_node=4 examples/train_free.py --config configs/free_transformer.yaml --use-fsdp\n</code></pre>"},{"location":"training/multi-gpu/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/multi-gpu/#common-issues","title":"Common Issues","text":"<p>Out of Memory - Reduce batch size per GPU - Enable gradient checkpointing - Use CPU offloading - Increase sharding level</p> <p>Slow Training - Check network bandwidth between GPUs - Reduce communication overhead - Use mixed precision - Optimize data loading</p> <p>Convergence Issues - Adjust learning rate for effective batch size - Use gradient clipping - Check for numerical instabilities - Monitor gradient norms</p>"},{"location":"training/multi-gpu/#nccl-issues","title":"NCCL Issues","text":"<pre><code># Set NCCL debug level\nexport NCCL_DEBUG=INFO\n\n# Set NCCL timeout\nexport NCCL_TIMEOUT=1800\n\n# Use specific network interface\nexport NCCL_SOCKET_IFNAME=eth0\n</code></pre>"},{"location":"training/multi-gpu/#best-practices","title":"Best Practices","text":"<ol> <li>Start small: Test with 2 GPUs before scaling up</li> <li>Monitor memory: Use memory profiling to optimize usage</li> <li>Scale gradually: Increase model size and GPU count incrementally</li> <li>Use checkpointing: Save frequently with full state dict</li> <li>Profile communication: Identify and optimize bottlenecks</li> <li>Test convergence: Ensure multi-GPU results match single-GPU</li> </ol>"},{"location":"training/multi-gpu/#hardware-recommendations","title":"Hardware Recommendations","text":""},{"location":"training/multi-gpu/#gpu-configuration","title":"GPU Configuration","text":"<ul> <li>Memory: 24GB+ per GPU for large models</li> <li>Interconnect: NVLink or InfiniBand for best performance</li> <li>Topology: Avoid crossing CPU sockets when possible</li> </ul>"},{"location":"training/multi-gpu/#network-requirements","title":"Network Requirements","text":"<ul> <li>Bandwidth: 100Gbps+ for large-scale training</li> <li>Latency: Low latency interconnects (InfiniBand, NVLink)</li> <li>Topology: All-to-all connectivity preferred</li> </ul>"},{"location":"training/multi-gpu/#next-steps","title":"Next Steps","text":"<ul> <li>Training Guide: General training best practices</li> <li>Configuration: Detailed configuration options</li> <li>Examples: See multi-GPU examples in action</li> </ul>"},{"location":"training/synthetic-data/","title":"Synthetic Data Generation","text":"<p>This guide covers generating synthetic training data for Free Transformer experiments and prototyping.</p>"},{"location":"training/synthetic-data/#overview","title":"Overview","text":"<p>Synthetic data generation enables: - Fast prototyping: Test models without large datasets - Controlled experiments: Known data properties for analysis - Ablation studies: Isolate specific model behaviors - Development: Quick iteration during model development</p>"},{"location":"training/synthetic-data/#quick-start","title":"Quick Start","text":""},{"location":"training/synthetic-data/#using-makefile","title":"Using Makefile","text":"<pre><code># Generate large synthetic dataset\nmake generate-data\n\n# Generate small dataset for testing\nmake generate-data-small\n</code></pre>"},{"location":"training/synthetic-data/#using-python-script","title":"Using Python Script","text":"<pre><code># Basic usage\npython examples/generate_data.py --output-dir data/synthetic\n\n# Custom parameters\npython examples/generate_data.py \\\n  --output-dir data/custom \\\n  --vocab-size 5000 \\\n  --seq-length 256 \\\n  --num-train 10000 \\\n  --num-val 1000 \\\n  --seed 42\n</code></pre>"},{"location":"training/synthetic-data/#data-generation-types","title":"Data Generation Types","text":""},{"location":"training/synthetic-data/#1-random-token-sequences","title":"1. Random Token Sequences","text":"<p>Simple random sequences for basic testing:</p> <pre><code>from free_transformer.synthetic_data import generate_random_sequences\n\ndata = generate_random_sequences(\n    vocab_size=1000,\n    seq_length=128,\n    num_samples=5000,\n    seed=42\n)\n</code></pre> <p>Use cases: - Model architecture testing - Memory usage profiling - Basic training pipeline validation</p>"},{"location":"training/synthetic-data/#2-pattern-based-sequences","title":"2. Pattern-Based Sequences","text":"<p>Sequences with embedded patterns for coherence testing:</p> <pre><code>from free_transformer.synthetic_data import generate_pattern_sequences\n\ndata = generate_pattern_sequences(\n    vocab_size=1000,\n    seq_length=256,\n    num_samples=10000,\n    pattern_types=['repetition', 'alternation', 'progression'],\n    pattern_probability=0.3,\n    seed=42\n)\n</code></pre> <p>Pattern types: - Repetition: <code>[A, B, C, A, B, C, ...]</code> - Alternation: <code>[A, B, A, B, A, B, ...]</code> - Progression: <code>[1, 2, 3, 4, 5, ...]</code> - Reversal: <code>[A, B, C, C, B, A, ...]</code></p>"},{"location":"training/synthetic-data/#3-hierarchical-sequences","title":"3. Hierarchical Sequences","text":"<p>Multi-level structured sequences:</p> <pre><code>from free_transformer.synthetic_data import generate_hierarchical_sequences\n\ndata = generate_hierarchical_sequences(\n    vocab_size=1000,\n    seq_length=512,\n    num_samples=20000,\n    hierarchy_levels=3,\n    structure_probability=0.5,\n    seed=42\n)\n</code></pre> <p>Structure levels: - Global: Overall sequence theme - Local: Subsequence patterns - Token: Individual token relationships</p>"},{"location":"training/synthetic-data/#4-conditional-sequences","title":"4. Conditional Sequences","text":"<p>Sequences conditioned on control tokens:</p> <pre><code>from free_transformer.synthetic_data import generate_conditional_sequences\n\ndata = generate_conditional_sequences(\n    vocab_size=1000,\n    seq_length=256,\n    num_samples=15000,\n    num_conditions=10,\n    condition_strength=0.8,\n    seed=42\n)\n</code></pre> <p>Conditioning: - First token determines sequence properties - Style, length, or pattern controlled by condition - Tests model's ability to follow instructions</p>"},{"location":"training/synthetic-data/#configuration-options","title":"Configuration Options","text":""},{"location":"training/synthetic-data/#basic-parameters","title":"Basic Parameters","text":"<pre><code>config = {\n    'vocab_size': 10000,        # Vocabulary size\n    'seq_length': 512,          # Sequence length\n    'num_train': 50000,         # Training samples\n    'num_val': 5000,            # Validation samples\n    'seed': 42,                 # Random seed\n}\n</code></pre>"},{"location":"training/synthetic-data/#advanced-parameters","title":"Advanced Parameters","text":"<pre><code>config = {\n    # Pattern control\n    'pattern_probability': 0.3,  # Probability of pattern occurrence\n    'pattern_length': 10,        # Average pattern length\n    'pattern_types': ['rep', 'alt', 'prog'],  # Pattern types to use\n\n    # Hierarchy control\n    'hierarchy_levels': 3,       # Number of hierarchy levels\n    'structure_probability': 0.5, # Probability of structured content\n    'global_coherence': 0.7,     # Global coherence strength\n\n    # Conditioning\n    'num_conditions': 20,        # Number of condition types\n    'condition_strength': 0.8,   # How strongly condition affects sequence\n    'condition_tokens': [0, 1, 2], # Special condition tokens\n\n    # Noise and variation\n    'noise_probability': 0.1,    # Random token injection probability\n    'length_variation': 0.2,     # Sequence length variation\n    'vocab_distribution': 'uniform', # Token distribution (uniform, zipf)\n}\n</code></pre>"},{"location":"training/synthetic-data/#data-analysis","title":"Data Analysis","text":""},{"location":"training/synthetic-data/#sequence-statistics","title":"Sequence Statistics","text":"<pre><code>from free_transformer.synthetic_data import analyze_sequences\n\ndef analyze_dataset(sequences):\n    \"\"\"Analyze synthetic dataset properties.\"\"\"\n    stats = {\n        'num_sequences': len(sequences),\n        'avg_length': np.mean([len(seq) for seq in sequences]),\n        'vocab_usage': len(set(token for seq in sequences for token in seq)),\n        'token_distribution': Counter(token for seq in sequences for token in seq),\n        'pattern_detection': detect_patterns(sequences),\n    }\n    return stats\n\n# Example usage\ntrain_data = load_synthetic_data('data/synthetic/train.jsonl')\nstats = analyze_dataset(train_data)\nprint(f\"Dataset contains {stats['num_sequences']} sequences\")\nprint(f\"Average length: {stats['avg_length']:.1f} tokens\")\nprint(f\"Vocabulary usage: {stats['vocab_usage']}/{config['vocab_size']}\")\n</code></pre>"},{"location":"training/synthetic-data/#pattern-detection","title":"Pattern Detection","text":"<pre><code>def detect_patterns(sequences, min_length=3):\n    \"\"\"Detect common patterns in sequences.\"\"\"\n    patterns = Counter()\n\n    for seq in sequences:\n        for i in range(len(seq) - min_length + 1):\n            pattern = tuple(seq[i:i + min_length])\n            patterns[pattern] += 1\n\n    # Find most common patterns\n    common_patterns = patterns.most_common(10)\n    return common_patterns\n\n# Example usage\npatterns = detect_patterns(train_data)\nprint(\"Most common patterns:\")\nfor pattern, count in patterns:\n    print(f\"  {pattern}: {count} occurrences\")\n</code></pre>"},{"location":"training/synthetic-data/#coherence-metrics","title":"Coherence Metrics","text":"<pre><code>def measure_coherence(sequences):\n    \"\"\"Measure sequence coherence metrics.\"\"\"\n    metrics = {}\n\n    # Local coherence (adjacent token similarity)\n    local_coherence = []\n    for seq in sequences:\n        similarities = []\n        for i in range(len(seq) - 1):\n            # Simple similarity: same token = 1, different = 0\n            sim = 1 if seq[i] == seq[i+1] else 0\n            similarities.append(sim)\n        local_coherence.append(np.mean(similarities))\n\n    metrics['local_coherence'] = np.mean(local_coherence)\n\n    # Global coherence (sequence-level consistency)\n    global_coherence = []\n    for seq in sequences:\n        # Measure repetition of tokens\n        token_counts = Counter(seq)\n        max_count = max(token_counts.values())\n        coherence = max_count / len(seq)\n        global_coherence.append(coherence)\n\n    metrics['global_coherence'] = np.mean(global_coherence)\n\n    return metrics\n</code></pre>"},{"location":"training/synthetic-data/#custom-data-generators","title":"Custom Data Generators","text":""},{"location":"training/synthetic-data/#creating-custom-generators","title":"Creating Custom Generators","text":"<pre><code>from free_transformer.synthetic_data import BaseDataGenerator\n\nclass CustomDataGenerator(BaseDataGenerator):\n    def __init__(self, vocab_size, seq_length, **kwargs):\n        super().__init__(vocab_size, seq_length, **kwargs)\n        self.custom_param = kwargs.get('custom_param', 0.5)\n\n    def generate_sequence(self):\n        \"\"\"Generate a single sequence with custom logic.\"\"\"\n        sequence = []\n\n        # Custom generation logic here\n        for i in range(self.seq_length):\n            if np.random.random() &lt; self.custom_param:\n                # Custom behavior\n                token = self.generate_special_token(i)\n            else:\n                # Random token\n                token = np.random.randint(0, self.vocab_size)\n            sequence.append(token)\n\n        return sequence\n\n    def generate_special_token(self, position):\n        \"\"\"Generate special token based on position.\"\"\"\n        # Example: position-dependent token\n        return position % 100\n\n# Usage\ngenerator = CustomDataGenerator(\n    vocab_size=1000,\n    seq_length=256,\n    custom_param=0.3\n)\n\nsequences = generator.generate_dataset(num_samples=10000)\n</code></pre>"},{"location":"training/synthetic-data/#template-based-generation","title":"Template-Based Generation","text":"<pre><code>class TemplateDataGenerator(BaseDataGenerator):\n    def __init__(self, vocab_size, seq_length, templates, **kwargs):\n        super().__init__(vocab_size, seq_length, **kwargs)\n        self.templates = templates\n\n    def generate_sequence(self):\n        \"\"\"Generate sequence from template.\"\"\"\n        template = np.random.choice(self.templates)\n        sequence = []\n\n        for element in template:\n            if element == 'RANDOM':\n                token = np.random.randint(0, self.vocab_size)\n            elif element == 'PATTERN':\n                token = self.generate_pattern_token(len(sequence))\n            else:\n                token = element\n            sequence.append(token)\n\n        # Pad or truncate to desired length\n        sequence = self.adjust_length(sequence)\n        return sequence\n\n# Define templates\ntemplates = [\n    [1, 'RANDOM', 'RANDOM', 2, 'PATTERN', 'PATTERN'],\n    [3, 'PATTERN', 'RANDOM', 'PATTERN', 4],\n    ['RANDOM'] * 10 + [5] + ['PATTERN'] * 5,\n]\n\ngenerator = TemplateDataGenerator(\n    vocab_size=1000,\n    seq_length=256,\n    templates=templates\n)\n</code></pre>"},{"location":"training/synthetic-data/#data-quality-control","title":"Data Quality Control","text":""},{"location":"training/synthetic-data/#validation-checks","title":"Validation Checks","text":"<pre><code>def validate_synthetic_data(sequences, config):\n    \"\"\"Validate generated synthetic data.\"\"\"\n    issues = []\n\n    # Check sequence lengths\n    lengths = [len(seq) for seq in sequences]\n    if min(lengths) &lt; config['min_length']:\n        issues.append(f\"Sequences too short: min={min(lengths)}\")\n    if max(lengths) &gt; config['max_length']:\n        issues.append(f\"Sequences too long: max={max(lengths)}\")\n\n    # Check vocabulary usage\n    all_tokens = set(token for seq in sequences for token in seq)\n    if max(all_tokens) &gt;= config['vocab_size']:\n        issues.append(f\"Invalid tokens: max={max(all_tokens)}\")\n    if min(all_tokens) &lt; 0:\n        issues.append(f\"Negative tokens: min={min(all_tokens)}\")\n\n    # Check for empty sequences\n    empty_count = sum(1 for seq in sequences if len(seq) == 0)\n    if empty_count &gt; 0:\n        issues.append(f\"Empty sequences: {empty_count}\")\n\n    return issues\n\n# Usage\nissues = validate_synthetic_data(train_data, config)\nif issues:\n    print(\"Data validation issues:\")\n    for issue in issues:\n        print(f\"  - {issue}\")\nelse:\n    print(\"Data validation passed!\")\n</code></pre>"},{"location":"training/synthetic-data/#quality-metrics","title":"Quality Metrics","text":"<pre><code>def compute_quality_metrics(sequences):\n    \"\"\"Compute data quality metrics.\"\"\"\n    metrics = {}\n\n    # Diversity metrics\n    unique_sequences = len(set(tuple(seq) for seq in sequences))\n    metrics['sequence_diversity'] = unique_sequences / len(sequences)\n\n    all_tokens = [token for seq in sequences for token in seq]\n    unique_tokens = len(set(all_tokens))\n    metrics['token_diversity'] = unique_tokens\n\n    # Complexity metrics\n    avg_entropy = np.mean([compute_entropy(seq) for seq in sequences])\n    metrics['average_entropy'] = avg_entropy\n\n    # Pattern metrics\n    pattern_count = sum(1 for seq in sequences if has_pattern(seq))\n    metrics['pattern_ratio'] = pattern_count / len(sequences)\n\n    return metrics\n\ndef compute_entropy(sequence):\n    \"\"\"Compute entropy of a sequence.\"\"\"\n    token_counts = Counter(sequence)\n    total_tokens = len(sequence)\n\n    entropy = 0\n    for count in token_counts.values():\n        prob = count / total_tokens\n        entropy -= prob * np.log2(prob)\n\n    return entropy\n</code></pre>"},{"location":"training/synthetic-data/#integration-with-training","title":"Integration with Training","text":""},{"location":"training/synthetic-data/#data-loading","title":"Data Loading","text":"<pre><code>from torch.utils.data import Dataset, DataLoader\n\nclass SyntheticDataset(Dataset):\n    def __init__(self, sequences, max_length=None):\n        self.sequences = sequences\n        self.max_length = max_length\n\n    def __len__(self):\n        return len(self.sequences)\n\n    def __getitem__(self, idx):\n        sequence = self.sequences[idx]\n\n        if self.max_length:\n            sequence = sequence[:self.max_length]\n\n        return {\n            'input_ids': torch.tensor(sequence, dtype=torch.long),\n            'length': len(sequence)\n        }\n\n# Usage\ntrain_sequences = load_synthetic_data('data/synthetic/train.jsonl')\ntrain_dataset = SyntheticDataset(train_sequences, max_length=512)\ntrain_loader = DataLoader(train_dataset, batch_size=32, shuffle=True)\n</code></pre>"},{"location":"training/synthetic-data/#curriculum-learning","title":"Curriculum Learning","text":"<pre><code>def create_curriculum_data(base_sequences, stages):\n    \"\"\"Create curriculum learning datasets.\"\"\"\n    curriculum_data = {}\n\n    for stage_name, stage_config in stages.items():\n        # Filter sequences by complexity\n        filtered_sequences = filter_by_complexity(\n            base_sequences, \n            stage_config['min_complexity'],\n            stage_config['max_complexity']\n        )\n\n        # Adjust sequence lengths\n        adjusted_sequences = adjust_sequence_lengths(\n            filtered_sequences,\n            stage_config['max_length']\n        )\n\n        curriculum_data[stage_name] = adjusted_sequences\n\n    return curriculum_data\n\n# Define curriculum stages\nstages = {\n    'stage1': {'min_complexity': 0.0, 'max_complexity': 0.3, 'max_length': 128},\n    'stage2': {'min_complexity': 0.2, 'max_complexity': 0.6, 'max_length': 256},\n    'stage3': {'min_complexity': 0.5, 'max_complexity': 1.0, 'max_length': 512},\n}\n\ncurriculum_data = create_curriculum_data(train_sequences, stages)\n</code></pre>"},{"location":"training/synthetic-data/#best-practices","title":"Best Practices","text":"<ol> <li>Start simple: Begin with basic random sequences</li> <li>Add complexity gradually: Introduce patterns incrementally</li> <li>Validate data: Always check generated data quality</li> <li>Monitor diversity: Ensure sufficient sequence variation</li> <li>Use appropriate size: Match real data characteristics</li> <li>Document generation: Keep track of generation parameters</li> <li>Version control: Save generation configs and seeds</li> </ol>"},{"location":"training/synthetic-data/#troubleshooting","title":"Troubleshooting","text":""},{"location":"training/synthetic-data/#common-issues","title":"Common Issues","text":"<p>Low diversity - Increase vocabulary size - Reduce pattern probability - Add noise to generation</p> <p>Poor patterns - Adjust pattern parameters - Check pattern detection logic - Validate pattern implementation</p> <p>Memory issues - Generate data in batches - Use streaming data loading - Reduce sequence length</p> <p>Training instability - Check data distribution - Validate sequence lengths - Ensure proper tokenization</p>"},{"location":"training/synthetic-data/#next-steps","title":"Next Steps","text":"<ul> <li>Training Guide: Use synthetic data for training</li> <li>Configuration: Configure data parameters</li> <li>Examples: See synthetic data in action</li> </ul>"}]}